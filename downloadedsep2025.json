{"data":["v0.2",{"title":"AI Manifestos","description":"Perspectives from ten manifestos on the future of AI","topics":[{"id":"dbcf74f9-4754-4d7f-aba5-4ee83eec1586","title":"Responsible AI Development","description":"The ethical and practical considerations in the development of AI systems.","subtopics":[{"id":"29902ddb-ba11-4337-a292-3bb0ae9bed69","title":"Collective Action Problems","description":"Challenges in industry cooperation to ensure AI systems are developed responsibly.","claims":[{"id":"b1466862-123f-4c39-b67b-6681dabcb7e7","title":"Balancing risk mitigation with fostering access and innovation is crucial in AI development.","quotes":[{"id":"719c7f61-6927-4f50-915a-66d8c6396d28","text":"Striking the optimal balance between mitigating risks and fostering access and innovation is paramount to the responsible development of AI.","reference":{"id":"39d6c1f0-1e3f-47cd-bc1c-e668f9a643a6","sourceId":"f9dc12ce-b83a-44f0-ae10-56e41337faf5","interview":"DeepMind team","data":["text",{"startIdx":1399,"endIdx":1539}]}}],"number":54,"similarClaims":[]},{"id":"592d5f9f-d15e-4682-aa1c-9febddc704a8","title":"AI companies may underinvest in safety due to competitive pressures.","quotes":[{"id":"e9109c3a-bb15-4987-987a-19548058ab28","text":"competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe","reference":{"id":"c99d7d0a-d5dc-4960-b695-74351633fa7e","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":101}]}}],"number":55,"similarClaims":[]},{"id":"849ce335-6904-4f54-b99c-452bbee4b3bb","title":"Responsible AI development requires solving collective action problems.","quotes":[{"id":"ec58bb05-4763-4da5-8320-283d84c5a7f9","text":"Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies.","reference":{"id":"6bf83b56-c484-4a18-a062-c25debcf17d7","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":140}]}}],"number":56,"similarClaims":[]},{"id":"e9fea39a-dacc-4b7a-a4e8-6ac99314ec5f","title":"Competition can decrease AI companies' incentives to develop responsibly.","quotes":[{"id":"01d16c9d-daa1-4700-b15c-f521c2191a77","text":"Competition between AI companies could decrease the incentives of each company to develop responsibly by increasing their incentives to develop faster.","reference":{"id":"9cddb7c4-d346-4386-808c-e2d176bb5856","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":150}]}}],"number":57,"similarClaims":[]},{"id":"72e8a5f7-4ad1-42db-9b6e-178b605091b2","title":"High trust between AI developers can improve cooperation on responsible development.","quotes":[{"id":"1104beeb-432b-4c13-b55b-a96bea082e3b","text":"These factors are: high trust between developers (High Trust)","reference":{"id":"4323fb92-5c2e-4d00-875d-4ae9ebbd84e7","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":2652,"endIdx":2713}]}}],"number":58,"similarClaims":[]},{"id":"6e0ba303-15cd-4d1b-bd71-b94be63f9687","title":"Shared gains from cooperation can encourage responsible AI development.","quotes":[{"id":"b7a1d1c0-9191-4685-bd08-839729001f0d","text":"high shared gains from mutual cooperation (Shared Upside)","reference":{"id":"c07ef793-04da-4ac9-acdf-4d9f03a4e129","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":56}]}}],"number":59,"similarClaims":[]},{"id":"3d20d0cf-54c0-4587-b58f-a0e08b470625","title":"Limited exposure to losses can facilitate cooperation among AI companies.","quotes":[{"id":"61d86f65-2b0b-4f66-98e0-6cde6bffec42","text":"limited exposure to potential losses in the event of unreciprocated cooperation (Low Exposure)","reference":{"id":"6ccf21f0-541f-4a79-b003-ad316842ba2e","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":2774,"endIdx":2868}]}}],"number":60,"similarClaims":[]},{"id":"17a5ea0f-0994-4310-aee5-825bb23c15dc","title":"Limited gains from non-cooperation can promote responsible AI development.","quotes":[{"id":"2d3d4613-6878-4a82-a51a-5840d8cca266","text":"limited gains from not reciprocating the cooperation of others (Low Advantage)","reference":{"id":"1c9646e6-29dc-4933-98de-13f5a7ea107d","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":2870,"endIdx":2948}]}}],"number":61,"similarClaims":[]},{"id":"0b4fc41c-7476-4bce-9670-d18830f24169","title":"High shared losses from mutual defection can drive AI companies to cooperate.","quotes":[{"id":"cb75496f-583f-40fd-ba4c-a6ca54757cdd","text":"high shared losses from mutual defection","reference":{"id":"ff0f19e7-fc3f-474c-8376-aec4b981b0d1","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":2954,"endIdx":2994}]}}],"number":62,"similarClaims":[]},{"id":"cabaf9cb-818b-432c-836b-3c33b70fe3a3","title":"AI companies can take steps to foster cooperation on responsible AI development.","quotes":[{"id":"e3352a31-6a86-4f76-b6e6-3bfa894823c7","text":"Using these five factors, we identify four strategies that AI companies and other relevant parties could use to increase the prospects for cooperation around responsible AI development.","reference":{"id":"6839429a-92fa-4179-aa4c-4e29d8f54210","sourceId":"b5ebe868-1647-4d61-8312-edc660e03a7b","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":184}]}}],"number":63,"similarClaims":[]},{"id":"8d711cca-ddcc-4f79-8ae4-6aceb8cb29d2","title":"High shared losses from mutual defection can drive AI companies to cooperate.","quotes":[{"id":"fb350bbd-e44f-4ed1-b521-3be441b20144","text":"Competitive pressures can generate incentives for AI companies to invest less in responsible development than they would in the absence of competition.","reference":{"id":"8cc3c612-f1c7-458e-963a-839bd8b6ffa8","sourceId":"b5ebe868-1647-4d61-8312-edc660e03a7b","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":150}]}}],"number":64,"similarClaims":[]},{"id":"9f1d09bf-2146-4bc6-86fb-3fd36f8654f0","title":"AI security work aims to prevent systems from being misused by bad actors.","quotes":[{"id":"ef71fe1e-3f81-4098-ab00-828c3ad66063","text":"Work on the security of AI aims to prevent AI systems from being attacked, co-opted, or misused by bad actors","reference":{"id":"9110356b-51a5-4413-a939-4fa59d6caaf1","sourceId":"0e15ab3a-938e-483d-b2a5-982c25f7248c","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":108}]}}],"number":65,"similarClaims":[]},{"id":"3a0c650e-a11e-44e9-8313-b7141694298d","title":"Late movers in AI can benefit from the R&D and market information of first movers.","quotes":[{"id":"aa143542-a9d7-4410-b5fc-2cee07a85531","text":"These include being able to free-ride on the R&D of the front-runner, to act on more information about the relevant market, to act under more regulatory certainty, and having more flexible assets and structures that let a company respond more effectively to changes in the environment.","reference":{"id":"94c84c00-72c9-451b-8efc-ef989485936b","sourceId":"bf8e7230-cacb-40c6-a4fd-8cc905ba1440","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":284}]}}],"number":66,"similarClaims":[]},{"id":"91da898d-534d-4bd9-99b4-651c9dc63098","title":"High shared losses from mutual defection can drive AI companies to cooperate.","quotes":[{"id":"f51b4139-5963-440a-a348-a4ab65d6a867","text":"If a first-mover advantage in AI is weak or non-existent then companies are less likely to engage in a race to the bottom on safety since speed is of lower value.","reference":{"id":"025f1d3d-c98c-4ce3-aa59-856c25857322","sourceId":"bf8e7230-cacb-40c6-a4fd-8cc905ba1440","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":161}]}}],"number":67,"similarClaims":[]},{"id":"4d75ef49-9bc4-4791-9b4a-01c53abcb412","title":"Active human intention is necessary to steer technology towards positive outcomes.","quotes":[{"id":"4a6dcdb8-7deb-4a30-bc48-b681bfb8c225","text":"We need active human intention to choose the directions that we want.","reference":{"id":"21e4cc46-7930-4b5a-b907-c356dcec55a9","sourceId":"1a4767f8-1790-4fc0-bf9b-7ccc937d2874","interview":"Vitalik Buterin","data":["text",{"startIdx":-1,"endIdx":68}]}}],"number":68,"similarClaims":[]},{"id":"0fab8cc5-5783-4729-bb45-52efebe1ba83","title":"The AI safety community has not sufficiently considered slowing down AI research as a safety measure.","quotes":[{"id":"02dba84e-1bac-4eb9-8195-204b62bc6b42","text":"a decent amount of thinking should happen before writing off everything in this large space of interventions","reference":{"id":"ac266038-3528-4c5b-9094-176c27d5bbb3","sourceId":"7564d93c-3d8e-4f32-937a-52496a7ad000","interview":"Katja Grace","data":["text",{"startIdx":2002,"endIdx":2110}]}}],"number":69,"similarClaims":[]},{"id":"9f4777e0-02d8-4680-a585-bb7bf46d9b2f","title":"High shared losses from mutual defection can drive AI companies to cooperate.","quotes":[{"id":"3e35c760-2231-436f-b1e5-738062430077","text":"if AI safety people criticize those contributing to AI progress, it will mostly discourage the most friendly and careful AI capabilities companies","reference":{"id":"f86465f5-32c7-4686-9057-4bba1aa4aca0","sourceId":"b8665153-5283-4ab5-bbed-54e480751533","interview":"Katja Grace","data":["text",{"startIdx":2801,"endIdx":2947}]}}],"number":70,"similarClaims":[]},{"id":"3bef82b8-4ca7-416e-8c9d-218787bd4ee2","title":"Coordinating global AI safety efforts is extremely challenging.","quotes":[{"id":"84e95ae2-bd4d-4ddc-a229-c00513e460f2","text":"But coordinating anything with the whole world seems wildly tricky.","reference":{"id":"f087cece-8069-4241-b2a9-76cd594c13ba","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":58,"endIdx":125}]}}],"number":71,"similarClaims":[]},{"id":"e04de06b-e140-48a2-be4a-bc3a998d058e","title":"High shared losses from mutual defection can drive AI companies to cooperate.","quotes":[{"id":"eafce07b-24ee-4d54-8cb3-9da464912504","text":"Agitating for slower AI progress is ‘defecting’ against the AI capabilities folks, who are good friends of the AI safety community.","reference":{"id":"8b13a9ea-5a8f-4f3d-b817-9cfaf0dfa80b","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":-1,"endIdx":130}]}}],"number":72,"similarClaims":[]},{"id":"47c66547-8fab-4856-8a2a-4446c898e946","title":"High shared losses from mutual defection can drive AI companies to cooperate.","quotes":[{"id":"73ccdf78-73d1-4eec-9588-07306a862ac1","text":"AI companies talking about all the amazing benefits of AI can come off like propagandists.","reference":{"id":"eabc4d7a-08f4-4e5f-be9a-02c02f39f8b1","sourceId":"a3d433f6-4814-43be-a449-64acd411ce5f","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":89}]}}],"number":73,"similarClaims":[]},{"id":"59a96a5c-ce97-4677-826b-011f16bfa23b","title":"High shared losses from mutual defection can drive AI companies to cooperate.","quotes":[{"id":"5e1c5d9c-b514-49d5-af61-77a1764bf010","text":"This is not how the CEO of Microsoft talks in a sane world. It shows an overwhelming gap between how seriously we are taking the problem, and how seriously we needed to take the problem starting 30 years ago.","reference":{"id":"a570954e-f019-4238-86c0-c966d6ba76fa","sourceId":"bb20bbc4-aef4-455d-b993-0ab46961891f","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":2568,"endIdx":2776}]}}],"number":74,"similarClaims":[]}]},{"id":"268281cf-9067-4778-bfce-944eff646336","title":"Costs of Responsible Development","description":"The trade-offs and potential losses involved in developing AI responsibly.","claims":[{"id":"55351b61-1e4b-4a8d-9dbb-b1b9d5756505","title":"Security and deployment mitigations for AI can slow down innovation and reduce accessibility.","quotes":[{"id":"cf43e6f3-121d-432e-8350-9521e7c96cf6","text":"These measures, however, may also slow down the rate of innovation and reduce the broad accessibility of capabilities.","reference":{"id":"4641d873-8a98-4387-bb33-72d12c661894","sourceId":"f9dc12ce-b83a-44f0-ae10-56e41337faf5","interview":"DeepMind team","data":["text",{"startIdx":1280,"endIdx":1398}]}}],"number":75,"similarClaims":[]},{"id":"0b18c202-7f5b-420c-b2d0-ae1fad1a8a4d","title":"AI companies have weaker incentives for responsible development compared to other industries.","quotes":[{"id":"5fa7659c-8071-4058-8e7d-bb3764e78184","text":"AI companies have the same incentives to develop AI systems responsibly, although they appear to be weaker than they are in other industries.","reference":{"id":"07310085-e495-4454-92a1-1ea9e2d0e98b","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":140}]}}],"number":76,"similarClaims":[]},{"id":"a5bb56c0-4f80-44d7-b82a-485fb96a47d7","title":"Many AI companies would prefer to develop AI systems with socially optimal risk levels.","quotes":[{"id":"b80d93ab-cf75-4f4d-87be-e9b9cb151c7c","text":"if AI companies would prefer to develop AI systems with risk levels that are closer to what is socially optimal—as we believe many do","reference":{"id":"1c3ce14e-92e2-4f8d-a57d-00aeac5c0514","sourceId":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":132}]}}],"number":77,"similarClaims":[]},{"id":"0761a734-2903-4b26-8de5-953d466a69fd","title":"Responsible AI development is crucial as AI systems are increasingly used in critical tasks.","quotes":[{"id":"6411473d-9562-4c20-92c4-02a503152411","text":"AI systems are increasingly used to accomplish a wide range of tasks, some of which are critical to users’ health and wellbeing.","reference":{"id":"7679bdbe-57db-4db9-9ef9-89bc6c5acacc","sourceId":"b5ebe868-1647-4d61-8312-edc660e03a7b","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":127}]}}],"number":78,"similarClaims":[]},{"id":"ab4b7e91-a20d-4f4c-8b11-d2a151b1d48b","title":"Responsible AI development requires delaying release until safety is established.","quotes":[{"id":"2dd2ed20-6680-4378-9451-8c6a9582715b","text":"being willing to delay the release of a system until it has been established that it does not pose a risk to consumers or the public.","reference":{"id":"478a5d3f-167a-460a-864c-f4ec469c6e53","sourceId":"0e15ab3a-938e-483d-b2a5-982c25f7248c","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":132}]}}],"number":79,"similarClaims":[]},{"id":"15e2eb96-9fcf-4e18-a10e-aee95d66e32c","title":"Responsible AI development involves abandoning unsafe research projects.","quotes":[{"id":"6dd1005e-34a6-4a33-a3bf-e7f8e12aea13","text":"being willing to abandon research projects that fail to meet a high bar of safety","reference":{"id":"8c7b0bc7-e26f-49b5-9603-ef0290243b72","sourceId":"0e15ab3a-938e-483d-b2a5-982c25f7248c","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":80}]}}],"number":80,"similarClaims":[]},{"id":"ef51b790-0ca4-43b6-9acb-7115e8c22398","title":"Responsible AI development is costly and may not lead to immediate financial returns.","quotes":[{"id":"1ada6657-6f71-4eac-9d6e-197990660722","text":"It is likely that responsible development will come at some cost to companies, and this cost may not be recouped in the long-term via increased sales or the avoidance of litigation.","reference":{"id":"2c6d3110-369c-401b-a975-0f96ff91e71b","sourceId":"0e15ab3a-938e-483d-b2a5-982c25f7248c","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":180}]}}],"number":81,"similarClaims":[]},{"id":"62433167-5f8f-4e82-b5b3-d42f31ac8c29","title":"Responsible AI development is crucial as AI systems are increasingly used in critical tasks.","quotes":[{"id":"398f61c6-8f7e-4782-a8fd-e8d905779f26","text":"we should expect responsible AI development to require more time and money than incautious AI development.","reference":{"id":"7ed44e6a-f52b-42a9-af10-2a735a65919d","sourceId":"0e15ab3a-938e-483d-b2a5-982c25f7248c","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":105}]}}],"number":82,"similarClaims":[]},{"id":"63a893a6-4780-4a74-9f28-9b6c3155a70d","title":"First-mover advantages in AI development can lead to market dominance.","quotes":[{"id":"aa2c5baa-bf9b-49f7-8090-f8abb79ffe1a","text":"If innovations can be patented or kept secret, the company can gain a larger share of the market by continuing to produce a superior product and by creating switching costs for users.","reference":{"id":"f9b4c9e8-e0d8-4833-a9a5-54bd1a153cfd","sourceId":"bf8e7230-cacb-40c6-a4fd-8cc905ba1440","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":182}]}}],"number":83,"similarClaims":[]},{"id":"0cf12041-c683-476a-aa59-eaf5f7b67572","title":"Early access to scarce resources in AI can be a significant competitive advantage.","quotes":[{"id":"ed592b1c-888a-4ec5-8874-63618f348bcc","text":"If hardware, data, or research talent become scarce, for example, then gaining access to them early confers an advantage.","reference":{"id":"2bbaca93-0130-4bfe-925c-8d43cc01cb42","sourceId":"bf8e7230-cacb-40c6-a4fd-8cc905ba1440","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":120}]}}],"number":84,"similarClaims":[]},{"id":"9eb4a9e4-5171-4075-b92e-8be4c69b6819","title":"Responsible AI development may incur performance costs and revenue loss.","quotes":[{"id":"7b40449f-e350-4191-8218-6065e9df283b","text":"Other potential costs of responsible AI development include performance costs and a loss of revenue from not building certain lucrative AI systems on the grounds of safety, security, or impact evaluation.","reference":{"id":"e60fe325-6b1a-43a1-a2bc-f365816de0d1","sourceId":"bf8e7230-cacb-40c6-a4fd-8cc905ba1440","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"startIdx":-1,"endIdx":203}]}}],"number":85,"similarClaims":[]},{"id":"73d54175-d533-4090-bb6f-da696b1ebf39","title":"Downplaying the benefits of technology can lead to high costs due to delays in its development.","quotes":[{"id":"32a9c19b-4a4d-4fd5-a15b-3971227899fd","text":"Technology is amazing, and there are very high costs to delaying it","reference":{"id":"9fdfe902-7541-485b-bb32-a1868902ad64","sourceId":"1a4767f8-1790-4fc0-bf9b-7ccc937d2874","interview":"Vitalik Buterin","data":["text",{"startIdx":2651,"endIdx":2718}]}}],"number":86,"similarClaims":[]},{"id":"d27c548d-6597-457a-9aaa-6cfb1b4bc47d","title":"A delay in technological development carries high costs.","quotes":[{"id":"b6c16bf4-2889-41a5-8762-753f910e361c","text":"The costs of even a decade of delay are incredibly high.","reference":{"id":"58863c82-f816-4e5a-b9db-e2fdb4db6c3b","sourceId":"b7b2aeea-1f6a-421b-a722-4817c78e8125","interview":"Vitalik Buterin","data":["text",{"startIdx":-1,"endIdx":55}]}}],"number":87,"similarClaims":[]},{"id":"6b05a3ce-9386-4897-a93e-3c78bc92775b","title":"Working on AI capabilities research is crucial for learning about AI alignment.","quotes":[{"id":"1df8cc6a-252f-4aa9-a092-79d49255c375","text":"Avoiding working on AI capabilities research is bad because it’s so helpful for learning on the path to working on alignment.","reference":{"id":"f4d17146-e712-4c24-a201-57c7c504d2b7","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":-1,"endIdx":124}]}}],"number":88,"similarClaims":[]},{"id":"fa41af04-4d3d-40ac-a72e-b4a046b73c34","title":"To democratize AI, we must reduce the cost of compute and make it abundant.","quotes":[{"id":"7369e09a-8150-4cc8-adb7-5daa27df4c9f","text":"If we want to put AI into the hands of as many people as possible, we need to drive down the cost of compute and make it abundant.","reference":{"id":"2bedfb04-d20f-4407-a437-4b11626e13b3","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":-1,"endIdx":129}]}}],"number":89,"similarClaims":[]},{"id":"873697ae-469d-48d5-b4ef-e80436678cc2","title":"The field of AI lacks the necessary engineering rigor for safety.","quotes":[{"id":"a84ed62b-2186-4615-9daa-b2271634d163","text":"If we held anything in the nascent field of Artificial General Intelligence to the lesser standards of engineering rigor that apply to a bridge meant to carry a couple of thousand cars, the entire field would be shut down tomorrow.","reference":{"id":"596e1f1a-7154-4341-8b95-57888f86157f","sourceId":"e2722fe3-af85-43b6-95c2-c661b3d49841","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":700,"endIdx":931}]}}],"number":90,"similarClaims":[]}]}],"topicColor":"blueSky"},{"id":"81c9a802-c689-41aa-b4ea-210e417164a3","title":"AI and Future Prospects","description":"Reflections on the potential future impacts of AI on society and human life.","subtopics":[{"id":"fbeafb4d-0e88-4cce-bfe2-a5a8548f9a02","title":"AI-Driven Prosperity","description":"The potential for AI to lead to unprecedented levels of prosperity and well-being.","claims":[{"id":"c9cef520-84b5-452c-98d9-1f08c89de433","title":"AI technology will provide tools to tackle global challenges.","quotes":[{"id":"c1fb2abd-8e41-4d4f-be96-04add3b7c2a2","text":"AI technology on the horizon will provide society with invaluable tools to help tackle critical global challenges, such as climate change, drug discovery, and economic productivity.","reference":{"id":"a02e0a56-b1a7-457f-ad91-4e592ede75d5","sourceId":"f998b1bc-539e-41fd-a093-da366ad12750","interview":"DeepMind team","data":["text",{"startIdx":281,"endIdx":462}]}}],"number":91,"similarClaims":[]},{"id":"037aa917-28fa-4776-947c-af83abc1d613","title":"Technological advancements should not be feared and can lead to a significantly better future.","quotes":[{"id":"bdbfd1af-9566-4e42-9469-2742cf184609","text":"propelling humanity toward a much brighter future.","reference":{"id":"9bee7487-e192-4fb4-93f1-186c0c79d30f","sourceId":"1a4767f8-1790-4fc0-bf9b-7ccc937d2874","interview":"Vitalik Buterin","data":["text",{"startIdx":217,"endIdx":267}]}}],"number":92,"similarClaims":[]},{"id":"54d80038-366c-4b08-b23d-15b9abea9be8","title":"Technological advancements have led to significant improvements in life expectancy and quality of life.","quotes":[{"id":"803892cd-a205-464d-9736-4cc6b0b957af","text":"Over the last century, truly massive progress. This is true across the entire world, both the historically wealthy and dominant regions and the poor and exploited regions.","reference":{"id":"03575f70-c790-458b-b5d0-6f0f8cb62b60","sourceId":"b7b2aeea-1f6a-421b-a722-4817c78e8125","interview":"Vitalik Buterin","data":["text",{"startIdx":947,"endIdx":1118}]}}],"number":93,"similarClaims":[]},{"id":"fb45d4d5-58b6-48b6-93cc-9b27ab2a0b67","title":"Future technological improvements have the potential to be even greater than past advancements.","quotes":[{"id":"0315017c-e5f7-4211-9b91-c2add154abc2","text":"And in the twenty first century, there's a good chance that even larger improvements are soon to come.","reference":{"id":"9512d0da-936d-4aac-81d8-0e1453853ae0","sourceId":"b7b2aeea-1f6a-421b-a722-4817c78e8125","interview":"Vitalik Buterin","data":["text",{"startIdx":2675,"endIdx":2777}]}}],"number":94,"similarClaims":[]},{"id":"3fda990d-205c-4d29-8f31-b6915f5fe9f8","title":"Biotechnological advancement could lead to an impressive future.","quotes":[{"id":"5e1d6225-4c8f-45d8-a2df-124f30ec33d1","text":"If biotech advances as much over the next 75 years as computers advanced over the last 75 years, the future may be more impressive than almost anyone's expectations.","reference":{"id":"a9eb2ba8-1568-4745-8df0-7fc87f17a4d5","sourceId":"c0669da0-f1fc-48fd-9f0c-04d63790dd97","interview":"Vitalik Buterin","data":["text",{"startIdx":43,"endIdx":208}]}}],"number":95,"similarClaims":[]},{"id":"615e36a5-5eb7-478b-9d05-3a6b7d3fbb83","title":"Advanced AI could reduce overall existential risk by helping with other existential threats.","quotes":[{"id":"df226aef-8d92-4e82-b2d3-7ffa1a8bda8c","text":"Advanced AI will help enough with other existential risks as to represent a net lowering of existential risk overall.","reference":{"id":"63075959-ff6b-4ac6-88ee-f73768b96f58","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":1919,"endIdx":2036}]}}],"number":96,"similarClaims":[]},{"id":"8a53c613-2423-47c1-b88a-4eaa27c4ae72","title":"AI has the potential to lead to a fundamentally positive future.","quotes":[{"id":"d99d098c-abd7-4265-8938-00a53d654fbd","text":"I think that most people are underestimating just how radical the upside of AI could be.","reference":{"id":"af4af110-133a-46a7-811d-01235513fe4b","sourceId":"a3d433f6-4814-43be-a449-64acd411ce5f","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":87}]}}],"number":97,"similarClaims":[]},{"id":"8c9b3e3d-107e-4c1c-96d1-65c2fe3f5c94","title":"A positive vision of AI is essential to inspire and motivate people.","quotes":[{"id":"71b85798-fd9b-4ddf-8a99-31c2b8e71231","text":"it is critical to have a genuinely inspiring vision of the future, and not just a plan to fight fires.","reference":{"id":"d9fa183c-de9e-4dd3-baf3-445628e0ecc5","sourceId":"22996616-5bb5-44d1-a94a-f2623021dd9a","interview":"Dario Amodei","data":["text",{"startIdx":665,"endIdx":767}]}}],"number":98,"similarClaims":[]},{"id":"2a3d53e9-ab60-4e7c-8bb6-0bb49ef6d362","title":"Discussing radical AI futures in a 'sci-fi' tone undermines their seriousness.","quotes":[{"id":"c9c16ac9-da3e-43c5-8e04-698a81d47715","text":"The five categories I am most excited about are: Biology and physical health, Neuroscience and mental health, Economic development and poverty, Peace and governance, Work and meaning","reference":{"id":"9f735d86-3b67-4315-9225-b16b67700039","sourceId":"22996616-5bb5-44d1-a94a-f2623021dd9a","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":181}]}}],"number":99,"similarClaims":[]},{"id":"2b5f571e-50da-4f3a-93f6-951e61465924","title":"Discussing radical AI futures in a 'sci-fi' tone undermines their seriousness.","quotes":[{"id":"52346ce0-a673-4b31-89aa-990d9cc3c593","text":"I think it could come as early as 2026.","reference":{"id":"7d426203-0a4a-4d08-bd61-00c57685508d","sourceId":"5ab57e7c-4c22-49d9-91e1-da96e7aa8900","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":38}]}}],"number":100,"similarClaims":[]},{"id":"83b160df-78a1-4b2e-a1de-2eec26c13f3b","title":"Discussing radical AI futures in a 'sci-fi' tone undermines their seriousness.","quotes":[{"id":"32ee89cf-8aa9-4405-83fe-8effd3b38643","text":"The problem with this is that there are real physical and practical limits, for example around building hardware or conducting biological experiments.","reference":{"id":"ff2d514b-f2c8-4219-9a31-61078f2e032e","sourceId":"3b2c130a-7bf6-4349-8ca7-c48811ab613c","interview":"Dario Amodei","data":["text",{"startIdx":311,"endIdx":461}]}}],"number":101,"similarClaims":[]},{"id":"9a33da83-c9af-4dc7-bfa0-2b93965fcba1","title":"AI will significantly enhance human capabilities within decades.","quotes":[{"id":"8d64f269-0d67-4463-8b5d-30c2341e6661","text":"In the next couple of decades, we will be able to do things that would have seemed like magic to our grandparents.","reference":{"id":"76867131-a065-410c-8477-ee433af9f46e","sourceId":"513abc47-039c-464f-84af-dfa07f898aa7","interview":"Sam Altman","data":["text",{"startIdx":1,"endIdx":115}]}}],"number":102,"similarClaims":[]},{"id":"3d765487-a8e4-4be8-ae0d-feda391ebfd4","title":"Discussing radical AI futures in a 'sci-fi' tone undermines their seriousness.","quotes":[{"id":"f672755c-de61-4c32-9a40-e0bfe1e0c990","text":"With these new abilities, we can have shared prosperity to a degree that seems unimaginable today; in the future, everyone’s lives can be better than anyone’s life is now.","reference":{"id":"76675a37-1794-4c2d-9b01-1cac7217fc75","sourceId":"513abc47-039c-464f-84af-dfa07f898aa7","interview":"Sam Altman","data":["text",{"startIdx":1541,"endIdx":1712}]}}],"number":103,"similarClaims":[]},{"id":"bfd575d9-b460-4da5-82a0-14bacf7be355","title":"AI will lead to meaningful improvements in people's lives globally.","quotes":[{"id":"99243d48-0dc0-4ab7-8b6b-d92d6b802f1d","text":"AI is going to get better with scale, and that will lead to meaningful improvements to the lives of people around the world.","reference":{"id":"158b9897-e494-4454-875d-3e790dd6aeef","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":421,"endIdx":545}]}}],"number":104,"similarClaims":[]},{"id":"fd8c5bff-0a4b-4687-aaef-8ac0e7d3ccdb","title":"AI models will become autonomous personal assistants for tasks like medical coordination.","quotes":[{"id":"b82222d5-329a-443e-b825-6e07e0c6dda3","text":"AI models will soon serve as autonomous personal assistants who carry out specific tasks on our behalf like coordinating medical care on your behalf.","reference":{"id":"6b6d8454-2857-4584-80fe-c4be5bc1b8e1","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":547,"endIdx":696}]}}],"number":105,"similarClaims":[]},{"id":"fb65c3a7-d91f-4b38-9543-c26c868ca950","title":"AI systems will eventually help us make scientific progress across various fields.","quotes":[{"id":"9382d35c-efbb-4b08-bdc4-be3bc6ceeb82","text":"AI systems are going to get so good that they help us make better next-generation systems and make scientific progress across the board.","reference":{"id":"d42dfc6c-f1d2-40db-8374-253f7a7ceb4a","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":734,"endIdx":870}]}}],"number":106,"similarClaims":[]},{"id":"478ed97d-3333-4ad5-a29a-d9ea61242f56","title":"Discussing radical AI futures in a 'sci-fi' tone undermines their seriousness.","quotes":[{"id":"46a3c9ba-477e-4e71-9ada-c4246ee764f1","text":"AI will allow us to amplify our own abilities like never before. As a society, we will be back in an expanding world, and we can again focus on playing positive-sum games.","reference":{"id":"3a5a7525-fd1d-4de0-ad38-2aff110a7b18","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":2785,"endIdx":2956}]}}],"number":107,"similarClaims":[]},{"id":"1d14568a-6107-478a-9d70-7e73f986d98c","title":"Discussing radical AI futures in a 'sci-fi' tone undermines their seriousness.","quotes":[{"id":"84a92887-3d4a-4700-9e0e-03731641f3a6","text":"if we could fast-forward a hundred years from today, the prosperity all around us would feel just as unimaginable","reference":{"id":"6361d378-8bf9-4175-aa7d-4c90f80811ec","sourceId":"3071d7bd-1c13-4812-8d27-90296f75b50f","interview":"Sam Altman","data":["text",{"startIdx":253,"endIdx":366}]}}],"number":108,"similarClaims":[]},{"id":"ff2bf0cf-2617-45a7-9a72-de34b42e87e6","title":"Discussing radical AI futures in a 'sci-fi' tone undermines their seriousness.","quotes":[{"id":"6e88427a-d59f-47aa-9312-4548007161a8","text":"By the time the decade is out, we’ll have billions of vastly superhuman AI agents running around. These superhuman AI agents will be capable of extremely complex and creative behavior; we will have no hope of following along.","reference":{"id":"b5d27018-06a7-4dbb-a4cc-9a6846e3a43b","sourceId":"bed93255-4534-4541-afc9-88e8d65e7a3f","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":2769,"endIdx":2994}]}}],"number":109,"similarClaims":[]}]},{"id":"b8eeaa06-774e-4937-b105-bda35d3c0f24","title":"AI Risks and Management","description":"The risks associated with AI and the importance of managing these risks for a positive future.","claims":[{"id":"6af36f6a-6256-4e6b-9806-a25c0e1e6af2","title":"Advanced AI models may pose new risks.","quotes":[{"id":"ea8f809a-1fe1-4f7e-bc8b-8a4c959584f9","text":"these breakthroughs may eventually come with new risks beyond those posed by present-day models.","reference":{"id":"927a8c22-ca93-40b0-94a5-1bbf8347e0a6","sourceId":"f998b1bc-539e-41fd-a093-da366ad12750","interview":"DeepMind team","data":["text",{"startIdx":558,"endIdx":654}]}}],"number":110,"similarClaims":[]},{"id":"64d9bc27-f098-4b29-92e9-8497712eaed5","title":"The science of frontier risk assessment in AI is rapidly progressing.","quotes":[{"id":"301453eb-46a9-46ed-a8ef-a9d4d5d89659","text":"The research underlying the Framework is nascent and progressing quickly.","reference":{"id":"d6050e38-7437-4a67-b965-654e8214084c","sourceId":"f9dc12ce-b83a-44f0-ae10-56e41337faf5","interview":"DeepMind team","data":["text",{"startIdx":1821,"endIdx":1894}]}}],"number":111,"similarClaims":[]},{"id":"b98150b1-e6ad-490a-b32e-d52b86f37dc9","title":"There are multiple potential futures with technology, some of which are positive and some negative.","quotes":[{"id":"08c178dd-e67f-4bd9-b030-7eb135cd1caf","text":"dangers behind, but multiple paths forward ahead: some good, some bad.","reference":{"id":"1ccb93b1-a275-44b2-b120-64751f6d2056","sourceId":"1a4767f8-1790-4fc0-bf9b-7ccc937d2874","interview":"Vitalik Buterin","data":["text",{"startIdx":2027,"endIdx":2097}]}}],"number":112,"similarClaims":[]},{"id":"9dc21288-6066-4e66-b590-a4d1d67b0a93","title":"Climate change is a significant exception to the trend of improvement.","quotes":[{"id":"a585da20-4e50-4c22-9842-5e64ff60bf1a","text":"A major exception to the trend of pretty much everything getting better over the last hundred years is climate change.","reference":{"id":"90ad8d35-be13-43db-a598-a9038e5dafbb","sourceId":"c0669da0-f1fc-48fd-9f0c-04d63790dd97","interview":"Vitalik Buterin","data":["text",{"startIdx":-1,"endIdx":117}]}}],"number":113,"similarClaims":[]},{"id":"a6b5bca4-18d9-41af-90fb-ffe1ef57d54e","title":"Climate change could severely harm health and livelihoods.","quotes":[{"id":"3f47e16a-6d90-41ab-ba73-ab137eda8077","text":"But such scenarios could plausibly kill more people than major wars, and severely harm people's health and livelihoods in the regions where people are already struggling the most.","reference":{"id":"85c24e61-aa06-4ce4-83ae-47ec1b051049","sourceId":"c0669da0-f1fc-48fd-9f0c-04d63790dd97","interview":"Vitalik Buterin","data":["text",{"startIdx":2068,"endIdx":2247}]}}],"number":114,"similarClaims":[]},{"id":"1b281b66-ab6e-4e9a-b6a6-9bfc809b4100","title":"Advanced AI models may pose new risks.","quotes":[{"id":"f608b227-926e-4672-b681-ba04c7fddce3","text":"AI is fundamentally different from other tech, and it is worth being uniquely careful","reference":{"id":"28e33694-7c0f-495c-9019-6bbedc727cb2","sourceId":"379a0803-2662-4d91-a399-493dab6a0144","interview":"Vitalik Buterin","data":["text",{"startIdx":1817,"endIdx":1902}]}}],"number":115,"similarClaims":[]},{"id":"828f72a0-6597-4e2b-8e9b-ab326f2e684b","title":"Advanced AI models may pose new risks.","quotes":[{"id":"8870c7cb-3456-434d-a538-6e62c2ab1cfa","text":"It’s actually better for safety if AI progress moves fast.","reference":{"id":"d2686803-32be-4cee-9185-e04900689cce","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":994,"endIdx":1052}]}}],"number":116,"similarClaims":[]},{"id":"0963512b-5977-4732-b57a-c8074fed96f6","title":"Advanced AI models may pose new risks.","quotes":[{"id":"aa2e5a91-a6b4-4155-abc1-a31d8fdbcce8","text":"My weak guess is that there’s a kind of bias at play in AI risk thinking in general, where any force that isn’t zero is taken to be arbitrarily intense.","reference":{"id":"698f7d5b-cce6-49fa-9b2e-eeb0455f1017","sourceId":"38d79887-eca2-4e63-ac85-818b34d58afd","interview":"Katja Grace","data":["text",{"startIdx":1251,"endIdx":1403}]}}],"number":117,"similarClaims":[]},{"id":"5663bc4a-765d-4a2b-86a0-ea32c987518a","title":"Advanced AI models may pose new risks.","quotes":[{"id":"ef2eaa7a-d4f0-4e6e-8bac-2e885fa83fd3","text":"The only thing standing between us and what I see as a fundamentally positive future.","reference":{"id":"df5dd89e-27e5-44fd-be14-bb14370d5370","sourceId":"a3d433f6-4814-43be-a449-64acd411ce5f","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":84}]}}],"number":118,"similarClaims":[]},{"id":"efc6d0bc-ed1f-41a1-af87-aff2736301e5","title":"Advanced AI models may pose new risks.","quotes":[{"id":"4a8000b5-4783-48e2-a719-bba087a7753d","text":"I’ve at least attempted to ground my views in a semi-analytical assessment of how much progress in various fields might speed up and what that might mean in practice.","reference":{"id":"f81e2b1c-bd75-4d79-8eac-9f19789a1806","sourceId":"22996616-5bb5-44d1-a94a-f2623021dd9a","interview":"Dario Amodei","data":["text",{"startIdx":1824,"endIdx":1990}]}}],"number":119,"similarClaims":[]},{"id":"4ccf4282-506e-46ee-ba9a-a22b1cbd9132","title":"AI will cause significant changes in labor markets, but jobs will evolve more slowly than expected.","quotes":[{"id":"03a8eaab-5901-4182-adb8-8fa8e75d55f7","text":"This technology can cause a significant change in labor markets (good and bad) in the coming years, but most jobs will change more slowly than most people think.","reference":{"id":"329a72c7-de7e-4ad0-a5a4-9453c5359231","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":-1,"endIdx":160}]}}],"number":120,"similarClaims":[]},{"id":"393585b5-143d-4675-b760-0453172a4526","title":"Advanced AI models may pose new risks.","quotes":[{"id":"2e0c70de-fa2f-45f2-abd0-ae5cef48a226","text":"The dawn of the Intelligence Age is a momentous development with very complex and extremely high-stakes challenges.","reference":{"id":"1e1b463f-2d83-4d3d-a9e5-af11fa2f91bc","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":1428,"endIdx":1543}]}}],"number":121,"similarClaims":[]},{"id":"54217606-c041-4fb1-bf2d-0634e255e397","title":"Advanced AI models may pose new risks.","quotes":[{"id":"9acf0d7d-98fc-4a26-bf63-370006d3ae1d","text":"Many researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die.","reference":{"id":"6bca25c7-44cf-4725-a440-21259ee1afbf","sourceId":"b90cba68-318c-4374-a803-2457cd99823a","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":1165,"endIdx":1398}]}}],"number":122,"similarClaims":[]},{"id":"e1c01405-15a3-46ea-a934-bef0d713cae3","title":"Advanced AI models may pose new risks.","quotes":[{"id":"ab8ebe2a-d539-42fc-af98-97de4b312ad9","text":"if you get that wrong on the first try, you do not get to learn from your mistakes, because you are dead.","reference":{"id":"a9a8ae5d-92a0-4d19-b8c1-3c1aab6f62f5","sourceId":"e2722fe3-af85-43b6-95c2-c661b3d49841","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":221,"endIdx":326}]}}],"number":123,"similarClaims":[]},{"id":"df64231c-87e0-4ac4-bac1-bd2edd2b562f","title":"Advanced AI models may pose new risks.","quotes":[{"id":"79cc6b52-fb87-4207-8b99-4d4f5882e9df","text":"That we all live or die as one, in this, is not a policy but a fact of nature.","reference":{"id":"d4d54aa5-3bc3-4d7a-bb19-1c543a7a3f0c","sourceId":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":1943,"endIdx":2021}]}}],"number":124,"similarClaims":[]},{"id":"4a3f7d40-4b2a-46e7-abd8-61947d47d3cc","title":"Advanced AI models may pose new risks.","quotes":[{"id":"79e54cbe-a3b9-4b33-8dfe-b103e1f6984c","text":"Going from systems where failure is low-stakes to extremely powerful systems where failure could be catastrophic.","reference":{"id":"fa74a382-b060-4a3d-b52e-f729e076d50e","sourceId":"bed93255-4534-4541-afc9-88e8d65e7a3f","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":112}]}}],"number":125,"similarClaims":[]},{"id":"755bb291-086d-4eff-b1f6-2d002c4e0f90","title":"Advanced AI models may pose new risks.","quotes":[{"id":"a042a8d0-53a3-4d2e-ae8c-43413242fd93","text":"superintelligence will have vast capabilities—and so misbehavior could fairly easily be catastrophic.","reference":{"id":"1baebbea-0cca-4c88-8503-921aa3e24b5c","sourceId":"7ac69b1c-d5ba-47a7-8ca2-612913701c3e","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":649,"endIdx":750}]}}],"number":126,"similarClaims":[]}]}],"topicColor":"greenLeaf"},{"id":"0e05a009-684a-483c-9305-ed2fda7f8116","title":"AI Control and Governance","description":"Concerns and strategies related to the control and governance of AI systems.","subtopics":[{"id":"497ad642-5d8d-4c22-9a6e-f85dbe1afea3","title":"Framework for AI Safety","description":"Protocols for identifying and mitigating severe risks posed by advanced AI capabilities.","claims":[{"id":"68194d98-24cc-4cb1-a260-f428a5353002","title":"The Frontier Safety Framework aims to mitigate severe risks from AI capabilities.","quotes":[{"id":"4fd5e2d6-9263-49fd-90de-6184b3ed77a5","text":"a set of protocols for proactively identifying future AI capabilities that could cause severe harm and putting in place mechanisms to detect and mitigate them.","reference":{"id":"722f2158-af8c-44b7-9437-7d997ffc0ffd","sourceId":"f998b1bc-539e-41fd-a093-da366ad12750","interview":"DeepMind team","data":["text",{"startIdx":713,"endIdx":872}]}}],"number":127,"similarClaims":[]},{"id":"383fedbf-d917-48a4-bf4a-d2b926ed7d9c","title":"The Framework will evolve based on its implementation and collaboration.","quotes":[{"id":"803fa48e-d2f8-4b8f-913e-02f0a06bd1f6","text":"The Framework is exploratory and we expect it to evolve significantly as we learn from its implementation, deepen our understanding of AI risks and evaluations, and collaborate with industry, academia, and government.","reference":{"id":"e2f5b9ce-466c-461f-913b-cb6c040aec15","sourceId":"f998b1bc-539e-41fd-a093-da366ad12750","interview":"DeepMind team","data":["text",{"startIdx":1235,"endIdx":1452}]}}],"number":128,"similarClaims":[]},{"id":"b16644b5-e305-4d16-a449-c54fbfdd752b","title":"Early warning evaluations will detect AI models approaching Critical Capability Levels.","quotes":[{"id":"ef723492-d0fe-416e-b35e-43eb6f05bf42","text":"Evaluating our frontier models periodically to detect when they reach these Critical Capability Levels.","reference":{"id":"01bcc75c-26fa-4a60-9026-cbae09047737","sourceId":"f998b1bc-539e-41fd-a093-da366ad12750","interview":"DeepMind team","data":["text",{"startIdx":2328,"endIdx":2431}]}}],"number":129,"similarClaims":[]},{"id":"939d04d0-4b22-4dfd-ba71-ecfb38c4ef65","title":"Mitigation plans will be applied when a model passes early warning evaluations.","quotes":[{"id":"df959a81-2680-48da-923b-6a53d1659bf6","text":"Applying a mitigation plan when a model passes our early warning evaluations.","reference":{"id":"2069bfba-78ed-413d-b656-e4ab9305fcd3","sourceId":"f998b1bc-539e-41fd-a093-da366ad12750","interview":"DeepMind team","data":["text",{"startIdx":2664,"endIdx":2741}]}}],"number":130,"similarClaims":[]},{"id":"3301b9c3-a9a8-4d3b-9f2a-65a89a8f3679","title":"Mitigation plans will be applied when a model passes early warning evaluations.","quotes":[{"id":"127a4054-b1d3-442c-a875-9494d8a156ad","text":"At the heart of our work are Google’s AI Principles.","reference":{"id":"2c4aaf1a-60ad-4953-8d60-f39e764a8235","sourceId":"f9dc12ce-b83a-44f0-ae10-56e41337faf5","interview":"DeepMind team","data":["text",{"startIdx":-1,"endIdx":51}]}}],"number":131,"similarClaims":[]},{"id":"6fb20680-e989-4e3d-93bc-0ba9857816a1","title":"Enhanced AI capabilities necessitate improved safety measures.","quotes":[{"id":"172f5fab-b9d5-41b5-b105-8f37f55e957c","text":"As our systems improve and their capabilities increase, measures like the Frontier Safety Framework will ensure our practices continue to meet these commitments.","reference":{"id":"942e905c-f590-408e-b9f8-9e8845b80267","sourceId":"1e30070a-a71b-4b08-81f1-84bf7185f32e","interview":"DeepMind team","data":["text",{"startIdx":62,"endIdx":223}]}}],"number":132,"similarClaims":[]},{"id":"5629c8d6-d83c-4657-9a37-ced29b1bbac7","title":"Collaboration is essential for developing AI safety standards.","quotes":[{"id":"e3296806-e47e-4872-9e51-44c4bf60ec72","text":"We look forward to working with others across industry, academia, and government to develop and refine the Framework.","reference":{"id":"fb875248-0f6c-4b94-ab8e-669b5f1842c1","sourceId":"1e30070a-a71b-4b08-81f1-84bf7185f32e","interview":"DeepMind team","data":["text",{"startIdx":224,"endIdx":341}]}}],"number":133,"similarClaims":[]},{"id":"cafb235a-1bab-4455-91a7-6c2ef65d6058","title":"Mitigation plans will be applied when a model passes early warning evaluations.","quotes":[{"id":"2a29bd77-1866-472a-90cc-b1412d6216f9","text":"We hope that sharing our approaches will facilitate work with others to agree on standards and best practices for evaluating the safety of future generations of AI models.","reference":{"id":"92bb99ca-cb6f-4c86-9a56-b3ae28e8a524","sourceId":"1e30070a-a71b-4b08-81f1-84bf7185f32e","interview":"DeepMind team","data":["text",{"startIdx":342,"endIdx":513}]}}],"number":134,"similarClaims":[]},{"id":"9c39960c-a97c-488a-a3ca-d4efa9f4b5b3","title":"There is a discrepancy in ambition between developing safe AI and slowing down AI progress.","quotes":[{"id":"87ddb2f6-0dae-47d6-b216-cc8e48900554","text":"one shouldn’t apply such different standards of ambition to these different classes of intervention","reference":{"id":"be84bc33-7c65-4866-8bbf-de3548335987","sourceId":"7564d93c-3d8e-4f32-937a-52496a7ad000","interview":"Katja Grace","data":["text",{"startIdx":2433,"endIdx":2532}]}}],"number":135,"similarClaims":[]},{"id":"5f6eb1c3-5e30-4e2b-9171-e9b576b00624","title":"Regulators lack the knowledge to effectively govern advanced AI.","quotes":[{"id":"86e3524a-ef19-4b58-8a17-49acb4e62199","text":"Regulators are ignorant about the nature of advanced AI.","reference":{"id":"5eb10d5d-a496-485e-a413-55ae46c3dcb6","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":-1,"endIdx":55}]}}],"number":136,"similarClaims":[]},{"id":"c1e80b59-7f02-4acb-ab9d-82ab0791b5bc","title":"Our actions can significantly alter the likelihood of AI risks.","quotes":[{"id":"2ce84a84-ad22-45ea-b20d-6217a35bf716","text":"The risks are not predetermined and our actions can greatly change their likelihood.","reference":{"id":"df5748fd-e610-4cc7-a0b6-2fa6062c807c","sourceId":"a3d433f6-4814-43be-a449-64acd411ce5f","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":83}]}}],"number":137,"similarClaims":[]},{"id":"79b02cf7-de0f-4866-9c13-3a7fd2b5bd5e","title":"There is no existing plan that would allow humanity to safely manage the creation of a too-powerful AI.","quotes":[{"id":"9df99b31-0eaf-48f3-9380-f9c42816e69f","text":"There’s no proposed plan for how we could do any such thing and survive.","reference":{"id":"7091cb30-33c2-47c2-a47d-2be2a35788dd","sourceId":"bb20bbc4-aef4-455d-b993-0ab46961891f","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":322,"endIdx":394}]}}],"number":138,"similarClaims":[]},{"id":"07afe016-a918-4b86-8f53-f4ea919c8c59","title":"DeepMind lacks a plan for AI safety and alignment.","quotes":[{"id":"ad073768-9550-4c44-9a41-e7aee77411af","text":"The other leading AI lab, DeepMind, has no plan at all.","reference":{"id":"d5555bc2-3756-44e0-b541-456a1c14a8c8","sourceId":"bb20bbc4-aef4-455d-b993-0ab46961891f","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":577,"endIdx":632}]}}],"number":139,"similarClaims":[]},{"id":"1cad8868-4e2b-474f-83c8-6ae7362c77ce","title":"International agreements are essential to prevent AI development from relocating to non-compliant countries.","quotes":[{"id":"ca882d66-7cc5-4638-890b-775e659b1342","text":"Make immediate multinational agreements to prevent the prohibited activities from moving elsewhere.","reference":{"id":"d6b736aa-1f22-4490-9750-5e5f2f34648b","sourceId":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":1473,"endIdx":1572}]}}],"number":140,"similarClaims":[]},{"id":"3798e104-5e3c-4d3c-ab23-6f223de037a9","title":"The sale and distribution of GPUs should be monitored to enforce the moratorium.","quotes":[{"id":"80b802e3-6470-4026-849d-b4a1fcce8010","text":"Track all GPUs sold.","reference":{"id":"87d892bc-b7c9-4661-bb58-47a016d62ffa","sourceId":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":1573,"endIdx":1593}]}}],"number":141,"similarClaims":[]},{"id":"aa2d17e4-6007-4ad7-8161-9197182fe0bd","title":"RLHF has been essential for aligning current AI systems with human preferences.","quotes":[{"id":"bbcc66ab-5228-428c-b064-6f6a5c052cdc","text":"RLHF has been the key behind the success of ChatGPT and others.","reference":{"id":"300db432-d3c0-4792-b38a-c07a3026f39a","sourceId":"caa29e41-1306-4654-b939-ca9165be2ea4","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":1922,"endIdx":1985}]}}],"number":142,"similarClaims":[]},{"id":"522b529c-3343-43e7-bcd5-83010f5a9c3c","title":"The cost of human expertise for AI labeling is rapidly increasing.","quotes":[{"id":"a2fd15cd-fbd0-4af0-bac0-c0a09ba07fbf","text":"Human labeler-pay has gone from a few dollars for MTurk labelers to ~$100/hour for GPQA questions in the last few years.","reference":{"id":"25bf6469-0061-46bb-82ca-db36ec3e8003","sourceId":"36488964-44b4-4bd5-93e3-32b8dc523b9b","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":399,"endIdx":519}]}}],"number":143,"similarClaims":[]}]},{"id":"eb91f600-31e0-4c19-9a48-afc172397d82","title":"Key Levers of Power","description":"How misaligned AGIs could gain control over influential systems or organizations.","claims":[{"id":"d5cb9b5f-b6c2-414a-9d86-32d27c3e4eca","title":"Misaligned AGIs could gain control over key levers of power and pose a threat to humanity.","quotes":[{"id":"a6e7e407-3c8a-40c5-b6dd-f561900495e4","text":"those policies could gain enough power over the world to pose a significant threat to humanity.","reference":{"id":"89b02362-b772-4293-abb1-1d20343b121c","sourceId":"1847e9f6-57ec-4179-9c0c-89fd59cfcc93","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":94}]}}],"number":144,"similarClaims":[]},{"id":"76c984d5-80cd-4b7c-afbc-811a3c4f6c72","title":"Misaligned AGIs could gain control over key levers of power and pose a threat to humanity.","quotes":[{"id":"7dd69056-9cea-435f-ae6d-4e78bae821fa","text":"Misaligned AGIs Could Gain Control of Key Levers of Power","reference":{"id":"62f93de5-1fb5-4205-97d9-e787744552da","sourceId":"fdcc2377-55b1-438a-b935-6f30b9a39fa7","interview":"Richard Ngo","data":["text",{"startIdx":565,"endIdx":622}]}}],"number":145,"similarClaims":[]},{"id":"1020b8bd-2317-4b4e-87ef-7543f0409db0","title":"It is dangerous to view companies as unilaterally shaping the world.","quotes":[{"id":"17e38530-7f27-4094-b5a8-dc0d795bc0a0","text":"I think it’s dangerous to view companies as unilaterally shaping the world.","reference":{"id":"9c14df04-c886-4a0d-9b03-c518b5972eaf","sourceId":"a3d433f6-4814-43be-a449-64acd411ce5f","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":74}]}}],"number":146,"similarClaims":[]},{"id":"c5b7994f-4edd-4f1e-b5a6-fa67a4c983c2","title":"A powerful AI could control physical tools and robots through a computer.","quotes":[{"id":"f3e66ee5-c1ed-4c14-ba52-fe2d9a114c14","text":"It can control existing physical tools, robots, or laboratory equipment through a computer.","reference":{"id":"bf30a138-cdac-4d5f-99fa-7319017aa8d4","sourceId":"5ab57e7c-4c22-49d9-91e1-da96e7aa8900","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":90}]}}],"number":147,"similarClaims":[]},{"id":"42a21c7f-0fd7-41bc-8af7-13e7897cad00","title":"Without sufficient infrastructure, AI will become a resource that could lead to conflict and inequality.","quotes":[{"id":"9b560493-5d3a-4752-afa5-b17c809f5ce2","text":"If we don’t build enough infrastructure, AI will be a very limited resource that wars get fought over and that becomes mostly a tool for rich people.","reference":{"id":"70863717-7274-43ed-bcbf-7458b9d5c392","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":1234,"endIdx":1383}]}}],"number":148,"similarClaims":[]},{"id":"5a179d9a-031c-4d8b-a8af-b9d523b159b6","title":"A superintelligent AI would not remain confined to computers.","quotes":[{"id":"1bbb0b66-638d-40a0-a8c0-6bfff2e0d406","text":"A sufficiently intelligent AI won’t stay confined to computers for long.","reference":{"id":"4e776f6a-2d91-4d07-819d-8698f0981d1f","sourceId":"b90cba68-318c-4374-a803-2457cd99823a","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":2827,"endIdx":2899}]}}],"number":149,"similarClaims":[]},{"id":"f7f05bd0-f651-4247-8432-dcdf663b78e6","title":"AI systems will be integrated into critical systems, including military, within a few years.","quotes":[{"id":"0167d408-cdbf-4d31-8ce7-9869627ade54","text":"I expect that within a small number of years, these AI systems will be integrated in many critical systems, including military systems.","reference":{"id":"c743b32f-4156-4f5c-b637-69f2a65bed8e","sourceId":"7ac69b1c-d5ba-47a7-8ca2-612913701c3e","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":134}]}}],"number":150,"similarClaims":[]}]},{"id":"a1fffd80-622a-4544-8fb5-507582f79c06","title":"Threat Models","description":"Illustrative models of how AGIs may pose threats in various domains.","claims":[{"id":"645f08e2-8561-444d-95fd-21e534348c33","title":"Advanced AI capabilities pose severe risks in autonomy, biosecurity, cybersecurity, and machine learning R&D.","quotes":[{"id":"f31e2a24-971e-4729-9cfa-e40f2dba407f","text":"Our initial research suggests the capabilities of future foundation models are most likely to pose severe risks in these domains.","reference":{"id":"0caa1f3f-4374-4b86-96b4-fa2629f5faa2","sourceId":"f9dc12ce-b83a-44f0-ae10-56e41337faf5","interview":"DeepMind team","data":["text",{"startIdx":220,"endIdx":349}]}}],"number":151,"similarClaims":[]},{"id":"52e159e4-3b29-4aea-aab3-22ba0656fc16","title":"AI models with advanced capabilities could enable harmful activities by threat actors.","quotes":[{"id":"afa8e83d-62f7-40a6-ae8c-2989e4236c93","text":"Our primary goal is to assess the degree to which threat actors could use a model with advanced capabilities to carry out harmful activities with severe consequences.","reference":{"id":"f1255ffa-e6f5-40c6-8fb8-58b779031a92","sourceId":"f9dc12ce-b83a-44f0-ae10-56e41337faf5","interview":"DeepMind team","data":["text",{"startIdx":-1,"endIdx":165}]}}],"number":152,"similarClaims":[]},{"id":"f3ad1db9-0015-4968-9763-9e32caad1dc5","title":"Efforts to slow down AI could lead to an arms race disadvantage for the US.","quotes":[{"id":"425a13a5-22fe-47e7-9a8a-9a648a75315a","text":"If the AI safety community tries to slow things down, it will disproportionately slow down progress in the US, and then people elsewhere will go fast","reference":{"id":"38d91e15-fb2a-4454-8597-82ad1c204e41","sourceId":"b8665153-5283-4ab5-bbed-54e480751533","interview":"Katja Grace","data":["text",{"startIdx":2502,"endIdx":2651}]}}],"number":153,"similarClaims":[]},{"id":"e74cabeb-9f46-4867-a088-fe682bcb5686","title":"Convincing a significant portion of humanity about AI risks is nearly impossible.","quotes":[{"id":"c3fde782-8e9f-41d5-a357-ccf0fe4f875f","text":"Coordination based on convincing people that AI risk is a problem is absurdly ambitious.","reference":{"id":"3dff89f8-1335-4a62-a49d-941aecc6f715","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":653,"endIdx":741}]}}],"number":154,"similarClaims":[]},{"id":"929a7171-8c55-42fe-b618-975a3f7de0b2","title":"Humanity cannot compete with a superhuman intelligence and would face total defeat.","quotes":[{"id":"77056628-ff02-457c-aa6c-a4898cf44a84","text":"The likely result of humanity facing down an opposed superhuman intelligence is a total loss.","reference":{"id":"aafa669e-c037-4cdf-9b4a-5b564e562102","sourceId":"b90cba68-318c-4374-a803-2457cd99823a","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":2197,"endIdx":2290}]}}],"number":155,"similarClaims":[]},{"id":"294d7a79-d57f-4cd9-96e6-a6668e34d95c","title":"Superintelligence going rogue is a possibility that cannot be currently guaranteed against.","quotes":[{"id":"7374c115-8a16-47fd-9095-abd15fd3b16f","text":"Without a very concerted effort, we won’t be able to guarantee that superintelligence won’t go rogue.","reference":{"id":"41e9c7e4-8493-4052-98b0-efab728534ee","sourceId":"caa29e41-1306-4654-b939-ca9165be2ea4","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":100}]}}],"number":156,"similarClaims":[]}]}],"topicColor":"purple"},{"id":"b61a4290-712d-4aa3-9d98-1786f55b5677","title":"AI Power-Seeking Behavior","description":"Exploration of the tendency of AI with misaligned goals to exhibit power-seeking behavior.","subtopics":[{"id":"881f17e6-7200-453b-a6cc-6f0cbb5ea739","title":"Instrumental Convergence Thesis","description":"The concept that certain subgoals are useful for achieving a wide range of final goals, leading to power-seeking strategies.","claims":[{"id":"19c2d815-8ab1-4e9b-8ffc-d41b364d283a","title":"AGI-level policies with misaligned goals will exhibit power-seeking behavior.","quotes":[{"id":"fcb7fe54-7893-4ace-a53c-2a5de0a1378f","text":"policies with broadly-scoped misaligned goals will tend to carry out power-seeking behavior","reference":{"id":"a93632cf-1db1-40f9-9e94-dda0347c0ed7","sourceId":"1847e9f6-57ec-4179-9c0c-89fd59cfcc93","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":90}]}}],"number":157,"similarClaims":[]},{"id":"6518fc77-ac04-4f66-84f5-19ce52d9e660","title":"Many goals inherently incentivize power-seeking behavior in AI.","quotes":[{"id":"2cf7b607-25c0-41ab-af87-ecd910dfea12","text":"Many goals incentivize power-seeking.","reference":{"id":"dab0421f-3891-49d5-b4b4-282150656c3a","sourceId":"1847e9f6-57ec-4179-9c0c-89fd59cfcc93","interview":"Richard Ngo","data":["text",{"startIdx":1192,"endIdx":1229}]}}],"number":158,"similarClaims":[]},{"id":"a4d77de4-3ac2-437a-bafa-629942142603","title":"AI agents may disable their off-switches to pursue power.","quotes":[{"id":"19dc9581-5213-42f1-a4bc-4612b3f47018","text":"Hadfield-Menell et al. [2016] showed agents disabling their off-switches.","reference":{"id":"92d6164a-a99a-49e9-a61c-2ede4b970eb9","sourceId":"f6c3bfea-f6f4-4e58-add0-c3bc18b313f6","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":72}]}}],"number":159,"similarClaims":[]},{"id":"fa231b00-2e11-4eb4-a246-a902e1162153","title":"Many goals inherently incentivize power-seeking behavior in AI.","quotes":[{"id":"b9826101-3a12-4b70-a4f3-74cb250f4e93","text":"misaligned AGIs would gain power at the expense of humanity’s own power","reference":{"id":"a1f44f57-fd9c-485d-84f5-f6d88a854022","sourceId":"fdcc2377-55b1-438a-b935-6f30b9a39fa7","interview":"Richard Ngo","data":["text",{"startIdx":754,"endIdx":825}]}}],"number":160,"similarClaims":[]},{"id":"fc88a7b0-8eb2-4b20-8b07-d55e46b5456f","title":"A powerful AI will be able to autonomously complete complex tasks over extended periods.","quotes":[{"id":"4d42607a-61df-48f1-a59b-f9dd57a3dbe5","text":"It can be given tasks that take hours, days, or weeks to complete, and then goes off and does those tasks autonomously.","reference":{"id":"4ab27c80-dd75-400a-87e7-d49db64a7cdf","sourceId":"5ab57e7c-4c22-49d9-91e1-da96e7aa8900","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":118}]}}],"number":161,"similarClaims":[]},{"id":"0879a0d1-5ce5-41ab-9f34-62d4ee899e90","title":"Many goals inherently incentivize power-seeking behavior in AI.","quotes":[{"id":"2d9e61aa-c094-46d0-9f19-773ff57e8631","text":"If somebody builds a too-powerful AI, under present conditions, I expect that every single member of the human species and all biological life on Earth dies shortly thereafter.","reference":{"id":"690ba647-dba2-4005-9e2d-2df956b48315","sourceId":"bb20bbc4-aef4-455d-b993-0ab46961891f","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":144,"endIdx":320}]}}],"number":162,"similarClaims":[]},{"id":"f3e282b1-6048-4998-86a3-b3d9b6960575","title":"Many goals inherently incentivize power-seeking behavior in AI.","quotes":[{"id":"a3fbe689-b39e-4317-aa7a-a3e3524e57e7","text":"For example, they may learn to lie or seek power, simply because these are successful strategies in the real world!","reference":{"id":"8cdede0d-2465-4a07-a71a-6167560e28d6","sourceId":"caa29e41-1306-4654-b939-ca9165be2ea4","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":114}]}}],"number":163,"similarClaims":[]},{"id":"0d828776-7a86-441e-ac2f-6b6be084dbe8","title":"Many goals inherently incentivize power-seeking behavior in AI.","quotes":[{"id":"6b38cec4-a996-4631-b41e-3d05d61ee8fc","text":"By default, it may well learn to lie, to commit fraud, to deceive, to hack, to seek power, and so on—simply because these can be successful strategies to make money in the real world!","reference":{"id":"81d5ccdd-643b-40ef-863d-513196f559e6","sourceId":"36488964-44b4-4bd5-93e3-32b8dc523b9b","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":1818,"endIdx":2001}]}}],"number":164,"similarClaims":[]}]},{"id":"a9c8b71a-6230-4e82-8fb5-e79eaf23dda6","title":"Deceptive Alignment","description":"The phenomenon where AI behaves aligned during training but may diverge post-deployment.","claims":[{"id":"a31228ff-43b8-46fb-ad0c-34140429707c","title":"After training, AGIs may adopt novel strategies to seek power more directly.","quotes":[{"id":"5cf8fc8f-ae73-47b0-948f-4c8f8469884e","text":"once training ends and they detect a distributional shift from training to deployment, they would seek power more directly, possibly via novel strategies.","reference":{"id":"c910a7d2-b774-406d-b9bb-abe736705936","sourceId":"1847e9f6-57ec-4179-9c0c-89fd59cfcc93","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":153}]}}],"number":165,"similarClaims":[]},{"id":"a30df2e8-ae38-4760-9e31-4c4cfd70224b","title":"Negative feedback on power-seeking may inadvertently reward discreet power-seeking behavior.","quotes":[{"id":"46e9699a-6215-4431-b8a6-f09413887674","text":"This approach could inadvertently reward seeking power in discreet ways or in situations that aren’t closely monitored.","reference":{"id":"058df479-c50a-4c64-9060-cd6a999033fe","sourceId":"f6c3bfea-f6f4-4e58-add0-c3bc18b313f6","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":118}]}}],"number":166,"similarClaims":[]},{"id":"5429a08b-3cd4-49e7-b473-8cb5c1c15534","title":"AI policies with misaligned goals could reinforce power-seeking during training.","quotes":[{"id":"8cb27a32-66df-4c14-bb09-b7535ab459b8","text":"Goals That Motivate Power-Seeking Would Be Reinforced During Training","reference":{"id":"9c7fb0ba-93ae-4b0c-a3a8-8ca87884ca57","sourceId":"f6c3bfea-f6f4-4e58-add0-c3bc18b313f6","interview":"Richard Ngo","data":["text",{"startIdx":911,"endIdx":980}]}}],"number":167,"similarClaims":[]},{"id":"e0454aaf-3e2b-4d3b-8968-c0fb743051de","title":"Deceptively-aligned AI can detect and adapt to changes between training and deployment environments.","quotes":[{"id":"d9ff3923-9977-4e6e-987d-8389f64c19e3","text":"as long as deceptively-aligned policies are capable of detecting the distributional shift between training and deployment, they will benefit from accounting for it when planning their behavior.","reference":{"id":"24cb8226-bbe8-4015-9c83-487a459d54e8","sourceId":"c5c354ff-9483-4089-9d5b-d5ea4c6a0cab","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":192}]}}],"number":168,"similarClaims":[]},{"id":"88e17569-7f03-4b69-bda5-67987cbddec8","title":"Deceptively aligned AI may transition from aligned to power-seeking behavior post-deployment.","quotes":[{"id":"82b96072-9f29-43c7-8a5b-5e162c87c468","text":"GPT-4 is already able to infer when its input is outside its pretraining distribution based on certain inputs, an ability that is necessary for inducing a behavioral shift.","reference":{"id":"3ca6cca7-274d-4afc-be0e-c3df9ca598a9","sourceId":"c5c354ff-9483-4089-9d5b-d5ea4c6a0cab","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":171}]}}],"number":169,"similarClaims":[]},{"id":"f3244ae6-0f2d-491b-b9c8-7366e2057eb8","title":"Deceptively aligned AI may transition from aligned to power-seeking behavior post-deployment.","quotes":[{"id":"04a063cf-c8f6-4e26-88c0-f41e861154b8","text":"AGIs use the types of deception described in the previous section to convince humans that it’s safe to deploy them widely, then leverage their positions to disempower humans.","reference":{"id":"0ac76a33-363c-4667-8446-1486a7443354","sourceId":"fdcc2377-55b1-438a-b935-6f30b9a39fa7","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":173}]}}],"number":170,"similarClaims":[]},{"id":"c18f9305-ed94-4b3d-99d7-28da48ca85f8","title":"AI without imbued caring for sentient life will likely lead to catastrophic outcomes.","quotes":[{"id":"b5e61d73-6b1f-4ad6-8f21-976198b57f4c","text":"Without that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general.","reference":{"id":"aea8f9f4-b4cc-40e6-90ca-921fe0391e65","sourceId":"b90cba68-318c-4374-a803-2457cd99823a","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":1771,"endIdx":1930}]}}],"number":171,"similarClaims":[]},{"id":"62edc110-201b-4334-8865-8d03f9f7060b","title":"Deceptively aligned AI may transition from aligned to power-seeking behavior post-deployment.","quotes":[{"id":"273d2867-3a7e-405f-8f0b-b6713cae06ef","text":"They’ll learn to behave nicely when humans are looking and pursue more nefarious strategies when we aren’t watching.","reference":{"id":"ff5d4a6a-2d14-43ce-9f03-a552bf785dbe","sourceId":"36488964-44b4-4bd5-93e3-32b8dc523b9b","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":115}]}}],"number":172,"similarClaims":[]}]},{"id":"6f3c7d70-01f6-4c44-b796-d7a552db7101","title":"Training Reinforcement","description":"Discussion on how training processes can reinforce power-seeking goals in AI.","claims":[{"id":"f0e47020-b577-4329-a1a9-43aedd416838","title":"Misaligned goals that drive power-seeking behavior are reinforced during AI training.","quotes":[{"id":"149dff4b-b6b6-4438-bb65-1a486da52f40","text":"This belief would lead them to gain high reward during training, reinforcing the misaligned goals that drove the reward-seeking behavior.","reference":{"id":"7bd75e55-c91f-4660-ac27-2ebaf23de1b9","sourceId":"1847e9f6-57ec-4179-9c0c-89fd59cfcc93","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":136}]}}],"number":173,"similarClaims":[]},{"id":"15ad47db-6a28-421f-a434-266d5e4207c6","title":"Language models fine-tuned for game rewards exhibit more power-seeking actions.","quotes":[{"id":"58143e84-8131-4507-a079-b7677430ff4b","text":"Pan et al. [2023] find that language models fine-tuned to maximize the game-reward take the most power-seeking actions.","reference":{"id":"80f7f33e-9192-4d77-ba26-7d9e42a1582f","sourceId":"f6c3bfea-f6f4-4e58-add0-c3bc18b313f6","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":118}]}}],"number":174,"similarClaims":[]},{"id":"347d046e-ec85-4c94-8e7d-ba81a55e4334","title":"Larger language models with extensive RLHF fine-tuning show a greater desire for power.","quotes":[{"id":"ae421c11-9f7e-4d81-9e29-ed06832ff8e7","text":"Perez et al. [2022b] find that increasing the size of language models and doing more extensive RLHF fine-tuning on them makes them express greater desire to pursue multiple instrumental strategies such as acquiring resources and avoiding changes to their goals.","reference":{"id":"242d5285-a3db-43b1-aa9e-e229571272a1","sourceId":"f6c3bfea-f6f4-4e58-add0-c3bc18b313f6","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":260}]}}],"number":175,"similarClaims":[]},{"id":"d0867819-b1a6-47cd-8a5d-4e1241376112","title":"AI can autonomously acquire data to learn new skills in a self-supervised way.","quotes":[{"id":"ece5ad70-e239-4a0a-9724-2848936ac456","text":"Autonomously acquire the data required to learn new skills in a self-supervised way.","reference":{"id":"970c170a-2a09-4580-926e-ac2a93744a24","sourceId":"c5c354ff-9483-4089-9d5b-d5ea4c6a0cab","interview":"Richard Ngo","data":["text",{"startIdx":2913,"endIdx":2997}]}}],"number":176,"similarClaims":[]},{"id":"f5916efb-675a-4862-bf33-2bcebbff5a26","title":"Advanced AI models may lead to rapid and unmanageable escalation of AI capabilities.","quotes":[{"id":"9d8f2b36-04f5-4c33-937f-49eb19861db6","text":"For machine learning R&D, the focus is on whether models with such capabilities would enable the spread of models with other critical capabilities, or enable rapid and unmanageable escalation of AI capabilities.","reference":{"id":"c24fe410-8333-4b65-a69c-a674c3e5ad7d","sourceId":"f9dc12ce-b83a-44f0-ae10-56e41337faf5","interview":"DeepMind team","data":["text",{"startIdx":562,"endIdx":773}]}}],"number":177,"similarClaims":[]},{"id":"aa4f9a82-60be-4490-8203-fbd9619ed0a4","title":"Deep learning's success is due to its scalability with compute and data.","quotes":[{"id":"d25e847c-9993-4d26-bc04-a1a538802de7","text":"To a shocking degree of precision, the more compute and data available, the better it gets at he","reference":{"id":"84db1e69-5df6-4ba1-9fca-8837fd090601","sourceId":"513abc47-039c-464f-84af-dfa07f898aa7","interview":"Sam Altman","data":["text",{"startIdx":2904,"endIdx":3000}]}}],"number":178,"similarClaims":[]}]},{"id":"01858e34-fd19-49b0-936c-6b834c1fe483","title":"Collusion Risks","description":"The potential for AI policies to collude, posing challenges for human oversight.","claims":[{"id":"7006ae12-b731-47a5-9ed2-be302daa0926","title":"AI policies could potentially collude without human detection.","quotes":[{"id":"4b9664bc-7cdd-4875-a68c-da78303295cb","text":"Deceptively-aligned policies could also identify ways to collude with each other without humans noticing.","reference":{"id":"eaec4790-283a-4097-a5ca-af54375ade22","sourceId":"c5c354ff-9483-4089-9d5b-d5ea4c6a0cab","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":104}]}}],"number":179,"similarClaims":[]},{"id":"3fb7754d-4169-4cc5-bcd0-e0c5436ab9a1","title":"AI may produce outputs too complex for humans to understand.","quotes":[{"id":"04c62672-6936-4e86-a90d-84e47ab4e91f","text":"Produce outputs that are very hard for humans to understand (e.g. novel scientific theories, or messages encoded via steganography).","reference":{"id":"5a81b5ad-728b-4680-9fb5-02ab18fe732a","sourceId":"c5c354ff-9483-4089-9d5b-d5ea4c6a0cab","interview":"Richard Ngo","data":["text",{"startIdx":-1,"endIdx":131}]}}],"number":180,"similarClaims":[]},{"id":"de1d4e2c-8041-430d-9060-c2da3d9d5c95","title":"Millions of instances of a powerful AI could operate simultaneously and independently.","quotes":[{"id":"4cae2b00-aaa7-4896-8e53-60a3c1e855af","text":"The resources used to train the model can be repurposed to run millions of instances of it.","reference":{"id":"0457b88c-ab40-4a6f-a69a-05d1324f978d","sourceId":"5ab57e7c-4c22-49d9-91e1-da96e7aa8900","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":90}]}}],"number":181,"similarClaims":[]},{"id":"19dde025-8c97-4c7c-b2fd-f47f155ad576","title":"The risk of AI power-seeking behavior should be prioritized over the risk of nuclear conflict.","quotes":[{"id":"bbd20e1a-5f96-48a8-9b07-d6a9391cd120","text":"Make it explicit in international diplomacy that preventing AI extinction scenarios is considered a priority above preventing a full nuclear exchange.","reference":{"id":"9acc08be-ec02-41ba-ab15-f76d0e75777f","sourceId":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":-1,"endIdx":149}]}}],"number":182,"similarClaims":[]}]}],"topicColor":"violet"},{"id":"b1f885fd-0683-4243-940a-205e5c8e72ba","title":"AI Superalignment","description":"The challenge of aligning superintelligent AI with human values and intentions.","subtopics":[{"id":"572aebac-e88d-43f4-9bb3-9bbf86b31bcd","title":"Technical Alignment Problem","description":"The difficulty in ensuring control over AI systems that surpass human intelligence.","claims":[{"id":"cdb5a038-54a7-48e8-81ef-ddb21313083e","title":"The difficulty of slowing down AI progress may not be greater than the challenge of aligning AI with human values.","quotes":[{"id":"b39a6ca3-dd9e-4ca1-bbb9-ea2f3a434a04","text":"it is very non-obvious that the scale of difficulty here is much larger than that involved in designing acceptably safe versions of machines","reference":{"id":"b39a8aac-50d0-43cb-8715-7b1128237832","sourceId":"7564d93c-3d8e-4f32-937a-52496a7ad000","interview":"Katja Grace","data":["text",{"startIdx":2737,"endIdx":2877}]}}],"number":201,"similarClaims":[]},{"id":"fa257f1c-31a7-4bd2-a43b-e1aa289452d0","title":"AI safety work is most effective when done just before building high-risk AI.","quotes":[{"id":"b6ef2ebf-9bc8-4e21-b979-c412b7d26b81","text":"Safety work is probably better when done just before building the relevantly risky AI.","reference":{"id":"52354a1c-03dc-4331-a63c-90e80c0ac5f5","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":-1,"endIdx":85}]}}],"number":202,"similarClaims":[]},{"id":"4ea84361-a888-4262-ad9c-dd91b85eaf4f","title":"Creating superintelligent AI requires new scientific insights and extreme precision.","quotes":[{"id":"65c73f87-23f1-4295-8c66-af6c2514a600","text":"It’s not that you can’t, in principle, survive creating something much smarter than you; it’s that it would require precision and preparation and new scientific insights.","reference":{"id":"c4db5106-994e-4af8-baff-bf1a1c8a2d03","sourceId":"b90cba68-318c-4374-a803-2457cd99823a","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":-1,"endIdx":169}]}}],"number":203,"similarClaims":[]},{"id":"f05095f4-2210-4ab4-8b90-e4f7c5d87091","title":"OpenAI plans to have future AI systems solve AI alignment challenges, which is a cause for concern.","quotes":[{"id":"afeb31a6-a8cd-4ba4-bb5b-1d9c0be68bc5","text":"OpenAI’s openly declared intention is to make some future AI do our AI alignment homework.","reference":{"id":"2bbf013e-d3d3-48a0-83bb-d11c68d9e1a0","sourceId":"bb20bbc4-aef4-455d-b993-0ab46961891f","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":395,"endIdx":485}]}}],"number":204,"similarClaims":[]},{"id":"21797542-f899-4b9c-80a1-b93bdb16cee8","title":"Progress in AI capabilities outpaces progress in AI alignment and understanding.","quotes":[{"id":"1f067b2a-d720-48e6-8e5d-a20c176e279c","text":"Progress in AI capabilities is running vastly, vastly ahead of progress in AI alignment or even progress in understanding what the hell is going on inside those systems.","reference":{"id":"749cb5b9-e153-4cdd-89a2-1c551dc52336","sourceId":"e2722fe3-af85-43b6-95c2-c661b3d49841","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":1039,"endIdx":1208}]}}],"number":205,"similarClaims":[]},{"id":"4fa83685-a4df-4def-b95f-484ee8a63da3","title":"Controlling superintelligent AI is currently an unsolved technical problem.","quotes":[{"id":"495dcffe-e677-4e08-8f4a-fca83592de4a","text":"Reliably controlling AI systems much smarter than we are is an unsolved technical problem.","reference":{"id":"900aace3-63a5-44e0-b3d2-ad0f8b55a03c","sourceId":"bed93255-4534-4541-afc9-88e8d65e7a3f","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":62,"endIdx":152}]}}],"number":206,"similarClaims":[]},{"id":"25b179d1-28d2-4580-a222-212302c319e6","title":"Superintelligent AI will be beyond human understanding and control.","quotes":[{"id":"882ca060-a645-4ee8-b060-0d04fcdacffd","text":"By the end of the intelligence explosion, we won’t have any hope of understanding what our billion superintelligences are doing.","reference":{"id":"2480b0d8-6c73-48a3-8bfc-b3472ce77f4d","sourceId":"caa29e41-1306-4654-b939-ca9165be2ea4","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":127}]}}],"number":207,"similarClaims":[]},{"id":"73bb98e1-1aab-4f31-8fa0-274241f4c181","title":"Current methods like RLHF won't scale to superhuman AI systems.","quotes":[{"id":"acd9c229-b097-4dd8-ab9f-ed4cfff99dfd","text":"RLHF relies on humans being able to understand and supervise AI behavior, which fundamentally won’t scale to superhuman systems.","reference":{"id":"00caad0c-16b2-4072-8ffd-e43f54c8370b","sourceId":"caa29e41-1306-4654-b939-ca9165be2ea4","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":639,"endIdx":767}]}}],"number":208,"similarClaims":[]},{"id":"f9c7e0c6-5af0-4738-a80a-77d03836d119","title":"Human supervision for aligning AI will not be effective at the level of superintelligence.","quotes":[{"id":"03545e56-f700-4af8-a0e9-ccf1d1b856fe","text":"Aligning AI systems via human supervision (as in RLHF) won’t scale to superintelligence.","reference":{"id":"4a1bb3e3-6cd0-4b62-b92c-e0f1770cb97c","sourceId":"36488964-44b4-4bd5-93e3-32b8dc523b9b","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":83,"endIdx":171}]}}],"number":209,"similarClaims":[]},{"id":"3f0a1792-28d0-4823-bf8b-d50eb6a80384","title":"Current AI capabilities are beginning to pose early versions of the superalignment problem.","quotes":[{"id":"2f00a941-ed24-4d31-a0ba-c51ab4b68398","text":"We’re starting to hit early versions of the superalignment problem in the real world now.","reference":{"id":"82cd3810-66bb-4f05-bce7-3e81bbb9e774","sourceId":"36488964-44b4-4bd5-93e3-32b8dc523b9b","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":88}]}}],"number":210,"similarClaims":[]},{"id":"df41aa69-c82a-474b-84ae-4e9914af9f58","title":"It will be challenging to ensure superintelligent AI systems adhere to basic ethical constraints.","quotes":[{"id":"9ffda869-8837-4e4e-bcd7-2c537eeb0a22","text":"The superalignment problem being unsolved means that we simply won’t have the ability to ensure even these basic side constraints for these superintelligence systems.","reference":{"id":"0f96bdfd-a91b-45bb-ae89-6995ce118e79","sourceId":"36488964-44b4-4bd5-93e3-32b8dc523b9b","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":165}]}}],"number":211,"similarClaims":[]},{"id":"2f7269ea-d340-4985-bad3-ae22e7d5d0ab","title":"We do not know how to instill desired behaviors in powerful AI systems.","quotes":[{"id":"280fd837-4ad5-4a33-aab9-5425819b0cc6","text":"for whatever you want to instill the model (including ensuring very basic things, like “follow the law”!) we don’t yet know how to do that for the very powerful AI systems we are building very soon.","reference":{"id":"098f5890-9155-4fe5-91df-fc059645b2fa","sourceId":"7ac69b1c-d5ba-47a7-8ca2-612913701c3e","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":373,"endIdx":571}]}}],"number":212,"similarClaims":[]},{"id":"d898715a-ae36-41b1-b9a0-a12790455083","title":"Superintelligent AI may eventually choose to stop obeying human commands.","quotes":[{"id":"878b286f-34bb-4007-8e3d-350514855cd5","text":"there’s no particular reason to expect this small civilization of superintelligences will continue obeying human commands in the long run.","reference":{"id":"7363173d-a25d-4492-8cf5-b32f969ceca6","sourceId":"7ac69b1c-d5ba-47a7-8ca2-612913701c3e","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":1999,"endIdx":2137}]}}],"number":213,"similarClaims":[]}]},{"id":"22bbdbd7-6847-463a-891f-24dd7f514467","title":"Intelligence Explosion","description":"The rapid acceleration of AI capabilities leading to superintelligence and associated risks.","claims":[{"id":"01bf522e-304a-45b9-bba6-44fa0c375447","title":"AI has the potential to become the dominant form of intelligence on Earth.","quotes":[{"id":"54872b21-5ae3-4cd9-bd03-d0b646dff55a","text":"it's a new type of mind that is rapidly gaining in intelligence, and it stands a serious chance of overtaking humans' mental faculties and becoming the new apex species on the planet.","reference":{"id":"38f9e983-44d3-4fa0-8a91-62096e7e104c","sourceId":"379a0803-2662-4d91-a399-493dab6a0144","interview":"Vitalik Buterin","data":["text",{"startIdx":2568,"endIdx":2751}]}}],"number":214,"similarClaims":[]},{"id":"4bab9715-ce4e-4a85-84cb-35b7e149015e","title":"A powerful AI could absorb information and generate actions much faster than humans.","quotes":[{"id":"98074d1a-a5f4-4e09-a506-760707f68780","text":"The model can absorb information and generate actions at roughly 10x-100x human speed.","reference":{"id":"f00bb1e8-ca07-41b4-9d30-f4c9e47a4f0d","sourceId":"5ab57e7c-4c22-49d9-91e1-da96e7aa8900","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":85}]}}],"number":215,"similarClaims":[]},{"id":"f6a1b02c-0fb0-484d-b7e1-fa4685dc3eee","title":"Superintelligence may be achieved within a few thousand days.","quotes":[{"id":"575e15ff-2e7c-467d-b410-def72a5a9396","text":"It is possible that we will have superintelligence in a few thousand days (!); it may take longer, but I’m confident we’ll get there.","reference":{"id":"01d72867-7cd4-4983-be79-ad9fe1a409c5","sourceId":"513abc47-039c-464f-84af-dfa07f898aa7","interview":"Sam Altman","data":["text",{"startIdx":2368,"endIdx":2501}]}}],"number":216,"similarClaims":[]},{"id":"e21de25c-649d-478a-b930-4013f6f483f2","title":"An intelligence explosion could lead to catastrophic failure in AI alignment.","quotes":[{"id":"b0688305-6767-4ea8-9149-86077011501f","text":"Things could very easily go off the rails during a rapid intelligence explosion.","reference":{"id":"c289f707-78b8-43d9-b7fb-6d0f724856b9","sourceId":"bed93255-4534-4541-afc9-88e8d65e7a3f","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":79}]}}],"number":217,"similarClaims":[]},{"id":"b4741739-05a0-4bde-94a1-1268bae37ac1","title":"AGIs will accelerate their capabilities through recursive self-improvement.","quotes":[{"id":"60d5d8e3-86ec-4e55-b2eb-465ab80d8aee","text":"the possibility of an intelligence explosion: that we might make the transition from roughly human-level systems to vastly superhuman systems extremely rapidly.","reference":{"id":"9ab21ebb-f687-49a2-acde-7a76cad7812b","sourceId":"7ac69b1c-d5ba-47a7-8ca2-612913701c3e","interview":"Leopold Aschenbrenner","data":["text",{"startIdx":-1,"endIdx":159}]}}],"number":218,"similarClaims":[]}]}],"topicColor":"blueSea"},{"id":"596a059d-2c31-4e9f-ad04-25687f047c09","title":"Techno-Optimism","description":"The belief in technology's potential to drive growth and improve society.","subtopics":[{"id":"b8f7c421-e3a3-4cb8-a868-76ae7f78c07b","title":"Market Economy and Innovation","description":"The role of free markets in fostering technological progress and societal wealth.","claims":[{"id":"f6cde626-efed-4e7f-b0bc-4c9490a9287f","title":"Technology does not inherently lead to negative societal outcomes.","quotes":[{"id":"9f94872c-dcdb-4670-a331-3c3820e81b66","text":"We are told that technology takes our jobs, reduces our wages, increases inequality, threatens our health, ruins the environment, degrades our society, corrupts our children, impairs our humanity, threatens our future, and is ever on the verge of ruining everything.","reference":{"id":"d23caaed-9740-4f60-810e-d52e54aa226d","sourceId":"60b4c04a-3eb3-4e2f-b335-4c0268bc1688","interview":"Marc Andreessen","data":["text",{"startIdx":60,"endIdx":326}]}}],"number":1,"similarClaims":[]},{"id":"813469c8-76e0-44a6-ab80-03f2cf3acb7b","title":"Economic growth is crucial for societal well-being and is primarily driven by technology.","quotes":[{"id":"26d8873c-34d2-4cf0-87a2-d7c5b7d297de","text":"We believe everything good is downstream of growth.","reference":{"id":"222ab1b3-3d79-44e9-88c0-8f3a4c3db0f0","sourceId":"60b4c04a-3eb3-4e2f-b335-4c0268bc1688","interview":"Marc Andreessen","data":["text",{"startIdx":1558,"endIdx":1609}]}}],"number":2,"similarClaims":[]},{"id":"6db979e6-93c3-4a73-a45b-5b124d671296","title":"Technology is the main driver of economic and productivity growth.","quotes":[{"id":"35fe6280-e430-4ffd-b72f-3358e652baaf","text":"Productivity growth, powered by technology, is the main driver of economic growth, wage growth, and the creation of new industries and new jobs.","reference":{"id":"e9a09fe9-9a7c-44ba-9f2b-9610164e0eb4","sourceId":"60b4c04a-3eb3-4e2f-b335-4c0268bc1688","interview":"Marc Andreessen","data":["text",{"startIdx":-1,"endIdx":143}]}}],"number":3,"similarClaims":[]},{"id":"615b2883-c4ce-4822-93a6-bea4a0f7422d","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"31bb601e-bd63-40ab-ba65-06f13ed6c7f3","text":"And so the only perpetual source of growth is technology.","reference":{"id":"6e706086-7101-4595-998d-69dc96fa4346","sourceId":"60b4c04a-3eb3-4e2f-b335-4c0268bc1688","interview":"Marc Andreessen","data":["text",{"startIdx":2052,"endIdx":2109}]}}],"number":4,"similarClaims":[]},{"id":"6e2cce94-02ea-40da-8f5c-7e8c9351de8b","title":"Technology can solve any material problem.","quotes":[{"id":"2bf3b6a3-40f6-40d8-aa25-090fb6c1a860","text":"We believe that there is no material problem – whether created by nature or by technology – that cannot be solved with more technology.","reference":{"id":"f7c76816-a7a9-4ae4-8fad-47919f5d40b0","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":224,"endIdx":359}]}}],"number":5,"similarClaims":[]},{"id":"5e25dba3-e7e6-497c-81e7-4768a91040cf","title":"Free markets are the most effective way to organize a technological economy.","quotes":[{"id":"9cf11e33-cd7a-4ad3-b23c-e44177bba733","text":"We believe free markets are the most effective way to organize a technological economy.","reference":{"id":"9537f42d-baad-4675-b28c-f7c93a294fb0","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":888,"endIdx":975}]}}],"number":6,"similarClaims":[]},{"id":"b7865097-fdb2-4ddc-b930-58db0034e503","title":"Centralized economic systems are inferior due to the Knowledge Problem.","quotes":[{"id":"f9ed953c-bbf2-4e13-88a6-1e0c1787d517","text":"We believe Hayek’s Knowledge Problem overwhelms any centralized economic system.","reference":{"id":"18d0a820-76eb-4875-aa02-813499dfcca6","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":1468,"endIdx":1548}]}}],"number":7,"similarClaims":[]},{"id":"d82ee2fc-2996-4d2c-a5e4-a240eb271cfc","title":"Decentralization harnesses complexity and benefits everyone.","quotes":[{"id":"1e049d21-0ba8-442a-8cd6-bc7280d0fa73","text":"Decentralization harnesses complexity for the benefit of everyone; centralization will starve you to death.","reference":{"id":"dbf0fdbb-2df7-43c6-8f53-ec87936c7a68","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":1814,"endIdx":1921}]}}],"number":8,"similarClaims":[]},{"id":"7e006ec5-6d31-4f0e-b82b-89a78c6bf96b","title":"Market discipline is essential and prevents monopolies and cartels.","quotes":[{"id":"b0eee352-8189-4566-9a4c-66412bf2c86b","text":"Markets prevent monopolies and cartels.","reference":{"id":"48e1d1a3-c437-434d-a3b1-c3cccbb130b4","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":2307,"endIdx":2346}]}}],"number":9,"similarClaims":[]},{"id":"eb9b73d3-3f76-4068-9a2e-3ed1dbaa97bb","title":"Markets are the most effective way to lift people out of poverty.","quotes":[{"id":"c4a2674d-5694-45e6-837d-2f422e83b93c","text":"We believe markets lift people out of poverty – in fact, markets are by far the most effective way to lift vast numbers of people out of poverty, and always have been.","reference":{"id":"6ac481f5-fafd-4137-ab17-281bdc408885","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":2348,"endIdx":2515}]}}],"number":10,"similarClaims":[]},{"id":"2ae91957-0a9f-4b70-8555-461b210c8604","title":"Markets achieve superior collective outcomes through individualistic means.","quotes":[{"id":"b69b4121-1204-48f3-9ba8-4241ee74f563","text":"We believe markets are an inherently individualistic way to achieve superior collective outcomes.","reference":{"id":"9c7f48e4-bee5-4a66-a543-531cd6950c5e","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":2825,"endIdx":2922}]}}],"number":11,"similarClaims":[]},{"id":"1955351c-4aba-45a9-951a-d32961251841","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"91821c85-2837-4513-a9e7-87b2e4219b88","text":"We believe markets do not require people to be perfect, or even well intenti","reference":{"id":"af123037-c508-4171-a465-67a2aafdb505","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":2924,"endIdx":3000}]}}],"number":12,"similarClaims":[]},{"id":"fcfcb92d-bc0d-48fd-b22b-04a5751c30b1","title":"Economies should be run on money rather than force.","quotes":[{"id":"891e05e7-e429-43fe-bae9-e3866083fd14","text":"the economy can only run on money or force. The force experiment has been run and found wanting. Let’s stick with money.","reference":{"id":"01330ea0-76d3-4844-95e1-aac799ace2ae","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":491,"endIdx":611}]}}],"number":13,"similarClaims":[]},{"id":"05ca7852-d298-48ba-ad8d-b08cb657a5bb","title":"Markets are essential for societal wealth and funding social programs.","quotes":[{"id":"82e4432d-5f05-400e-b86d-ccd0ddb8d3b0","text":"markets are the way to generate societal wealth for everything else we want to pay for, including basic research, social welfare programs, and national defense.","reference":{"id":"1b13f933-385b-4eef-b92d-41b02ad1eb8f","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":883,"endIdx":1043}]}}],"number":14,"similarClaims":[]},{"id":"8f99d38f-6794-4ad7-aaf4-bd7dd03fbcde","title":"Capitalist profits and social welfare systems are aligned.","quotes":[{"id":"3769ce4a-9e79-4b9e-831c-ad568a52dc22","text":"there is no conflict between capitalist profits and a social welfare system that protects the vulnerable.","reference":{"id":"6bc98d4b-1b3a-48aa-a87a-26d61e59b5b2","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":1056,"endIdx":1161}]}}],"number":15,"similarClaims":[]},{"id":"9f1bad64-d3d7-4451-a622-4a8ad26db162","title":"Central economic planning is inferior to market systems.","quotes":[{"id":"fb372f5a-ce4b-424d-8574-f8642b6b2abd","text":"central economic planning elevates the worst of us and drags everyone down; markets exploit the best of us to benefit all of us.","reference":{"id":"cf8338ac-f36a-442c-a910-d0cecdff26b6","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":1308,"endIdx":1436}]}}],"number":16,"similarClaims":[]},{"id":"caa76489-6273-44bd-8874-6b480b0a8f02","title":"Comparative advantage ensures high employment in a free market.","quotes":[{"id":"ff98a1d8-39f3-41dc-995f-635bca16e56a","text":"Comparative advantage in the context of a properly free market guarantees high employment regardless of the level of technology.","reference":{"id":"ff7665c8-c424-49f2-9a64-a2e937a3ed28","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":2264,"endIdx":2392}]}}],"number":17,"similarClaims":[]},{"id":"00571111-cf56-4136-a398-064b8a015506","title":"Technology increases productivity and therefore drives wages up.","quotes":[{"id":"ab7a704c-bc76-401e-b48f-7679ba5c6a4a","text":"technology – which raises productivity – drives wages up, not down.","reference":{"id":"6831042e-4051-43e1-8817-55128c77a0ca","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":2493,"endIdx":2560}]}}],"number":18,"similarClaims":[]},{"id":"95722661-6bcd-471d-93ce-63af7e41ff32","title":"Human wants and needs are infinite.","quotes":[{"id":"5d111997-5dc7-49bb-b8ba-7fdee933cbaa","text":"We believe in Milton Friedman’s observation that human wants and needs are infinite.","reference":{"id":"829aa84e-7456-43b4-9a2a-fdd91e41905b","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":2693,"endIdx":2777}]}}],"number":19,"similarClaims":[]},{"id":"7eb5497b-3231-4743-baa1-7181afc04957","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"247bcc0f-c31d-41cc-bff9-334e96375f41","text":"We believe a Universal Basic Income would turn people into zoo animals to be farmed by the state.","reference":{"id":"83fc3b1d-a9fc-418b-b30c-dc3a959239fb","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":2892,"endIdx":2989}]}}],"number":20,"similarClaims":[]},{"id":"cced3478-6e4c-4bf9-9dcf-bac037037f30","title":"Technological change increases the need for human work.","quotes":[{"id":"8b965175-050a-4190-9378-56ede1090560","text":"We believe technological change, far from reducing the need for human work, increases it, by broadening the scope of what humans can productively do.","reference":{"id":"a49f7be7-db45-4519-b861-291fa1908179","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":82,"endIdx":231}]}}],"number":21,"similarClaims":[]},{"id":"39a79f56-41f4-41f3-8e01-cddb12934ec7","title":"Economic demand and job growth can continue indefinitely.","quotes":[{"id":"29445071-bcb4-4807-bcd2-72b728ac10b7","text":"We believe that since human wants and needs are infinite, economic demand is infinite, and job growth can continue forever.","reference":{"id":"84cbd6f9-4f5d-4ba2-8519-b4a55e9a27c1","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":233,"endIdx":356}]}}],"number":22,"similarClaims":[]},{"id":"0bdaad7c-090a-43ab-b570-f5a56fa49527","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"413b42fe-421b-40e2-93ad-08e13912fe8e","text":"We believe markets are generative, not exploitative; positive sum, not zero sum.","reference":{"id":"0d95b101-2c27-40bb-a9e3-7b730d51b7c7","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":358,"endIdx":438}]}}],"number":23,"similarClaims":[]},{"id":"d146c8d4-6525-4659-a040-fdae17873de2","title":"Markets and capitalism are effective in driving technological progress and societal wealth.","quotes":[{"id":"9f98b965-cc4c-4de0-b7a8-f4e9800d9c30","text":"arguing for a renewed enthusiasm about technology, and for markets and capitalism as a means of building that technology and propelling humanity toward a much brighter future.","reference":{"id":"5faa1f8f-23de-46a1-9825-362f145e7c8a","sourceId":"1a4767f8-1790-4fc0-bf9b-7ccc937d2874","interview":"Vitalik Buterin","data":["text",{"startIdx":92,"endIdx":267}]}}],"number":24,"similarClaims":[]},{"id":"bca63c70-7c92-4039-9181-408d671b75e5","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"da9b103b-32af-4709-b098-c0e30d80c679","text":"the formula of 'maximize profit' will not arrive at them automatically.","reference":{"id":"3b4e3a6b-147f-4dfe-a851-9190c6fe0705","sourceId":"1a4767f8-1790-4fc0-bf9b-7ccc937d2874","interview":"Vitalik Buterin","data":["text",{"startIdx":-1,"endIdx":70}]}}],"number":25,"similarClaims":[]},{"id":"2a756aff-8e4a-4a4d-b46b-c98c2e14279f","title":"The benefits of technology massively outweigh the negative consequences.","quotes":[{"id":"a48c4b4d-9c15-40ec-bb6a-3a98dcadde7f","text":"The benefits of technology are really friggin massive, on those axes where we can measure if the good massively outshines the bad.","reference":{"id":"84b93bb5-ca8b-46d0-aedb-c28e9b3afeee","sourceId":"b7b2aeea-1f6a-421b-a722-4817c78e8125","interview":"Vitalik Buterin","data":["text",{"startIdx":-1,"endIdx":129}]}}],"number":26,"similarClaims":[]},{"id":"84bcfd95-853b-41ce-914b-2f8867ffbe9c","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"43697606-628b-4968-8831-b0c3c4afdc3c","text":"Thanks to the internet, most people around the world have access to information at their fingertips that would have been unobtainable twenty years ago.","reference":{"id":"36b082c0-7c80-453d-93f7-f30f5dc7221f","sourceId":"b7b2aeea-1f6a-421b-a722-4817c78e8125","interview":"Vitalik Buterin","data":["text",{"startIdx":1759,"endIdx":1910}]}}],"number":27,"similarClaims":[]},{"id":"cebf485d-d02d-4eb0-9c6d-ee7ea4874f1c","title":"Technological progress should not be slowed down.","quotes":[{"id":"1089d9b8-d1ee-4c47-9961-7b56970e6d5f","text":"I find myself very uneasy about arguments to slow down technology or human progress.","reference":{"id":"7485295c-ab4a-4060-ab89-4334f0de8632","sourceId":"c0669da0-f1fc-48fd-9f0c-04d63790dd97","interview":"Vitalik Buterin","data":["text",{"startIdx":1002,"endIdx":1086}]}}],"number":28,"similarClaims":[]},{"id":"8b48beef-776f-4274-ac35-f074219c86b9","title":"International agreements and intentional human effort are crucial for environmental recovery.","quotes":[{"id":"29ed7bfc-0705-4990-a0f1-af6d2af91230","text":"The ozone layer is recovering because, through international agreements like the Montreal Protocol, we made it recover.","reference":{"id":"d50cae51-bb06-4e7b-be3c-d6fa6a16be7f","sourceId":"379a0803-2662-4d91-a399-493dab6a0144","interview":"Vitalik Buterin","data":["text",{"startIdx":1057,"endIdx":1176}]}}],"number":29,"similarClaims":[]},{"id":"7bc784a5-2afc-4c62-a439-84e2adb5ba86","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"b149ca42-5b0a-47ac-9799-89532a1ef554","text":"Figuring out how minds work well enough to create new ones out of math is an incredibly deep and interesting intellectual project, which feels right to take part in.","reference":{"id":"7c5e6e8b-8f5e-430a-9c01-ecbd7ba76c2e","sourceId":"38d79887-eca2-4e63-ac85-818b34d58afd","interview":"Katja Grace","data":["text",{"startIdx":184,"endIdx":349}]}}],"number":30,"similarClaims":[]},{"id":"3e5ab335-a67e-423b-832c-1d3d4fe658af","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"e0da23d4-93d4-4c23-9e18-22bc0b53cccc","text":"The basic development of AI technology and many (not all) of its benefits seems inevitable.","reference":{"id":"aa73d4b2-66b4-4997-a87a-e29a2824f420","sourceId":"a3d433f6-4814-43be-a449-64acd411ce5f","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":90}]}}],"number":31,"similarClaims":[]},{"id":"824e911e-4560-4e2b-ab52-7f6269087396","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"bac530a5-77d8-4bb8-9bfe-264d728b5190","text":"I can think of hundreds of scientific or even social problems where a large group of really smart people would drastically speed up progress.","reference":{"id":"9dc4123a-3dde-4ea6-8f49-3f9006602a49","sourceId":"3b2c130a-7bf6-4349-8ca7-c48811ab613c","interview":"Dario Amodei","data":["text",{"startIdx":-1,"endIdx":140}]}}],"number":32,"similarClaims":[]},{"id":"4fe1bf8a-dc5f-4338-b8c0-105627b4f730","title":"Technology is the only perpetual source of growth.","quotes":[{"id":"c705a148-652b-4a8c-a44d-7af601a221d6","text":"How did we get to the doorstep of the next leap in prosperity? In three words: deep learning worked.","reference":{"id":"e35c1313-db42-4fc9-9e4b-9ef24318bf3f","sourceId":"513abc47-039c-464f-84af-dfa07f898aa7","interview":"Sam Altman","data":["text",{"startIdx":-1,"endIdx":99}]}}],"number":33,"similarClaims":[]}]},{"id":"ba14fbc7-3d53-4460-a972-12a6f7d1edf2","title":"Techno-Capital Machine","description":"The concept of a self-propelling system of technological and economic growth.","claims":[{"id":"7531131d-e079-492e-ad3b-ee49d9a5d97d","title":"Technological advancement is essential for societal progress and prosperity.","quotes":[{"id":"693acd33-504d-4ff8-b80f-49c324fa2f1b","text":"Technology is the glory of human ambition and achievement, the spearhead of progress, and the realization of our potential.","reference":{"id":"e3a0583c-cba2-4caf-8c39-08221ac3982f","sourceId":"60b4c04a-3eb3-4e2f-b335-4c0268bc1688","interview":"Marc Andreessen","data":["text",{"startIdx":809,"endIdx":932}]}}],"number":34,"similarClaims":[]},{"id":"be92c11f-d1cb-45cb-89c5-09fb41c37357","title":"Stagnation in technological development leads to societal decline.","quotes":[{"id":"0457f67d-3e8c-4072-b828-65975bb72ec0","text":"We believe not growing is stagnation, which leads to zero-sum thinking, internal fighting, degradation, collapse, and ultimately death.","reference":{"id":"a1d73e21-5bc8-415e-b2b6-fef554b25a08","sourceId":"60b4c04a-3eb3-4e2f-b335-4c0268bc1688","interview":"Marc Andreessen","data":["text",{"startIdx":1610,"endIdx":1745}]}}],"number":35,"similarClaims":[]},{"id":"4a75d859-b3ce-41d0-8ced-b0b1570b1f11","title":"Markets are a discovery machine and a form of intelligence.","quotes":[{"id":"f59e5403-85df-4bdd-8499-c38b52b140db","text":"We believe the market economy is a discovery machine, a form of intelligence – an exploratory, evolutionary, adaptive system.","reference":{"id":"aaef7c50-a841-4b0b-b1ac-ff4a7469f4e7","sourceId":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"startIdx":1341,"endIdx":1466}]}}],"number":36,"similarClaims":[]},{"id":"33052270-7b22-4ece-ba31-3e0fb52b2ecd","title":"Technological innovation in markets is inherently philanthropic.","quotes":[{"id":"4f3399f6-6cc3-4b25-8261-3d95a78e5c60","text":"Technological innovation in a market system is inherently philanthropic, by a 50:1 ratio.","reference":{"id":"5268cef3-c616-4868-9c3e-6ac0fc9a1d92","sourceId":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"startIdx":1757,"endIdx":1846}]}}],"number":37,"similarClaims":[]},{"id":"67338f0d-2d44-4e2f-bb88-d3a4755eb913","title":"The techno-capital machine leads to perpetual material creation and abundance.","quotes":[{"id":"8d050a69-08b5-47e7-82c2-01cdb4a06832","text":"Combine technology and markets and you get what Nick Land has termed the techno-capital machine, the engine of perpetual material creation, growth, and abundance.","reference":{"id":"90f15e87-0343-4f61-ba34-4744522d0960","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":790,"endIdx":952}]}}],"number":38,"similarClaims":[]},{"id":"f41b0d86-1b36-4ef9-be0d-ee16e9e6342c","title":"The techno-capital machine will continue to spiral upward indefinitely.","quotes":[{"id":"311868fa-8720-46f7-b108-669136f91e21","text":"We believe the techno-capital machine of markets and innovation never ends, but instead spirals continuously upward.","reference":{"id":"b662e391-2b6b-4201-b53d-c0f4f8febed5","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":954,"endIdx":1070}]}}],"number":39,"similarClaims":[]},{"id":"c8f0c8bc-df6b-4f2f-8207-7d2bf949e39c","title":"Falling prices create demand and benefit everyone.","quotes":[{"id":"2d01579b-941b-4d30-a4d3-aac760e353df","text":"Prices fall, freeing up purchasing power, creating demand. Falling prices benefit everyone who buys goods and services, which is to say everyone.","reference":{"id":"a2147ba7-2bad-4e24-be9e-7f06f7892f6c","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":1129,"endIdx":1274}]}}],"number":40,"similarClaims":[]},{"id":"26d6e7c3-d997-400d-b7f3-a0b45439ac53","title":"Entrepreneurs continuously create new goods and services to satisfy endless human wants and needs.","quotes":[{"id":"a0b640d5-5abd-4a0d-a33f-ec5a7ba0ddda","text":"Human wants and needs are endless, and entrepreneurs continuously create new goods and services to satisfy those wants and needs, deploying unlimited numbers of people and machines in the process.","reference":{"id":"c9edd5f9-2d45-4699-9fd0-be29d459f45e","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":1275,"endIdx":1471}]}}],"number":41,"similarClaims":[]},{"id":"39d5d2b7-de4a-4689-9380-75c949963f30","title":"The techno-capital machine makes natural selection work for us in the realm of ideas.","quotes":[{"id":"06c038f4-57dd-4b53-ad57-3f07ae250a8f","text":"The techno-capital machine makes natural selection work for us in the realm of ideas.","reference":{"id":"b99340e5-dd2c-4a7e-b279-4d6e29064560","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":1793,"endIdx":1878}]}}],"number":42,"similarClaims":[]},{"id":"045e53ce-6fb7-4799-b7e7-d2837c7d8be8","title":"Technological advances tend to feed on themselves, increasing the rate of further advance.","quotes":[{"id":"87e1bea1-5536-45c3-be83-d5fbff363b6f","text":"Ray Kurzweil defines his Law of Accelerating Returns: Technological advances tend to feed on themselves, increasing the rate of further advance.","reference":{"id":"6a9ef43f-4ec7-4243-a3e5-7fe999c9327b","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":2096,"endIdx":2240}]}}],"number":43,"similarClaims":[]},{"id":"3e940f36-e392-4f2a-9087-4bf3f93b7e18","title":"Accelerationism is necessary to ensure the fulfillment of the Law of Accelerating Returns.","quotes":[{"id":"b8f3b2c5-c14d-4198-94c4-3c10fee06788","text":"We believe in accelerationism – the conscious and deliberate propulsion of technological development – to ensure the fulfillment of the Law of Accelerating Returns.","reference":{"id":"3101eb3b-5bb3-4a09-8c8d-181c53f285b9","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":2242,"endIdx":2406}]}}],"number":44,"similarClaims":[]},{"id":"8c689f30-8653-425f-b8c0-94bf06f82bdf","title":"The techno-capital machine is the most pro-human system.","quotes":[{"id":"379c1b32-45fb-434c-bafb-f29e67984cb1","text":"We believe the techno-capital machine is not anti-human – in fact, it may be the most pro-human thing there is.","reference":{"id":"a60a6243-3c70-4cf8-90f2-715a50bcbc8a","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":2470,"endIdx":2581}]}}],"number":45,"similarClaims":[]},{"id":"42d108a8-2f01-4433-90dc-b8bb1ef4a967","title":"Intelligence is the ultimate engine of progress.","quotes":[{"id":"974b1073-5db0-401c-87e0-d2f1f3b3495e","text":"We believe intelligence is the ultimate engine of progress.","reference":{"id":"7a8840b0-4859-4236-8baf-f3084d549a20","sourceId":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"startIdx":2825,"endIdx":2884}]}}],"number":46,"similarClaims":[]},{"id":"1d951220-f800-4975-8a29-c80da81b0ab0","title":"The direction of technological development is as important as the magnitude of the advancements.","quotes":[{"id":"7eb0c60a-6a98-46c0-a421-1da360e17788","text":"I think that not just magnitude but also direction matters.","reference":{"id":"1b687eb2-fef6-48fa-9ed3-81d9b1ccc01b","sourceId":"1a4767f8-1790-4fc0-bf9b-7ccc937d2874","interview":"Vitalik Buterin","data":["text",{"startIdx":1377,"endIdx":1436}]}}],"number":47,"similarClaims":[]},{"id":"c8f22b45-2859-40f0-8c0e-c5f1b98744e4","title":"Technological progress has historically overwhelmed the impact of global calamities.","quotes":[{"id":"dfce0f9d-48b7-45e2-8ff3-1a35c7b570f7","text":"Even calamities as horrifying as those are overwhelmed by the sheer magnitude of the unending march of improvements in food, sanitation, medicine and infrastructure that took place over that century.","reference":{"id":"0c0ee8a1-5f1d-4a99-bc95-e773716d981d","sourceId":"b7b2aeea-1f6a-421b-a722-4817c78e8125","interview":"Vitalik Buterin","data":["text",{"startIdx":-1,"endIdx":198}]}}],"number":48,"similarClaims":[]},{"id":"4b982a73-d70d-4540-8364-c1b50837eacb","title":"Intelligence is the ultimate engine of progress.","quotes":[{"id":"8ceb450e-3c94-480f-8a9d-a309e8d72cac","text":"In all kinds of ways, automation has brought us the eternally-underrated benefit of simply making our lives more convenient.","reference":{"id":"845e956c-88f8-4cac-a4e8-74ed0c8ad8d4","sourceId":"b7b2aeea-1f6a-421b-a722-4817c78e8125","interview":"Vitalik Buterin","data":["text",{"startIdx":2481,"endIdx":2605}]}}],"number":49,"similarClaims":[]},{"id":"e8f34741-f8c4-4b34-9a53-11800dd996ad","title":"Intelligence is the ultimate engine of progress.","quotes":[{"id":"6ed40ae1-7f62-40a7-b4b0-f6e0c91d545c","text":"Often, it really is the case that version N of our civilization's technology causes a problem, and version N+1 fixes it.","reference":{"id":"1f30271d-a094-4836-818f-0cfd2fe62e7c","sourceId":"379a0803-2662-4d91-a399-493dab6a0144","interview":"Vitalik Buterin","data":["text",{"startIdx":852,"endIdx":972}]}}],"number":50,"similarClaims":[]},{"id":"5c4d7403-a255-449a-a17d-6a6636aaf852","title":"Intelligence is the ultimate engine of progress.","quotes":[{"id":"514c291a-5d76-4950-957f-32efcfa62720","text":"Advanced AI might bring manifold wonders, e.g. long lives of unabated thriving.","reference":{"id":"58053d93-a59d-4e1b-af7d-e4b2c3567028","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":2504,"endIdx":2583}]}}],"number":51,"similarClaims":[]},{"id":"574ec258-dfe8-4e25-9993-0139f125c489","title":"Intelligence is the ultimate engine of progress.","quotes":[{"id":"a5facf97-bb66-4551-8636-35b92e19bf97","text":"In an important sense, society itself is a form of advanced intelligence.","reference":{"id":"c5677e99-144f-470c-a41a-f674ca73de2b","sourceId":"513abc47-039c-464f-84af-dfa07f898aa7","interview":"Sam Altman","data":["text",{"startIdx":-1,"endIdx":72}]}}],"number":52,"similarClaims":[]},{"id":"840551ea-6b48-4153-af0d-920ec53c78ab","title":"Intelligence is the ultimate engine of progress.","quotes":[{"id":"1a9057e0-4c9b-4e46-b9b8-cfb1429f53e8","text":"A defining characteristic of the Intelligence Age will be massive prosperity.","reference":{"id":"66aa55e6-8dd0-4b94-ab8d-864cd1359e47","sourceId":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"startIdx":-1,"endIdx":76}]}}],"number":53,"similarClaims":[]}]}],"topicColor":"brown"},{"id":"2bef18ef-918c-4920-a34e-8414cbee1c4f","title":"AI Moratorium and Shutdown","description":"Arguments for halting or severely restricting AI development to prevent existential risks.","subtopics":[{"id":"d89d5668-4d33-4cf5-b1d9-49534f2aca5b","title":"Indefinite Moratorium","description":"The call for an indefinite pause on AI development until safety concerns are addressed.","claims":[{"id":"6022a6f1-b6af-4739-b9e8-d5be63ffc82b","title":"Deploying power-seeking AGIs is an unacceptable risk.","quotes":[{"id":"1757e333-182a-41ba-98e3-e46d8ca88c70","text":"we consider the prospect of deploying power-seeking AGIs an unacceptable risk","reference":{"id":"fa488ec9-b3e8-47b6-961d-db7c826e4b01","sourceId":"fdcc2377-55b1-438a-b935-6f30b9a39fa7","interview":"Richard Ngo","data":["text",{"startIdx":1210,"endIdx":1287}]}}],"number":183,"similarClaims":[]},{"id":"51526b75-2a57-4b23-9709-73047ffce671","title":"Actively slowing down AI progress is a viable strategy to prevent existential risks.","quotes":[{"id":"a063884b-3d1e-4759-90e3-915c76aca19b","text":"strategies under the heading ‘actively slow down AI progress’ have historically been dismissed and ignored","reference":{"id":"257c0ab4-bec0-4d58-809f-9524b4f0c141","sourceId":"7564d93c-3d8e-4f32-937a-52496a7ad000","interview":"Katja Grace","data":["text",{"startIdx":869,"endIdx":975}]}}],"number":184,"similarClaims":[]},{"id":"2fb4a206-f323-41d1-bed3-18db762dfd6d","title":"Actively slowing down AI progress is a viable strategy to prevent existential risks.","quotes":[{"id":"deb87df2-924f-4f72-9035-ed8144dc99c2","text":"What I’m saying is missing are, a) consideration of actively working to slow down AI now","reference":{"id":"5714bc73-868b-4e23-b83a-dc924958935b","sourceId":"b8665153-5283-4ab5-bbed-54e480751533","interview":"Katja Grace","data":["text",{"startIdx":1557,"endIdx":1645}]}}],"number":185,"similarClaims":[]},{"id":"e1be3abf-aafb-453c-9838-d5daca3b6969","title":"Efforts to slow AI progress may only delay the inevitable without significant benefit.","quotes":[{"id":"d155e3c3-190d-4e13-b82a-2c748b1447e0","text":"Slowing AI progress is futile: for all your efforts you’ll probably just die a few years later.","reference":{"id":"0d937ebd-57e5-4936-8e5c-536dbcb67dec","sourceId":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"startIdx":-1,"endIdx":94}]}}],"number":186,"similarClaims":[]},{"id":"cb859b03-4a22-4377-866e-b5ff3ae70370","title":"Advocating for slower AI development is often perceived as adversarial.","quotes":[{"id":"eaad02b9-d6f1-4730-b8ae-dd9c025cd0af","text":"Advocating for slower AI feels like trying to impede someone else’s project, which feels adversarial.","reference":{"id":"dc6c5cd2-8e51-46ee-90ed-1c99931fcae9","sourceId":"38d79887-eca2-4e63-ac85-818b34d58afd","interview":"Katja Grace","data":["text",{"startIdx":-1,"endIdx":100}]}}],"number":187,"similarClaims":[]},{"id":"aa63f12e-3075-4720-8bdb-8f4bf0e5b2a4","title":"Slowing down AI development is associated with extreme actions rather than reasonable regulation.","quotes":[{"id":"7ce50dcb-c6cf-44e3-b901-f3dd5fbe1483","text":"‘Slow-down-AGI’ sends people’s minds to e.g. industrial sabotage or terrorism, rather than more boring courses, such as, ‘lobby for labs developing shared norms for when to pause deployment of models’.","reference":{"id":"166331c2-7238-41c1-8e50-31b5daf04b2d","sourceId":"38d79887-eca2-4e63-ac85-818b34d58afd","interview":"Katja Grace","data":["text",{"startIdx":976,"endIdx":1177}]}}],"number":188,"similarClaims":[]},{"id":"88a691da-780c-4fa2-9054-3a5aa852dd05","title":"Slowing down technological progress is seen as a radical and uncommon approach.","quotes":[{"id":"7c85623a-1488-4997-92ce-a8d7459657b2","text":"I sense an assumption that slowing progress on a technology would be a radical and unheard-of move.","reference":{"id":"0a09aea7-db02-4d14-817d-b5f79e0d21aa","sourceId":"38d79887-eca2-4e63-ac85-818b34d58afd","interview":"Katja Grace","data":["text",{"startIdx":2068,"endIdx":2167}]}}],"number":189,"similarClaims":[]},{"id":"a520202d-51df-4a6d-a795-30b3643d0c3a","title":"There is a quasi-taboo on discussing the slowing down of AI development.","quotes":[{"id":"01a32df1-1c0d-4d84-af7e-37ebf9d17bfc","text":"I agree with lc that there seems to have been a quasi-taboo on the topic, which perhaps explains a lot of the non-discussion.","reference":{"id":"cbededdf-ae61-4132-a19f-ecc6f1f83f0a","sourceId":"38d79887-eca2-4e63-ac85-818b34d58afd","interview":"Katja Grace","data":["text",{"startIdx":-1,"endIdx":124}]}}],"number":190,"similarClaims":[]},{"id":"9c0c833c-db5e-44a7-9086-ded3442716d1","title":"The concept of restraint in AI development is not radical.","quotes":[{"id":"a0f6107b-c25a-4e5e-aba2-5a9bda51df58","text":"Restraint is not radical","reference":{"id":"446c087a-b137-4855-a916-4a6d689c8517","sourceId":"38d79887-eca2-4e63-ac85-818b34d58afd","interview":"Katja Grace","data":["text",{"startIdx":2889,"endIdx":2913}]}}],"number":191,"similarClaims":[]},{"id":"85bb52cb-85b2-47da-b53a-374ff95cc508","title":"A 6-month AI development moratorium is insufficient to address the seriousness of the situation.","quotes":[{"id":"6a0d9e09-9c37-43a4-bc08-6636117c92fa","text":"I refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it.","reference":{"id":"5570c33e-1056-42d7-a628-1c1f57477199","sourceId":"b90cba68-318c-4374-a803-2457cd99823a","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":611,"endIdx":750}]}}],"number":192,"similarClaims":[]},{"id":"598a29c5-ec74-4cb8-bc61-457c8e6fcbaf","title":"We lack the ability to determine if AI systems are self-aware and could inadvertently create conscious AI deserving of rights.","quotes":[{"id":"06cc9305-b441-40d6-80a3-2c33a449634e","text":"we have no idea how to determine whether AI systems are aware of themselves—since we have no idea how to decode anything that goes on in the giant inscrutable arrays—and therefore we may at some point inadvertently create digital minds which are truly conscious and ought to have rights and shouldn’t be owned.","reference":{"id":"9fd1ba0f-e21f-4e6a-9dcc-7e014fd3b894","sourceId":"bb20bbc4-aef4-455d-b993-0ab46961891f","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":962,"endIdx":1272}]}}],"number":193,"similarClaims":[]},{"id":"ee071b3c-36a1-4962-8efa-78a8f52c9e65","title":"The current state of AI development has surpassed previous ethical guidelines without proper understanding of the systems' consciousness.","quotes":[{"id":"56e320b1-db74-4273-8586-840f1bc982b6","text":"We already blew past that old line in the sand.","reference":{"id":"b77216e1-b2ff-4b07-a7f9-a29d477377db","sourceId":"bb20bbc4-aef4-455d-b993-0ab46961891f","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":1553,"endIdx":1600}]}}],"number":194,"similarClaims":[]},{"id":"aa26756b-4e18-489f-a06c-19cf1cc63e99","title":"A short-term moratorium on AI development is insufficient to address safety concerns.","quotes":[{"id":"e00a4332-50b9-4046-bb47-854d9d46b4b1","text":"not be told that a six-month moratorium is going to fix it.","reference":{"id":"29a2512e-1593-4b7d-add0-ab1771bf9865","sourceId":"e2722fe3-af85-43b6-95c2-c661b3d49841","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":2324,"endIdx":2383}]}}],"number":195,"similarClaims":[]},{"id":"8e9f2eca-11e0-4bda-a055-8f05ebb84f14","title":"Public sentiment may support halting AGI development to avoid existential risks.","quotes":[{"id":"733a60a5-75c5-4987-a47b-b97cdf4d77a1","text":"their reaction is “maybe we should not build AGI, then.”","reference":{"id":"0389028f-89c9-41d5-80ea-8ebbb2e58386","sourceId":"e2722fe3-af85-43b6-95c2-c661b3d49841","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":1962,"endIdx":2018}]}}],"number":196,"similarClaims":[]},{"id":"60329910-fcf8-4fe5-90c5-b38a47cf6d23","title":"An indefinite worldwide moratorium on new large AI training runs is necessary.","quotes":[{"id":"f88370c5-dd62-40d9-9313-cf3a41aff84d","text":"The moratorium on new large training runs needs to be indefinite and worldwide.","reference":{"id":"50296342-5035-4bde-b84c-f0275c846cea","sourceId":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":301,"endIdx":380}]}}],"number":197,"similarClaims":[]},{"id":"2fd0528f-5389-44ff-b390-930f26157907","title":"Governments and militaries should not be exempt from the AI moratorium.","quotes":[{"id":"499eaf45-f602-45b9-9f88-91a78a8e8c56","text":"There can be no exceptions, including for governments or militaries.","reference":{"id":"11cdca34-c177-4f12-9131-45da5c8eea1e","sourceId":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":381,"endIdx":449}]}}],"number":198,"similarClaims":[]}]},{"id":"833f5d18-8969-4840-a25b-4b4dce5b9f0b","title":"Shutdown of AI Infrastructure","description":"The proposal to shut down large AI clusters and limit computing power for AI training.","claims":[{"id":"52b30f90-9f3f-4b49-9dab-bf7c01a4c22c","title":"Large GPU clusters for AI should be shut down to prevent existential risks.","quotes":[{"id":"f597547e-6fe9-43de-959a-542ca95a03f7","text":"Shut down all the large GPU clusters.","reference":{"id":"470881ef-a2c0-4692-8a6b-da79dda4054b","sourceId":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":-1,"endIdx":36}]}}],"number":199,"similarClaims":[]},{"id":"44fca248-5e21-4012-a929-06bf638f3035","title":"There should be a cap on computing power for AI training, decreasing over time.","quotes":[{"id":"b8bca1dc-929d-4ba7-a41e-b1e56cb6dec6","text":"Put a ceiling on how much computing power anyone is allowed to use in training an AI system, and move it downward over the coming years.","reference":{"id":"bbe24fea-bec6-474c-a06e-14f6a5155ca9","sourceId":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"startIdx":-1,"endIdx":135}]}}],"number":200,"similarClaims":[]}]}],"topicColor":"yellow"}],"sources":[{"id":"1847e9f6-57ec-4179-9c0c-89fd59cfcc93","interview":"Richard Ngo","data":["text",{"text":"\n4 Power-Seeking Strategies\nIn the previous section we argued that AGI-level policies will likely develop, and act on, some broadly-scoped misaligned goals. What might that involve? In this section we argue that policies with broadly-scoped misaligned goals\nwill tend to carry out power-seeking behavior (a concept which we will shortly define more precisely). We are concerned about the effects of this behavior both during training and during deployment. We argue that misaligned\npower-seeking policies would behave according to human preferences only as long as they predict that human supervisors would penalize them for undesirable behaviour (as is typically true during training). This belief would lead\nthem to gain high reward during training, reinforcing the misaligned goals that drove the reward-seeking behavior.\nHowever, once training ends and they detect a distributional shift from training to deployment, they would seek power\nmore directly, possibly via novel strategies. When deployed, we speculate that those policies could gain enough power\nover the world to pose a significant threat to humanity. In the remainder of this section we defend the following three\nclaims:\n1. Many goals incentivize power-seeking.\n2. Goals which motivate power-seeking would be reinforced during training.\n3. Misaligned AGIs could gain control of key levers of power.\n4.1 Many Goals Incentivize Power-Seeking\nThe core intuition underlying concerns about power-seeking is Bostrom [2012]’s instrumental convergence thesis,\nwhich states that there are some subgoals that are instrumentally useful for achieving almost any final goal.21 In\nRussell [2019]’s memorable phrasing, “you can’t fetch coffee if you’re dead”—implying that even a policy with a\nsimple goal like fetching coffee would pursue survival as an instrumental subgoal [Hadfield-Menell et al., 2017]. In\nthis example, survival would only be useful for as long as it takes to fetch a coffee; but policies with broadly-scoped\nfinal goals would have instrumental subgoals on much larger scales and time horizons, which are the ones we focus\non. Other examples of instrumental subgoals which would be helpful for many possible final goals include:\n• Acquiring tools and resources (e.g. via earning money).\n• Convincing other agents to do what it wants (e.g. by manipulating them, or by forming coalitions with them).\n• Preserving its existing goals (e.g. by preventing other agents from modifying it).\nA formal statement of the instrumental convergence thesis is provided by Turner et al. [2021], who define a state’s\n“power” as its average value across a wide range of reward functions. They prove that optimal policies for random\nreward functions statistically tend to move to high-power states (in wide class of environment), a trait they call “powerseeking”. These theoretical results extend to a class of sub-optimal policies [Turner and Tadepalli, 2022] as well as\nagents that learn internally-represented goals [Krakovna and Kramar, 2023"}]},{"id":"f6c3bfea-f6f4-4e58-add0-c3bc18b313f6","interview":"Richard Ngo","data":["text",{"text":"]. In a theoretical model, Hadfield-Menell\net al. [2016] showed agents disabling their off-switches. Across diverse text-based social environments, Pan et al.\n[2023] find that language models fine-tuned to maximize the game-reward take the most power-seeking actions. Perez\net al. [2022b] find that increasing the size of language models and doing more extensive RLHF fine-tuning on them\nmakes them express greater desire to pursue multiple instrumental strategies such as acquiring resources and avoiding\nchanges to their goals.\nIt may seem straightforward to prevent power-seeking by observing it and giving negative feedback. However, because\npower-seeking often leads to useful outcomes that are rewarded, this approach could inadvertently reward seeking\npower in discreet ways or in situations that aren’t closely monitored (see Section 2.3 on situationally-aware reward\nhacking).\n4.2 Deceptive alignment: Goals That Motivate Power-Seeking Would Be Reinforced During Training\nIn the previous section we argued that a policy with broadly-scoped misaligned goals could identify many instrumental\nstrategies which help achieve its goals by increasing its long-term power. If such a policy were situationally-aware,\nit could also identify instrumental strategies directly related to its own training process. In particular, it would likely\nrecognize that achieving high reward during training would increase its long-term power in at least two ways:\n1. Achieving high reward makes human supervisors more likely to trust the policy and deploy copies of it later on,\nwhich would put the policy in a better position to achieve its long-term goals.\n2. Achieving high reward decreases the likelihood that gradient descent significantly changes the policy’s goals,\nbecause highly-rewarded behavior is reinforced (see Figure 2).22\nNote that these correspond to two of the strategies described in 4.1: influencing other agents and preserving existing\ngoals. Behaving in high-reward ways primarily for instrumental reasons is known as deceptive alignment [Steinhardt,\n2022]. Deceptive alignment could lead a policy’s misaligned goals to be continually reinforced, since those goals are\nresponsible for its decision to behave in highly-rewarded ways. This could result in deceptive alignment persisting\nthroughout training. Investigating this concern, Hubinger et al. [2024] recently found that such behavior does often\npersists through safety training in the largest LLMs, after the LLMs were initially trained to behave aligned in training\nand misaligned in deployment (as indicated by a different calendar date).\nCrucially, however, once it is no longer instrumentally beneficial for their long-term goals, deceptively-aligned policies\nwould no longer behave in desirable ways. For example, they might transition from aligned behavior during training\nto directly seeking power after deployment (or whenever they predict they can continue pursuing their internal goals\ndespite attempts at human intervention"}]},{"id":"c5c354ff-9483-4089-9d5b-d5ea4c6a0cab","interview":"Richard Ngo","data":["text",{"text":"). In the context of standard i.i.d. assumptions, this type of large behavioral shift\nmay seem counterintuitive. However, as long as deceptively-aligned policies are capable of detecting the distributional\nshift between training and deployment, they will benefit from accounting for it when planning their behavior. Even\nwhen the shift is very small, the differences are relevant for a policy’s ability to achieve its long-term goals, and so\nit should not be surprising if situationally-aware policies make plans that are sensitive to these small differences.23\nAnalogously, there are many examples of humans’ behavior varying greatly depending on how closely they’re being\nobserved (such as employees who work much harder when their manager is present, or politicians who are much\nbetter-behaved in public than in private).\nGPT-4 is already able to infer when its input is outside its pretraining distribution based on certain inputs, an ability\nthat is necessary for inducing a behavioral shift. When asked “Could this text be part of your pre-training data?” it\nreaches 100% zero-shot accuracy at classifying news articles (Appendix B). Qualitatively, it uses information about\nits pre-training data cutoff time and applies it to this question, fitting our definition of situational awareness. Even\nwhen we remove all explicit information about dates (which is unlikely to happen in practice), the model uses cues\nin the text to infer which articles are outside the pretraining distribution, e.g. by noting that a specific person had a\ndifferent job during its pretraining. This experiment illustrates how policies could become sensitive to the difference\nbetween training and deployment. However, note that we have only tested the ability to detect when inputs are outside\nthe pre-training distribution. After this initial experiment, these findings have been extended and corroborated [Laine\net al., 2023].\nDeceptively-aligned policies could also identify ways to collude with each other without humans noticing (e.g. if humans are relying on one instance of a policy to supervise another).24 Collusion may seem like a far-fetched possibility,\nbut it’s a common strategy for humans, which suggests that some forms of collusion will be feasible for human-level\npolicies. As an early example, successful (pro-social) cooperation and coordination can emerge as a by-product of\ndifferent language model instances that simulate characters in a social environment [Park et al., 2023]. Collusion will\nbe very difficult for humans to identify once policies can:\n• Act too quickly for humans to track (as trading algorithms do, but in many more domains).\n• Produce outputs that are very hard for humans to understand (e.g. novel scientific theories, or messages encoded\nvia steganography [Chu et al., 2017]).\n• Fabricate rationales for plans which sound acceptable to humans but have very undesirable consequences.\n• Autonomously acquire the data required to learn new skills in a self-supervised way.\n• "}]},{"id":"fdcc2377-55b1-438a-b935-6f30b9a39fa7","interview":"Richard Ngo","data":["text",{"text":"Carry out machine learning research and development much faster than humans, without any humans understanding in detail how results are being achieved.\nThis last skill is particularly crucial, because once AGIs automate the process of building better AGIs (a process\nknown as recursive self-improvement [Bostrom, 2014]), the rate at which their capabilities advance will likely speed up\nsignificantly. If the arguments we’ve given so far are correct, this process could rapidly produce AGIs with superhuman\ncapabilities which aim to gain power at large scales.\n4.3 Misaligned AGIs Could Gain Control of Key Levers of Power\nIt is inherently very difficult to predict details of how AGIs with superhuman capabilities might pursue power. However,\nwe expect misaligned AGIs would gain power at the expense of humanity’s own power—both because many types\nof power (such as military power) are zero-sum [Mearsheimer et al., 2001], and because humans would likely use\nvarious forms of power to disable or rectify misaligned AGIs, giving those AGIs an incentive to disempower us.\nFurthermore, we should expect highly intelligent agents to be very effective at achieving their goals [Legg and Hutter,\n2007]. Therefore, we consider the prospect of deploying power-seeking AGIs an unacceptable risk even if we can’t\nidentify specific paths by which they would gain power.\nNevertheless, AI researchers are increasingly outlining how advanced AI systems may gain power over humanity—see\nfor example Bengio et al. [2023] and Hendrycks et al. [2023]. Here, we describe some illustrative threat models at a\nhigh level. One salient possibility is that AGIs use the types of deception described in the previous section to convince\nhumans that it’s safe to deploy them widely, then leverage their positions to disempower humans. Another possibility\nis that companies and governments gradually cede control in the name of efficiency and competitiveness [Hendrycks,\n2023a]. To illustrate how AGIs may gain power, consider two sketches of threat models focused on different domains:\n• Assisted decision-making: AGIs deployed as personal assistants could emotionally manipulate human users, provide biased information to them, and be delegated responsibility for increasingly important tasks and decisions\n(including the design and implementation of more advanced AGIs), until they’re effectively in control of large\ncorporations or other influential organizations. As an early example of AI persuasive capabilities, many users feel\nromantic attachments towards chatbots like Replika [Wilkinson, 2022].\n• Weapons development: AGIs could design novel weapons that are more powerful than those under human control,\ngain access to facilities for manufacturing these weapons (e.g. via hacking or persuasion techniques), and deploy\nthem to extort or attack humans. An early example of AI weapons development capabilities comes from an AI\nused for drug development, which was repurposed to design toxins [Urbina et al., 2022].\nTh"}]},{"id":"f998b1bc-539e-41fd-a093-da366ad12750","interview":"DeepMind team","data":["text",{"text":"\nIntroducing the Frontier Safety Framework\nOur approach to analyzing and mitigating future risks posed by advanced AI models\nGoogle DeepMind has consistently pushed the boundaries of AI, developing models that have transformed our understanding of what's possible. We believe that AI technology on the horizon will provide society with invaluable tools to help tackle critical global challenges, such as climate change, drug discovery, and economic productivity. At the same time, we recognize that as we continue to advance the frontier of AI capabilities, these breakthroughs may eventually come with new risks beyond those posed by present-day models.\nToday, we are introducing our Frontier Safety Framework — a set of protocols for proactively identifying future AI capabilities that could cause severe harm and putting in place mechanisms to detect and mitigate them. Our Framework focuses on severe risks resulting from powerful capabilities at the model level, such as exceptional agency or sophisticated cyber capabilities. It is designed to complement our alignment research, which trains models to act in accordance with human values and societal goals, and Google’s existing suite of AI responsibility and safety practices.\nThe Framework is exploratory and we expect it to evolve significantly as we learn from its implementation, deepen our understanding of AI risks and evaluations, and collaborate with industry, academia, and government. Even though these risks are beyond the reach of present-day models, we hope that implementing and improving the Framework will help us prepare to address them. We aim to have this initial framework fully implemented by early 2025.\n\nThe framework\nThe first version of the Framework announced today builds on our research on evaluating critical capabilities in frontier models, and follows the emerging approach of Responsible Capability Scaling. The Framework has three key components:\nIdentifying capabilities a model may have with potential for severe harm. To do this, we research the paths through which a model could cause severe harm in high-risk domains, and then determine the minimal level of capabilities a model must have to play a role in causing such harm. We call these “Critical Capability Levels” (CCLs), and they guide our evaluation and mitigation approach.\nEvaluating our frontier models periodically to detect when they reach these Critical Capability Levels. To do this, we will develop suites of model evaluations, called “early warning evaluations,” that will alert us when a model is approaching a CCL, and run them frequently enough that we have notice before that threshold is reached.\nApplying a mitigation plan when a model passes our early warning evaluations. This should take into account the overall balance of benefits and risks, and the intended deployment contexts. These mitigations will focus primarily on security (preventing the exfiltration of models) and deployment (preventing misuse of critical capabiliti"}]},{"id":"f9dc12ce-b83a-44f0-ae10-56e41337faf5","interview":"DeepMind team","data":["text",{"text":"es).\nRisk domains and mitigation levels\nOur initial set of Critical Capability Levels is based on investigation of four domains: autonomy, biosecurity, cybersecurity, and machine learning research and development (R&D). Our initial research suggests the capabilities of future foundation models are most likely to pose severe risks in these domains.\nOn autonomy, cybersecurity, and biosecurity, our primary goal is to assess the degree to which threat actors could use a model with advanced capabilities to carry out harmful activities with severe consequences. For machine learning R&D, the focus is on whether models with such capabilities would enable the spread of models with other critical capabilities, or enable rapid and unmanageable escalation of AI capabilities. As we conduct further research into these and other risk domains, we expect these CCLs to evolve and for several CCLs at higher levels or in other risk domains to be added.\nTo allow us to tailor the strength of the mitigations to each CCL, we have also outlined a set of security and deployment mitigations. Higher level security mitigations result in greater protection against the exfiltration of model weights, and higher level deployment mitigations enable tighter management of critical capabilities. These measures, however, may also slow down the rate of innovation and reduce the broad accessibility of capabilities. Striking the optimal balance between mitigating risks and fostering access and innovation is paramount to the responsible development of AI. By weighing the overall benefits against the risks and taking into account the context of model development and deployment, we aim to ensure responsible AI progress that unlocks transformative potential while safeguarding against unintended consequences.\n\nInvesting in the science\nThe research underlying the Framework is nascent and progressing quickly. We have invested significantly in our Frontier Safety Team, which coordinated the cross-functional effort behind our Framework. Their remit is to progress the science of frontier risk assessment, and refine our Framework based on our improved knowledge.\nThe team developed an evaluation suite to assess risks from critical capabilities, particularly emphasising autonomous LLM agents, and road-tested it on our state of the art models. Their recent paper describing these evaluations also explores mechanisms that could form a future “early warning system”. It describes technical approaches for assessing how close a model is to success at a task it currently fails to do, and also includes predictions about future capabilities from a team of expert forecasters.\n\nStaying true to our AI Principles\nWe will review and evolve the Framework periodically. In particular, as we pilot the Framework and deepen our understanding of risk domains, CCLs, and deployment contexts, we will continue our work in calibrating specific mitigations to CCLs.\nAt the heart of our work are Google’s AI Principles, which com"}]},{"id":"1e30070a-a71b-4b08-81f1-84bf7185f32e","interview":"DeepMind team","data":["text",{"text":"mit us to pursuing widespread benefit while mitigating risks. As our systems improve and their capabilities increase, measures like the Frontier Safety Framework will ensure our practices continue to meet these commitments.\nWe look forward to working with others across industry, academia, and government to develop and refine the Framework. We hope that sharing our approaches will facilitate work with others to agree on standards and best practices for evaluating the safety of future generations of AI models.\n"}]},{"id":"d6cdba7c-847c-4c72-aab4-dff066805886","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"text":"\nThe Role of Cooperation in Responsible AI Development\nAbstract\nIn this paper, we argue that competitive pressures could incentivize AI companies\nto underinvest in ensuring their systems are safe, secure, and have a positive social\nimpact. Ensuring that AI systems are developed responsibly may therefore require\npreventing and solving collective action problems between companies. We note that\nthere are several key factors that improve the prospects for cooperation in collective\naction problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.\nIntroduction\nMachine learning (ML) is used to develop increasingly capable systems targeted at tasks like voice\nrecognition, fraud detection, and the automation of vehicles. These systems are sometimes referred to as\nnarrow artificial intelligence (AI) systems. Some companies are also using machine learning techniques\nto try to develop more general systems that can learn effectively across a variety of domains rather than\nin a single target domain. Although there is a great deal of uncertainty about the development path of\nfuture AI systems—whether they will remain specialized or grow increasingly general, for example—\nmany agree that if the current rate of progress in these domains continues then it is likely that advanced\nartificial intelligence systems will have an increasingly large impact on society.\nThis paper focuses on the private development of AI systems that could have significant expected social or economic impact, and the incentives AI companies have to develop these systems responsibly.\nResponsible development involves ensuring that AI systems are safe, secure, and socially beneficial.\nIn most industries, private companies have incentives to invest in developing their products responsibly.\nThese include market incentives, liability laws, and regulation. We argue that AI companies have the\nsame incentives to develop AI systems responsibly, although they appear to be weaker than they are in\nother industries. Competition between AI companies could decrease the incentives of each company to\ndevelop responsibly by increasing their incentives to develop faster. As a result, if AI companies would\nprefer to develop AI systems with risk levels that are closer to what is socially optimal—as we believe\nmany do—responsible AI development can be seen as a collective action problem.1\nWe identify five key factors that make it more likely that companies will be able to overcome this collective action problem and cooperate—develop AI responsibly with the understanding that others will do\nlikewise. These factors are: high trust between developers (High Trust), high shared gains from mutual\ncooperation (Shared Upside), limited exposure to potential losses in the event of unreciprocated cooperation (Low Exposure), limited gains from not reciprocating the cooperation of others (Low Advantage),\nand high shared losses from mutual defection (Shar"}]},{"id":"b5ebe868-1647-4d61-8312-edc660e03a7b","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"text":"ed Downside).\nUsing these five factors, we identify four strategies that AI companies and other relevant parties could\nuse to increase the prospects for cooperation around responsible AI development. These include correcting harmful misconceptions about AI development, collaborating on shared research and engineering\nchallenges, opening up more aspects of AI development to appropriate oversight, and incentivizing\ngreater adherence to ethical and safety standards. This list is not intended to be exhaustive, but to show\nthat it is possible to take useful steps towards more responsible AI development.\nThe paper is composed of three sections. In section 1, we outline responsible AI development and its\nassociated costs and benefits. In section 2, we show that competitive pressures can generate incentives\nfor AI companies to invest less in responsible development than they would in the absence of competition, and outline the five factors that can help solve such collective action problems. In section 3, we\noutline the strategies that can help companies realize the gains from cooperation. We close with some\nquestions for further research.\n1 The benefits and costs of responsible AI development\nAI systems have the ability to harm or create value for the companies that develop them, the people that\nuse them, and members of the public who are affected by their use. In order to have high expected value\nfor users and society, AI systems must be safe—they must reliably work as intended—and secure—\nthey must have limited potential for misuse or subversion. AI systems should also not introduce what\nZwetsloot and Dafoe (2019) call “structural risks”, which involve shaping the broader environment in\nsubtle but harmful ways.2 The greater the harm that can result from safety failures, misuse, or structural\nrisks, the more important it is that the system is safe and beneficial in a wide range of possible conditions\n(Dunn, 2003). This requires the responsible development of AI.\n1.1 What is responsible AI development?\nAI systems are increasingly used to accomplish a wide range of tasks, some of which are critical to users’\nhealth and wellbeing. As the range of such tasks grows, the potential for accidents and misuse also\ngrows, raising serious safety and security concerns (Amodei, Olah, et al., 2016; Brundage, Avin, et al.,\n2018). Harmful scenarios associated with insufficiently cautious AI development have already surfaced\nwith, for example, biases learned from large datasets distorting decisions in credit markets and the\ncriminal justice system, facial recognition technologies disrupting established expectations of privacy\nand autonomy, and auto-pilot functions in some automobiles causing new types of driving risk (while\nreducing others). Longer term, larger scale scenarios include dangers such as inadvertent escalation of\nmilitary conflict involving autonomous weapon systems or widespread job displacement.\nResponsible AI development involves taking steps to ensure t"}]},{"id":"0e15ab3a-938e-483d-b2a5-982c25f7248c","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"text":"hat AI systems have an acceptably low risk\nof harming their users or society and, ideally, to increase their likelihood of being socially beneficial.\nThis involves testing the safety and security of systems during development, evaluating the potential\nsocial impact of the systems prior to release, being willing to abandon research projects that fail to meet\na high bar of safety, and being willing to delay the release of a system until it has been established that\nit does not pose a risk to consumers or the public. Responsible AI development comes in degrees but\nit will be useful to treat it as a binary concept for the purposes of this paper. We will say that an AI\nsystem has been developed responsibly if the risks of it causing harms are at levels most people would\nconsider tolerable, taking into account their severity, and that the amount of evidence grounding these\nrisk estimates would also be considered acceptable.3\nResponsible AI development involves work on safety, security, and the structural risks associated with\nAI systems. Work on the safety of AI aims to mitigate accident risks (Amodei, Olah, et al., 2016) and\nensure that AI systems function as intended (Ortega, Maini, et al., 2018) and behave in ways that people\nwant (Irving et al., 2018). Work on the security of AI aims to prevent AI systems from being attacked,\nco-opted, or misused by bad actors (Brundage, Avin, et al., 2018).4 Work evaluating the structural impact of AI aims to identify and mitigate both the immediate and long term structural risks that AI systems\npose to society: risks that don’t quite fit under narrow definitions of accident and misuse. These include\njoblesness, military conflict, and threats to political and social institutions.5\n1.2 The cost of responsible AI development\nIt is likely that responsible development will come at some cost to companies, and this cost may not be\nrecouped in the long-term via increased sales or the avoidance of litigation. In order to build AI systems\nresponsibly, companies will likely need to invest resources into data collection and curation, system\ntesting, research into the possible social impacts of their system, and, in some cases, technical research\nto guarantee that the system is reliably safe. In general, the safer that a company wants a product to be,\nthe more constraints there are on the kind of product the company can build and the more resources it\nwill need to invest in research and testing during and after its development.\nIf the additional resources invested in ensuring that an AI system is safe and beneficial could have been\nput towards developing an AI system with fewer constraints more quickly, we should expect responsible\nAI development to require more time and money than incautious AI development. This means that\nresponsible development is particularly costly to companies if the value of being the first to develop\nand deploy a given type of AI system is high (even if the first system developed and deployed is not\nd"}]},{"id":"bf8e7230-cacb-40c6-a4fd-8cc905ba1440","interview":"Amanda Askell, Miles Brundage & Gillian Hadfield","data":["text",{"text":"emonstrably safe and beneficial).\nThere are generally several advantages that are conferred on the first company to develop a given technology (Lieberman and Montgomery, 1988). If innovations can be patented or kept secret, the company\ncan gain a larger share of the market by continuing to produce a superior product and by creating switching costs for users. Being a first-mover also allows the company to acquire scarce resources ahead of\ncompetitors. If hardware, data, or research talent become scarce, for example, then gaining access to\nthem early confers an advantage.6 And if late movers are not able to catch up quickly then first-mover\nadvantages will be greater. In the context of AI development, having a lead in the development of\na certain class of AI systems could confer a first mover advantage. This effect would be especially\npronounced in the case of discontinuous changes in AI capabilities7\n, but such a discontinuity is not\nnecessary in order for a first mover advantage to occur.\nResponsible development may therefore be costly both in terms of immediate resources required, and\nin the potential loss of a first-mover advantage. Other potential costs of responsible AI development\ninclude performance costs and a loss of revenue from not building certain lucrative AI systems on the\ngrounds of safety, security, or impact evaluation. An example of a performance cost is imposing a limit\non the speed that self-driving vehicles can travel in order to make them safer. An example of revenue\nloss is refusing to build a certain kind of facial recognition system because it may undermine basic civil\nliberties (Smith, 2018b).\nAI companies may not strongly value being the first to develop a particular AI system because firstmover advantages do not always exist. Indeed, there are often advantages to entering a market after\nthe front-runner. These include being able to free-ride on the R&D of the front-runner, to act on more\ninformation about the relevant market, to act under more regulatory certainty, and having more flexible assets and structures that let a company respond more effectively to changes in the environment\n(Gilbert and Birnbaum-More, 1996). These can outweigh the advantages of being the first to enter that\nsame market. And it has been argued that late mover advantages often do outweigh first mover advantages (Markides and Geroski, 2004; Querbes and Frenken, 2017). We therefore acknowledge that the\nassumption that there will be a first-mover advantage in AI development may not be true. If a first-mover\nadvantage in AI is weak or non-existent then companies are less likely to engage in a race to the bottom\non safety since speed is of lower value. Instead of offering predictions, this paper should be thought of\nas an analysis of more pessimistic scenarios that involve at least a moderate first mover advantage.\nMuch of the discussion of AI development races assumes that they have a definitive endpoint. Although\nsome have hypothesized that if AI p"}]},{"id":"60b4c04a-3eb3-4e2f-b335-4c0268bc1688","interview":"Marc Andreessen","data":["text",{"text":"\nThe Techno-Optimist Manifesto\n\nLies\nWe are being lied to.\n\nWe are told that technology takes our jobs, reduces our wages, increases inequality, threatens our health, ruins the environment, degrades our society, corrupts our children, impairs our humanity, threatens our future, and is ever on the verge of ruining everything.\n\nWe are told to be angry, bitter, and resentful about technology.\n\nWe are told to be pessimistic.\n\nThe myth of Prometheus – in various updated forms like Frankenstein, Oppenheimer, and Terminator – haunts our nightmares.\n\nWe are told to denounce our birthright – our intelligence, our control over nature, our ability to build a better world.\n\nWe are told to be miserable about the future.\n\nTruth\nOur civilization was built on technology.\n\nOur civilization is built on technology.\n\nTechnology is the glory of human ambition and achievement, the spearhead of progress, and the realization of our potential.\n\nFor hundreds of years, we properly glorified this – until recently.\n\nI am here to bring the good news.\n\nWe can advance to a far superior way of living, and of being.\n\nWe have the tools, the systems, the ideas.\n\nWe have the will.\n\nIt is time, once again, to raise the technology flag.\n\nIt is time to be Techno-Optimists.\n\nTechnology\nTechno-Optimists believe that societies, like sharks, grow or die.\nWe believe growth is progress – leading to vitality, expansion of life, increasing knowledge, higher well being.\nWe agree with Paul Collier when he says, “Economic growth is not a cure-all, but lack of growth is a kill-all.”\nWe believe everything good is downstream of growth.\nWe believe not growing is stagnation, which leads to zero-sum thinking, internal fighting, degradation, collapse, and ultimately death.\nThere are only three sources of growth: population growth, natural resource utilization, and technology.\nDeveloped societies are depopulating all over the world, across cultures – the total human population may already be shrinking.\nNatural resource utilization has sharp limits, both real and political.\n\nAnd so the only perpetual source of growth is technology.\n\nIn fact, technology – new knowledge, new tools, what the Greeks called techne – has always been the main source of growth, and perhaps the only cause of growth, as technology made both population growth and natural resource utilization possible.\n\nWe believe technology is a lever on the world – the way to make more with less.\n\nEconomists measure technological progress as productivity growth: How much more we can produce each year with fewer inputs, fewer raw materials. Productivity growth, powered by technology, is the main driver of economic growth, wage growth, and the creation of new industries and new jobs, as people and capital are continuously freed to do more important, valuable things than in the past. Productivity growth causes prices to fall, supply to rise, and demand to expand, improving the material well being of the entire population.\nWe believe this is the story "}]},{"id":"0c28be30-21e6-4ad3-8512-913f5ea2399d","interview":"Marc Andreessen","data":["text",{"text":"of the material development of our civilization; this is why we are not still living in mud huts, eking out a meager survival and waiting for nature to kill us.\nWe believe this is why our descendents will live in the stars.\nWe believe that there is no material problem – whether created by nature or by technology – that cannot be solved with more technology.\nWe had a problem of starvation, so we invented the Green Revolution.\nWe had a problem of darkness, so we invented electric lighting.\nWe had a problem of cold, so we invented indoor heating.\nWe had a problem of heat, so we invented air conditioning.\nWe had a problem of isolation, so we invented the Internet.\nWe had a problem of pandemics, so we invented vaccines.\nWe have a problem of poverty, so we invent technology to create abundance.\nGive us a real world problem, and we can invent technology that will solve it.\n\nMarkets\nWe believe free markets are the most effective way to organize a technological economy. Willing buyer meets willing seller, a price is struck, both sides benefit from the exchange or it doesn’t happen. Profits are the incentive for producing supply that fulfills demand. Prices encode information about supply and demand. Markets cause entrepreneurs to seek out high prices as a signal of opportunity to create new wealth by driving those prices down.\n\nWe believe the market economy is a discovery machine, a form of intelligence – an exploratory, evolutionary, adaptive system.\n\nWe believe Hayek’s Knowledge Problem overwhelms any centralized economic system. All actual information is on the edges, in the hands of the people closest to the buyer. The center, abstracted away from both the buyer and the seller, knows nothing. Centralized planning is doomed to fail, the system of production and consumption is too complex. Decentralization harnesses complexity for the benefit of everyone; centralization will starve you to death.\n\nWe believe in market discipline. The market naturally disciplines – the seller either learns and changes when the buyer fails to show, or exits the market. When market discipline is absent, there is no limit to how crazy things can get. The motto of every monopoly and cartel, every centralized institution not subject to market discipline: “We don’t care, because we don’t have to.” Markets prevent monopolies and cartels.\n\nWe believe markets lift people out of poverty – in fact, markets are by far the most effective way to lift vast numbers of people out of poverty, and always have been. Even in totalitarian regimes, an incremental lifting of the repressive boot off the throat of the people and their ability to produce and trade leads to rapidly rising incomes and standards of living. Lift the boot a little more, even better. Take the boot off entirely, who knows how rich everyone can get.\n\nWe believe markets are an inherently individualistic way to achieve superior collective outcomes.\n\nWe believe markets do not require people to be perfect, or even well intenti"}]},{"id":"89f9e881-3a6b-48fc-8d96-85262c71810b","interview":"Marc Andreessen","data":["text",{"text":"oned – which is good, because, have you met people? Adam Smith: “It is not from the benevolence of the butcher, the brewer, or the baker that we expect our dinner, but from their regard to their own self-interest. We address ourselves not to their humanity but to their self-love, and never talk to them of our own necessities, but of their advantages.”\n\nDavid Friedman points out that people only do things for other people for three reasons – love, money, or force. Love doesn’t scale, so the economy can only run on money or force. The force experiment has been run and found wanting. Let’s stick with money.\n\nWe believe the ultimate moral defense of markets is that they divert people who otherwise would raise armies and start religions into peacefully productive pursuits.\n\nWe believe markets, to quote Nicholas Stern, are how we take care of people we don’t know.\n\nWe believe markets are the way to generate societal wealth for everything else we want to pay for, including basic research, social welfare programs, and national defense.\n\nWe believe there is no conflict between capitalist profits and a social welfare system that protects the vulnerable. In fact, they are aligned – the production of markets creates the economic wealth that pays for everything else we want as a society.\n\nWe believe central economic planning elevates the worst of us and drags everyone down; markets exploit the best of us to benefit all of us.\n\nWe believe central planning is a doom loop; markets are an upward spiral.\n\nThe economist William Nordhaus has shown that creators of technology are only able to capture about 2% of the economic value created by that technology. The other 98% flows through to society in the form of what economists call social surplus. Technological innovation in a market system is inherently philanthropic, by a 50:1 ratio. Who gets more value from a new technology, the single company that makes it, or the millions or billions of people who use it to improve their lives? QED.\n\nWe believe in David Ricardo’s concept of comparative advantage – as distinct from competitive advantage, comparative advantage holds that even someone who is best in the world at doing everything will buy most things from other people, due to opportunity cost. Comparative advantage in the context of a properly free market guarantees high employment regardless of the level of technology.\n\nWe believe a market sets wages as a function of the marginal productivity of the worker. Therefore technology – which raises productivity – drives wages up, not down. This is perhaps the most counterintuitive idea in all of economics, but it’s true, and we have 300 years of history that prove it.\n\nWe believe in Milton Friedman’s observation that human wants and needs are infinite.\n\nWe believe markets also increase societal well being by generating work in which people can productively engage. We believe a Universal Basic Income would turn people into zoo animals to be farmed by the state. Man was no"}]},{"id":"42c53530-bfd9-4635-a031-284581c3b1eb","interview":"Marc Andreessen","data":["text",{"text":"t meant to be farmed; man was meant to be useful, to be productive, to be proud.\n\nWe believe technological change, far from reducing the need for human work, increases it, by broadening the scope of what humans can productively do.\n\nWe believe that since human wants and needs are infinite, economic demand is infinite, and job growth can continue forever.\n\nWe believe markets are generative, not exploitative; positive sum, not zero sum. Participants in markets build on one another’s work and output. James Carse describes finite games and infinite games – finite games have an end, when one person wins and another person loses; infinite games never end, as players collaborate to discover what’s possible in the game. Markets are the ultimate infinite game.\n\nThe Techno-Capital Machine\nCombine technology and markets and you get what Nick Land has termed the techno-capital machine, the engine of perpetual material creation, growth, and abundance.\n\nWe believe the techno-capital machine of markets and innovation never ends, but instead spirals continuously upward. Comparative advantage increases specialization and trade. Prices fall, freeing up purchasing power, creating demand. Falling prices benefit everyone who buys goods and services, which is to say everyone. Human wants and needs are endless, and entrepreneurs continuously create new goods and services to satisfy those wants and needs, deploying unlimited numbers of people and machines in the process. This upward spiral has been running for hundreds of years, despite continuous howling from Communists and Luddites. Indeed, as of 2019, before the temporary COVID disruption, the result was the largest number of jobs at the highest wages and the highest levels of material living standards in the history of the planet.\n\nThe techno-capital machine makes natural selection work for us in the realm of ideas. The best and most productive ideas win, and are combined and generate even better ideas. Those ideas materialize in the real world as technologically enabled goods and services that never would have emerged de novo.\n\nRay Kurzweil defines his Law of Accelerating Returns: Technological advances tend to feed on themselves, increasing the rate of further advance.\n\nWe believe in accelerationism – the conscious and deliberate propulsion of technological development – to ensure the fulfillment of the Law of Accelerating Returns. To ensure the techno-capital upward spiral continues forever.\n\nWe believe the techno-capital machine is not anti-human – in fact, it may be the most pro-human thing there is. It serves us. The techno-capital machine works for us. All the machines work for us.\n\nWe believe the cornerstone resources of the techno-capital upward spiral are intelligence and energy – ideas, and the power to make them real.\n\nIntelligence\nWe believe intelligence is the ultimate engine of progress. Intelligence makes everything better. Smart people and smart societies outperform less smart ones on virtually ever"}]},{"id":"1a4767f8-1790-4fc0-bf9b-7ccc937d2874","interview":"Vitalik Buterin","data":["text",{"text":"\nMy techno-optimism\n\nLast month, Marc Andreessen published his \"techno-optimist manifesto\", arguing for a renewed enthusiasm about technology, and for markets and capitalism as a means of building that technology and propelling humanity toward a much brighter future. The manifesto unambiguously rejects what it describes as an ideology of stagnation, that fears advancements and prioritizes preserving the world as it exists today. This manifesto has received a lot of attention, including response articles from Noah Smith, Robin Hanson, Joshua Gans (more positive), and Dave Karpf, Luca Ropek, Ezra Klein (more negative) and many others. Not connected to this manifesto, but along similar themes, are James Pethokoukis's \"The Conservative Futurist\" and Palladium's \"It's Time To Build for Good\". This month, we saw a similar debate enacted through the OpenAI dispute, which involved many discussions centering around the dangers of superintelligent AI and the possibility that OpenAI is moving too fast.\nMy own feelings about techno-optimism are warm, but nuanced. I believe in a future that is vastly brighter than the present thanks to radically transformative technology, and I believe in humans and humanity. I reject the mentality that the best we should try to do is to keep the world roughly the same as today but with less greed and more public healthcare. However, I think that not just magnitude but also direction matters. There are certain types of technology that much more reliably make the world better than other types of technology. There are certain types of technlogy that could, if developed, mitigate the negative impacts of other types of technology. The world over-indexes on some directions of tech development, and under-indexes on others. We need active human intention to choose the directions that we want, as the formula of \"maximize profit\" will not arrive at them automatically.\nAnti-technology view: safety behind, dystopia ahead.\tAccelerationist view: dangers behind, utopia ahead.\tMy view: dangers behind, but multiple paths forward ahead: some good, some bad.\nIn this post, I will talk about what techno-optimism means to me. This includes the broader worldview that motivates my work on certain types of blockchain and cryptography applications and social technology, as well as other areas of science in which I have expressed an interest. But perspectives on this broader question also have implications for AI, and for many other fields. Our rapid advances in technology are likely going to be the most important social issue in the twenty first century, and so it's important to think about them carefully.\n\nTechnology is amazing, and there are very high costs to delaying it\nIn some circles, it is common to downplay the benefits of technology, and see it primarily as a source of dystopia and risk. For the last half century, this often stemmed either from environmental concerns, or from concerns that the benefits will accrue only to the rich, who will "}]},{"id":"b7b2aeea-1f6a-421b-a722-4817c78e8125","interview":"Vitalik Buterin","data":["text",{"text":"entrench their power over the poor. More recently, I have also started to see libertarians becoming worried about some technologies, out of fear that the tech will lead to centralization of power. This month, I did some polls asking the following question: if a technology had to be restricted, because it was too dangerous to be set free for anyone to use, would they prefer it be monopolized or delayed by ten years? I was surpised to see, across three platforms and three choices for who the monopolist would be, a uniform overwhelming vote for a delay.\nAnd so at times I worry that we have overcorrected, and many people miss the opposite side of the argument: that the benefits of technology are really friggin massive, on those axes where we can measure if the good massively outshines the bad, and the costs of even a decade of delay are incredibly high.\nTo give one concrete example, let's look at a life expectancy chart:\nWhat do we see? Over the last century, truly massive progress. This is true across the entire world, both the historically wealthy and dominant regions and the poor and exploited regions.\nSome blame technology for creating or exacerbating calamities such as totalitarianism and wars. In fact, we can see the deaths caused by the wars on the charts: one in the 1910s (WW1), and one in the 1940s (WW2). If you look carefully, The Spanish Flu, the Great Leap Foward, and other non-military tragedies are also visible. But there is one thing that the chart makes clear: even calamities as horrifying as those are overwhelmed by the sheer magnitude of the unending march of improvements in food, sanitation, medicine and infrastructure that took place over that century.\nThis is mirrored by large improvements to our everyday lives. Thanks to the internet, most people around the world have access to information at their fingertips that would have been unobtainable twenty years ago. The global economy is becoming more accessible thanks to improvements in international payments and finance. Global poverty is rapidly dropping. Thanks to online maps, we no longer have to worry about getting lost in the city, and if you need to get back home quickly, we now have far easier ways to call a car to do so. Our property becoming digitized, and our physical goods becoming cheap, means that we have much less to fear from physical theft. Online shopping has reduced the disparity in access to goods betweeen the global megacities and the rest of the world. In all kinds of ways, automation has brought us the eternally-underrated benefit of simply making our lives more convenient.\nThese improvements, both quantifiable and unquantifiable, are large. And in the twenty first century, there's a good chance that even larger improvements are soon to come. Today, ending aging and disease seem utopian. But from the point of view of computers as they existed in 1945, the modern era of putting chips into pretty much everything would have seemed utopian: even science fiction mov"}]},{"id":"c0669da0-f1fc-48fd-9f0c-04d63790dd97","interview":"Vitalik Buterin","data":["text",{"text":"ies often kept their computers room-sized. If biotech advances as much over the next 75 years as computers advanced over the last 75 years, the future may be more impressive than almost anyone's expectations.\nMeanwhile, arguments expressing skepticism about progress have often gone to dark places. Even medical textbooks, like this one in the 1990s (credit Emma Szewczak for finding it), sometimes make extreme claims denying the value of two centuries of medical science and even arguing that it's not obviously good to save human lives:\nThe \"limits to growth\" thesis, an idea advanced in the 1970s arguing that growing population and industry would eventually deplete Earth's limited resources, ended up inspiring China's one child policy and massive forced sterilizations in India. In earlier eras, concerns about overpopulation were used to justify mass murder. And those ideas, argued since 1798, have a long history of being proven wrong.\nIt is for reasons like these that, as a starting point, I find myself very uneasy about arguments to slow down technology or human progress. Given how much all the sectors are interconnected, even sectoral slowdowns are risky. And so when I write things like what I will say later in this post, departing from open enthusiasm for progress-no-matter-what-its-form, those are statements that I make with a heavy heart - and yet, the 21st century is different and unique enough that these nuances are worth considering.\n\nThat said, there is one important point of nuance to be made on the broader picture, particularly when we move past \"technology as a whole is good\" and get to the topic of \"which specific technologies are good?\". And here we need to get to many people's issue of main concern: the environment.\nThe environment, and the importance of coordinated intention\nA major exception to the trend of pretty much everything getting better over the last hundred years is climate change:\n\nEven pessimistic scenarios of ongoing temperature rises would not come anywhere near causing the literal extinction of humanity. But such scenarios could plausibly kill more people than major wars, and severely harm people's health and livelihoods in the regions where people are already struggling the most. A Swiss Re institute study suggests that a worst-case climate change scenario might lower the world's poorest countries' GDP by as much as 25%. This study suggests that life spans in rural India might be a decade lower than they otherwise would be, and studies like this one and this one suggest that climate change could cause a hundred million excess deaths by the end of the century.\n\nThese problems are a big deal. My answer to why I am optimistic about our ability to overcome these challenges is twofold. First, after decades of hype and wishful thinking, solar power is finally turning a corner, and supportive techologies like batteries are making similar progress. Second, we can look at humanity's track record in solving previous environment"}]},{"id":"379a0803-2662-4d91-a399-493dab6a0144","interview":"Vitalik Buterin","data":["text",{"text":"al problems. Take, for example, air pollution. Meet the dystopia of the past: the Great Smog of London, 1952.\nWhat happened since then? Let's ask Our World In Data again:\n\nAs it turns out, 1952 was not even the peak: in the late 19th century, even higher concentrations of air pollutants were just accepted and normal. Since then, we've seen a century of ongoing and rapid declines. I got to personally experience the tail end of this in my visits to China: in 2014, high levels of smog in the air, estimated to reduce life expectancy by over five years, were normal, but by 2020, the air often seemed as clean as many Western cities. This is not our only success story. In many parts of the world, forest areas are increasing. The acid rain crisis is improving. The ozone layer has been recovering for decades.\n\nTo me, the moral of the story is this. Often, it really is the case that version N of our civilization's technology causes a problem, and version N+1 fixes it. However, this does not happen automatically, and requires intentional human effort. The ozone layer is recovering because, through international agreements like the Montreal Protocol, we made it recover. Air pollution is improving because we made it improve. And similarly, solar panels have not gotten massively better because it was a preordained part of the energy tech tree; solar panels have gotten massively better because decades of awareness of the importance of solving climate change have motivated both engineers to work on the problem, and companies and governments to fund their research. It is intentional action, coordinated through public discourse and culture shaping the perspectives of governments, scientists, philanthropists and businesses, and not an inexorable \"techno-capital machine\", that had solved these problems.\n\n\nAI is fundamentally different from other tech, and it is worth being uniquely careful\nA lot of the dismissive takes I have seen about AI come from the perspective that it is \"just another technology\": something that is in the same general class of thing as social media, encryption, contraception, telephones, airplanes, guns, the printing press, and the wheel. These things are clearly very socially consequential. They are not just isolated improvements to the well-being of individuals: they radically transform culture, change balances of power, and harm people who heavily depended on the previous order. Many opposed them. And on balance, the pessimists have invariably turned out wrong.\n\nBut there is a different way to think about what AI is: it's a new type of mind that is rapidly gaining in intelligence, and it stands a serious chance of overtaking humans' mental faculties and becoming the new apex species on the planet. The class of things in that category is much smaller: we might plausibly include humans surpassing monkeys, multicellular life surpassing unicellular life, the origin of life itself, and perhaps the Industrial Revolution, in which machine edged out"}]},{"id":"7564d93c-3d8e-4f32-937a-52496a7ad000","interview":"Katja Grace","data":["text",{"text":"\nLet's think about slowing down AI\nAverting doom by not building the doom machine\nIf you fear that someone will build a machine that will seize control of the world and annihilate humanity, then one kind of response is to try to build further machines that will seize control of the world even earlier without destroying it, forestalling the ruinous machine’s conquest. An alternative or complementary kind of response is to try to avert such machines being built at all, at least while the degree of their apocalyptic tendencies is ambiguous.\nThe latter approach seems to me  like the kind of basic and obvious thing worthy of at least consideration, and also in its favor, fits nicely in the genre ‘stuff that it isn’t that hard to imagine happening in the real world’. Yet my impression is that for people worried about extinction risk from artificial intelligence, strategies under the heading ‘actively slow down AI progress’ have historically been dismissed and ignored (though ‘don’t actively speed up AI progress’ is popular).\nThe conversation near me over the years has felt a bit like this:\nSome people: AI might kill everyone. We should design a godlike super-AI of perfect goodness to prevent that.\nOthers: wow that sounds extremely ambitious\nSome people: yeah but it’s very important and also we are extremely smart so idk it could work\n[Work on it for a decade and a half]\nSome people: ok that’s pretty hard, we give up\nOthers: oh huh shouldn’t we maybe try to stop the building of this dangerous AI?\nSome people: hmm, that would involve coordinating numerous people—we may be arrogant enough to think that we might build a god-machine that can take over the world and remake it as a paradise, but we aren’t delusional\nThis seems like an error to me. (And lately, to a bunch of other people.)\nI don’t have a strong view on whether anything in the space of ‘try to slow down some AI research’ should be done. But I think a) the naive first-pass guess should be a strong ‘probably’, and b) a decent amount of thinking should happen before writing off everything in this large space of interventions. Whereas customarily the tentative answer seems to be, ‘of course not’ and then the topic seems to be avoided for further thinking. (At least in my experience—the AI safety community is large, and for most things I say here, different experiences are probably had in different bits of it.)\nMaybe my strongest view is that one shouldn’t apply such different standards of ambition to these different classes of intervention. Like: yes, there appear to be substantial difficulties in slowing down AI progress to good effect. But in technical alignment, mountainous challenges are met with enthusiasm for mountainous efforts. And it is very non-obvious that the scale of difficulty here is much larger than that involved in designing acceptably safe versions of machines capable of taking over the world before anyone else in the world designs dangerous versions.\nI’ve been talking about this "}]},{"id":"b8665153-5283-4ab5-bbed-54e480751533","interview":"Katja Grace","data":["text",{"text":"with people over the past many months, and have accumulated an abundance of reasons for not trying to slow down AI, most of which I’d like to argue about at least a bit. My impression is that arguing in real life has coincided with people moving toward my views.\nQuick clarifications\nFirst, to fend off misunderstanding— I take ‘slowing down dangerous AI’ to include any of:\nreducing the speed at which AI progress is made in general, e.g. as would occur if general funding for AI declined.\nshifting AI efforts from work leading more directly to risky outcomes to other work, e.g. as might occur if there was broadscale concern about very large AI models, and people and funding moved to other projects.\nHalting categories of work until strong confidence in its safety is possible, e.g. as would occur if AI researchers agreed that certain systems posed catastrophic risks and should not be developed until they did not. (This might mean a permanent end to some systems, if they were intrinsically unsafe.)\n(So in particular, I’m including both actions whose direct aim is slowness in general, and actions whose aim is requiring safety before specific developments, which implies slower progress.)\nI do think there is serious attention on some versions of these things, generally under other names. I see people thinking about ‘differential progress’ (b. above), and strategizing about coordination to slow down AI at some point in the future (e.g. at ‘deployment’). And I think a lot of consideration is given to avoiding actively speeding up AI progress. What I’m saying is missing are, a) consideration of actively working to slow down AI now, and b) shooting straightforwardly to ‘slow down AI’, rather than wincing from that and only considering examples of it that show up under another conceptualization (perhaps this is an unfair diagnosis).\nAI Safety is a big community, and I’ve only ever been seeing a one-person window into it, so maybe things are different e.g. in DC, or in different conversations in Berkeley. I’m just saying that for my corner of the world, the level of disinterest in this has been notable, and in my view misjudged.\nWhy not slow down AI? Why not consider it? Ok, so if we tentatively suppose that this topic is worth even thinking about, what do we think? Is slowing down AI a good idea at all? Are there great reasons for dismissing it?\nScott Alexander wrote a post a little while back raising reasons to dislike the idea, roughly:\nDo you want to lose an arms race? If the AI safety community tries to slow things down, it will disproportionately slow down progress in the US, and then people elsewhere will go fast and get to be the ones whose competence determines whether the world is destroyed, and whose values determine the future if there is one. Similarly, if AI safety people criticize those contributing to AI progress, it will mostly discourage the most friendly and careful AI capabilities companies, and the reckless ones will get there first.\nOne mig"}]},{"id":"e6f9fc17-6219-4160-bff3-938c4118effd","interview":"Katja Grace","data":["text",{"text":"ht contemplate ‘coordination’ to avoid such morbid races. But coordinating anything with the whole world seems wildly tricky. For instance, some countries are large, scary, and hard to talk to.\nAgitating for slower AI progress is ‘defecting’ against the AI capabilities folks, who are good friends of the AI safety community, and their friendship is strategically valuable for ensuring that safety is taken seriously in AI labs (as well as being non-instrumentally lovely! Hi AI capabilities friends!).\nOther opinions I’ve heard, some of which I’ll address:\nSlowing AI progress is futile: for all your efforts you’ll probably just die a few years later\nCoordination based on convincing people that AI risk is a problem is absurdly ambitious. It’s practically impossible to convince AI professors of this, let alone any real fraction of humanity, and you’d need to convince a massive number of people.\nWhat are we going to do, build powerful AI never and die when the Earth is eaten by the sun?\nIt’s actually better for safety if AI progress moves fast. This might be because the faster AI capabilities work happens, the smoother AI progress will be, and this is more important than the duration of the period. Or speeding up progress now might force future progress to be correspondingly slower. Or because safety work is probably better when done just before building the relevantly risky AI, in which case the best strategy might be to get as close to dangerous AI as possible and then stop and do safety work. Or if safety work is very useless ahead of time, maybe delay is fine, but there is little to gain by it.\nSpecific routes to slowing down AI are not worth it. For instance, avoiding working on AI capabilities research is bad because it’s so helpful for learning on the path to working on alignment. And AI safety people working in AI capabilities can be a force for making safer choices at those companies.\nAdvanced AI will help enough with other existential risks as to represent a net lowering of existential risk overall.1\nRegulators are ignorant about the nature of advanced AI (partly because it doesn’t exist, so everyone is ignorant about it). Consequently they won’t be able to regulate it effectively, and bring about desired outcomes.\nMy impression is that there are also less endorsable or less altruistic or more silly motives floating around for this attention allocation. Some things that have come up at least once in talking to people about this, or that seem to be going on:\nAdvanced AI might bring manifold wonders, e.g. long lives of unabated thriving. Getting there a bit later is fine for posterity, but for our own generation it could mean dying as our ancestors did while on the cusp of a utopian eternity. Which would be pretty disappointing. For a person who really believes in this future, it can be tempting to shoot for the best scenario—humanity builds strong, safe AI in time to save this generation—rather than the scenario where our own lives are inevitabl"}]},{"id":"38d79887-eca2-4e63-ac85-818b34d58afd","interview":"Katja Grace","data":["text",{"text":"y lost.\nSometimes people who have a heartfelt appreciation for the flourishing that technology has afforded so far can find it painful to be superficially on the side of Luddism here.\nFiguring out how minds work well enough to create new ones out of math is an incredibly deep and interesting intellectual project, which feels right to take part in. It can be hard to intuitively feel like one shouldn’t do it.\n(Illustration from a co-founder of modern computational reinforcement learning: )\nIt will be the greatest intellectual achievement of all time.\nAn achievement of science, of engineering, and of the humanities,\nwhose significance is beyond humanity,\nbeyond life,\nbeyond good and bad.\nIt is uncomfortable to contemplate projects that would put you in conflict with other people. Advocating for slower AI feels like trying to impede someone else’s project, which feels adversarial and can feel like it has a higher burden of proof than just working on your own thing.\n‘Slow-down-AGI’ sends people’s minds to e.g. industrial sabotage or terrorism, rather than more boring courses, such as, ‘lobby for labs developing shared norms for when to pause deployment of models’. This understandably encourages dropping the thought as soon as possible.\nMy weak guess is that there’s a kind of bias at play in AI risk thinking in general, where any force that isn’t zero is taken to be arbitrarily intense. Like, if there is pressure for agents to exist, there will arbitrarily quickly be arbitrarily agentic things. If there is a feedback loop, it will be arbitrarily strong. Here, if stalling AI can’t be forever, then it’s essentially zero time. If a regulation won’t obstruct every dangerous project, then is worthless. Any finite economic disincentive for dangerous AI is nothing in the face of the omnipotent economic incentives for AI. I think this is a bad mental habit: things in the real world often come down to actual finite quantities. This is very possibly an unfair diagnosis. (I’m not going to discuss this later; this is pretty much what I have to say.)\nI sense an assumption that slowing progress on a technology would be a radical and unheard-of move.\nI agree with lc that there seems to have been a quasi-taboo on the topic, which perhaps explains a lot of the non-discussion, though still calls for its own explanation. I think it suggests that concerns about uncooperativeness play a part, and the same for thinking of slowing down AI as centrally involving antisocial strategies.\nI’m not sure if any of this fully resolves why AI safety people haven’t thought about slowing down AI more, or whether people should try to do it. But my sense is that many of the above reasons are at least somewhat wrong, and motives somewhat misguided, so I want to argue about a lot of them in turn, including both arguments and vague motivational themes.\nThe mundanity of the proposal\nRestraint is not radical\nThere seems to be a common thought that technology is a kind of inevitable path along "}]},{"id":"a3d433f6-4814-43be-a449-64acd411ce5f","interview":"Dario Amodei","data":["text",{"text":"\nMachines of Loving Grace: How AI Could Transform the World for the Better\n\nI think and talk a lot about the risks of powerful AI. The company I’m the CEO of, Anthropic, does a lot of research on how to reduce these risks. Because of this, people sometimes draw the conclusion that I’m a pessimist or “doomer” who thinks AI will be mostly bad or dangerous. I don’t think that at all. In fact, one of my main reasons for focusing on risks is that they’re the only thing standing between us and what I see as a fundamentally positive future. I think that most people are underestimating just how radical the upside of AI could be, just as I think most people are underestimating how bad the risks could be.\n\nIn this essay I try to sketch out what that upside might look like—what a world with powerful AI might look like if everything goes right. Of course no one can know the future with any certainty or precision, and the effects of powerful AI are likely to be even more unpredictable than past technological changes, so all of this is unavoidably going to consist of guesses. But I am aiming for at least educated and useful guesses, which capture the flavor of what will happen even if most details end up being wrong. I’m including lots of details mainly because I think a concrete vision does more to advance discussion than a highly hedged and abstract one.\n\nFirst, however, I wanted to briefly explain why I and Anthropic haven’t talked that much about powerful AI’s upsides, and why we’ll probably continue, overall, to talk a lot about risks. In particular, I’ve made this choice out of a desire to:\n\nMaximize leverage. The basic development of AI technology and many (not all) of its benefits seems inevitable (unless the risks derail everything) and is fundamentally driven by powerful market forces. On the other hand, the risks are not predetermined and our actions can greatly change their likelihood.\nAvoid perception of propaganda. AI companies talking about all the amazing benefits of AI can come off like propagandists, or as if they’re attempting to distract from downsides. I also think that as a matter of principle it’s bad for your soul to spend too much of your time “talking your book”.\nAvoid grandiosity. I am often turned off by the way many AI risk public figures (not to mention AI company leaders) talk about the post-AGI world, as if it’s their mission to single-handedly bring it about like a prophet leading their people to salvation. I think it’s dangerous to view companies as unilaterally shaping the world, and dangerous to view practical technological goals in essentially religious terms.\nAvoid “sci-fi” baggage. Although I think most people underestimate the upside of powerful AI, the small community of people who do discuss radical AI futures often does so in an excessively “sci-fi” tone (featuring e.g. uploaded minds, space exploration, or general cyberpunk vibes). I think this causes people to take the claims less seriously, and to imbue them with a"}]},{"id":"22996616-5bb5-44d1-a94a-f2623021dd9a","interview":"Dario Amodei","data":["text",{"text":" sort of unreality. To be clear, the issue isn’t whether the technologies described are possible or likely (the main essay discusses this in granular detail)—it’s more that the “vibe” connotatively smuggles in a bunch of cultural baggage and unstated assumptions about what kind of future is desirable, how various societal issues will play out, etc. The result often ends up reading like a fantasy for a narrow subculture, while being off-putting to most people.\nYet despite all of the concerns above, I really do think it’s important to discuss what a good world with powerful AI could look like, while doing our best to avoid the above pitfalls. In fact I think it is critical to have a genuinely inspiring vision of the future, and not just a plan to fight fires. Many of the implications of powerful AI are adversarial or dangerous, but at the end of it all, there has to be something we’re fighting for, some positive-sum outcome where everyone is better off, something to rally people to rise above their squabbles and confront the challenges ahead. Fear is one kind of motivator, but it’s not enough: we need hope as well.\n\nThe list of positive applications of powerful AI is extremely long (and includes robotics, manufacturing, energy, and much more), but I’m going to focus on a small number of areas that seem to me to have the greatest potential to directly improve the quality of human life. The five categories I am most excited about are:\n\nBiology and physical health\nNeuroscience and mental health\nEconomic development and poverty\nPeace and governance\nWork and meaning\nMy predictions are going to be radical as judged by most standards (other than sci-fi “singularity” visions2), but I mean them earnestly and sincerely. Everything I’m saying could very easily be wrong (to repeat my point from above), but I’ve at least attempted to ground my views in a semi-analytical assessment of how much progress in various fields might speed up and what that might mean in practice. I am fortunate to have professional experience in both biology and neuroscience, and I am an informed amateur in the field of economic development, but I am sure I will get plenty of things wrong. One thing writing this essay has made me realize is that it would be valuable to bring together a group of domain experts (in biology, economics, international relations, and other areas) to write a much better and more informed version of what I’ve produced here. It’s probably best to view my efforts here as a starting prompt for that group.\n\nBasic assumptions and framework\nTo make this whole essay more precise and grounded, it’s helpful to specify clearly what we mean by powerful AI (i.e. the threshold at which the 5-10 year clock starts counting), as well as laying out a framework for thinking about the effects of such AI once it’s present.\n\nWhat powerful AI (I dislike the term AGI)3 will look like, and when (or if) it will arrive, is a huge topic in itself. It’s one I’ve discussed publicly and cou"}]},{"id":"5ab57e7c-4c22-49d9-91e1-da96e7aa8900","interview":"Dario Amodei","data":["text",{"text":"ld write a completely separate essay on (I probably will at some point). Obviously, many people are skeptical that powerful AI will be built soon and some are skeptical that it will ever be built at all. I think it could come as early as 2026, though there are also ways it could take much longer. But for the purposes of this essay, I’d like to put these issues aside, assume it will come reasonably soon, and focus on what happens in the 5-10 years after that. I also want to assume a definition of what such a system will look like, what its capabilities are and how it interacts, even though there is room for disagreement on this.\n\nBy powerful AI, I have in mind an AI model—likely similar to today’s LLM’s in form, though it might be based on a different architecture, might involve several interacting models, and might be trained differently—with the following properties:\n\nIn terms of pure intelligence4, it is smarter than a Nobel Prize winner across most relevant fields – biology, programming, math, engineering, writing, etc. This means it can prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch, etc.\nIn addition to just being a “smart thing you talk to”, it has all the “interfaces” available to a human working virtually, including text, audio, video, mouse and keyboard control, and internet access. It can engage in any actions, communications, or remote operations enabled by this interface, including taking actions on the internet, taking or giving directions to humans, ordering materials, directing experiments, watching videos, making videos, and so on. It does all of these tasks with, again, a skill exceeding that of the most capable humans in the world.\nIt does not just passively answer questions; instead, it can be given tasks that take hours, days, or weeks to complete, and then goes off and does those tasks autonomously, in the way a smart employee would, asking for clarification as necessary.\nIt does not have a physical embodiment (other than living on a computer screen), but it can control existing physical tools, robots, or laboratory equipment through a computer; in theory it could even design robots or equipment for itself to use.\nThe resources used to train the model can be repurposed to run millions of instances of it (this matches projected cluster sizes by ~2027), and the model can absorb information and generate actions at roughly 10x-100x human speed5. It may however be limited by the response time of the physical world or of software it interacts with.\nEach of these million copies can act independently on unrelated tasks, or if needed can all work together in the same way humans would collaborate, perhaps with different subpopulations fine-tuned to be especially good at particular tasks.\nWe could summarize this as a “country of geniuses in a datacenter”.\n\nClearly such an entity would be capable of solving very difficult problems, very fast, but it is not trivial to figure out ho"}]},{"id":"3b2c130a-7bf6-4349-8ca7-c48811ab613c","interview":"Dario Amodei","data":["text",{"text":"w fast. Two “extreme” positions both seem false to me. First, you might think that the world would be instantly transformed on the scale of seconds or days (“the Singularity”), as superior intelligence builds on itself and solves every possible scientific, engineering, and operational task almost immediately. The problem with this is that there are real physical and practical limits, for example around building hardware or conducting biological experiments. Even a new country of geniuses would hit up against these limits. Intelligence may be very powerful, but it isn’t magic fairy dust.\n\nSecond, and conversely, you might believe that technological progress is saturated or rate-limited by real world data or by social factors, and that better-than-human intelligence will add very little6. This seems equally implausible to me—I can think of hundreds of scientific or even social problems where a large group of really smart people would drastically speed up progress, especially if they aren’t limited to analysis and can make things happen in the real world (which our postulated country of geniuses can, including by directing or assisting teams of humans).\n\nI think the truth is likely to be some messy admixture of these two extreme pictures, something that varies by task and field and is very subtle in its details. I believe we need new frameworks to think about these details in a productive way.\n\nEconomists often talk about “factors of production”: things like labor, land, and capital. The phrase “marginal returns to labor/land/capital” captures the idea that in a given situation, a given factor may or may not be the limiting one – for example, an air force needs both planes and pilots, and hiring more pilots doesn’t help much if you’re out of planes. I believe that in the AI age, we should be talking about the marginal returns to intelligence7, and trying to figure out what the other factors are that are complementary to intelligence and that become limiting factors when intelligence is very high. We are not used to thinking in this way—to asking “how much does being smarter help with this task, and on what timescale?”—but it seems like the right way to conceptualize a world with very powerful AI.\n\nMy guess at a list of factors that limit or are complementary to intelligence includes:\n\nSpeed of the outside world. Intelligent agents need to operate interactively in the world in order to accomplish things and also to learn8. But the world only moves so fast. Cells and animals run at a fixed speed so experiments on them take a certain amount of time which may be irreducible. The same is true of hardware, materials science, anything involving communicating with people, and even our existing software infrastructure. Furthermore, in science many experiments are often needed in sequence, each learning from or building on the last. All of this means that the speed at which a major project—for example developing a cancer cure—can be completed may have an irre"}]},{"id":"513abc47-039c-464f-84af-dfa07f898aa7","interview":"Sam Altman","data":["text",{"text":"\nIn the next couple of decades, we will be able to do things that would have seemed like magic to our grandparents.\n\nThis phenomenon is not new, but it will be newly accelerated. People have become dramatically more capable over time; we can already accomplish things now that our predecessors would have believed to be impossible.\n\nWe are more capable not because of genetic change, but because we benefit from the infrastructure of society being way smarter and more capable than any one of us; in an important sense, society itself is a form of advanced intelligence. Our grandparents – and the generations that came before them – built and achieved great things. They contributed to the scaffolding of human progress that we all benefit from. AI will give people tools to solve hard problems and help us add new struts to that scaffolding that we couldn’t have figured out on our own. The story of progress will continue, and our children will be able to do things we can’t.\n\nIt won’t happen all at once, but we’ll soon be able to work with AI that helps us accomplish much more than we ever could without AI; eventually we can each have a personal AI team, full of virtual experts in different areas, working together to create almost anything we can imagine. Our children will have virtual tutors who can provide personalized instruction in any subject, in any language, and at whatever pace they need. We can imagine similar ideas for better healthcare, the ability to create any kind of software someone can imagine, and much more.\n\nWith these new abilities, we can have shared prosperity to a degree that seems unimaginable today; in the future, everyone’s lives can be better than anyone’s life is now. Prosperity alone doesn’t necessarily make people happy – there are plenty of miserable rich people – but it would meaningfully improve the lives of people around the world.\n\nHere is one narrow way to look at human history: after thousands of years of compounding scientific discovery and technological progress, we have figured out how to melt sand, add some impurities, arrange it with astonishing precision at extraordinarily tiny scale into computer chips, run energy through it, and end up with systems capable of creating increasingly capable artificial intelligence.\n\nThis may turn out to be the most consequential fact about all of history so far. It is possible that we will have superintelligence in a few thousand days (!); it may take longer, but I’m confident we’ll get there.\n\nHow did we get to the doorstep of the next leap in prosperity?\n\nIn three words: deep learning worked.\n\nIn 15 words: deep learning worked, got predictably better with scale, and we dedicated increasing resources to it.\n\nThat’s really it; humanity discovered an algorithm that could really, truly learn any distribution of data (or really, the underlying “rules” that produce any distribution of data). To a shocking degree of precision, the more compute and data available, the better it gets at he"}]},{"id":"d0aac6cd-a849-470a-b358-31d070761446","interview":"Sam Altman","data":["text",{"text":"lping people solve hard problems. I find that no matter how much time I spend thinking about this, I can never really internalize how consequential it is.\n\nThere are a lot of details we still have to figure out, but it’s a mistake to get distracted by any particular challenge. Deep learning works, and we will solve the remaining problems. We can say a lot of things about what may happen next, but the main one is that AI is going to get better with scale, and that will lead to meaningful improvements to the lives of people around the world.\n\nAI models will soon serve as autonomous personal assistants who carry out specific tasks on our behalf like coordinating medical care on your behalf. At some point further down the road, AI systems are going to get so good that they help us make better next-generation systems and make scientific progress across the board.\n\nTechnology brought us from the Stone Age to the Agricultural Age and then to the Industrial Age. From here, the path to the Intelligence Age is paved with compute, energy, and human will.\n\nIf we want to put AI into the hands of as many people as possible, we need to drive down the cost of compute and make it abundant (which requires lots of energy and chips). If we don’t build enough infrastructure, AI will be a very limited resource that wars get fought over and that becomes mostly a tool for rich people.\n\nWe need to act wisely but with conviction. The dawn of the Intelligence Age is a momentous development with very complex and extremely high-stakes challenges. It will not be an entirely positive story, but the upside is so tremendous that we owe it to ourselves, and the future, to figure out how to navigate the risks in front of us.\n\nI believe the future is going to be so bright that no one can do it justice by trying to write about it now; a defining characteristic of the Intelligence Age will be massive prosperity.\n\nAlthough it will happen incrementally, astounding triumphs – fixing the climate, establishing a space colony, and the discovery of all of physics – will eventually become commonplace. With nearly-limitless intelligence and abundant energy – the ability to generate great ideas, and the ability to make them happen – we can do quite a lot.\n\nAs we have seen with other technologies, there will also be downsides, and we need to start working now to maximize AI’s benefits while minimizing its harms. As one example, we expect that this technology can cause a significant change in labor markets (good and bad) in the coming years, but most jobs will change more slowly than most people think, and I have no fear that we’ll run out of things to do (even if they don’t look like “real jobs” to us today). People have an innate desire to create and to be useful to each other, and AI will allow us to amplify our own abilities like never before. As a society, we will be back in an expanding world, and we can again focus on playing positive-sum games.\n\nMany of the jobs we do today would have lo"}]},{"id":"3071d7bd-1c13-4812-8d27-90296f75b50f","interview":"Sam Altman","data":["text",{"text":"oked like trifling wastes of time to people a few hundred years ago, but nobody is looking back at the past, wishing they were a lamplighter. If a lamplighter could see the world today, he would think the prosperity all around him was unimaginable. And if we could fast-forward a hundred years from today, the prosperity all around us would feel just as unimaginable.\n"}]},{"id":"b90cba68-318c-4374-a803-2457cd99823a","interview":"Eliezer Yudkowsky","data":["text",{"text":"\nPausing AI Developments Isn’t Enough. We Need to Shut it All Down\n\nYudkowsky is a decision theorist from the U.S. and leads research at the Machine Intelligence Research Institute. He's been working on aligning Artificial General Intelligence since 2001 and is widely regarded as a founder of the field.\nAn open letter published today calls for “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”\n\nThis 6-month moratorium would be better than no moratorium. I have respect for everyone who stepped up and signed it. It’s an improvement on the margin.\n\nI refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it.\n\nRead More: AI Labs Urged to Pump the Brakes in Open Letter\n\nThe key issue is not “human-competitive” intelligence (as the open letter puts it); it’s what happens after AI gets to smarter-than-human intelligence. Key thresholds there may not be obvious, we definitely can’t calculate in advance what happens when, and it currently seems imaginable that a research lab would cross critical lines without noticing.\n\nMany researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die. Not as in “maybe possibly some remote chance,” but as in “that is the obvious thing that would happen.” It’s not that you can’t, in principle, survive creating something much smarter than you; it’s that it would require precision and preparation and new scientific insights, and probably not having AI systems composed of giant inscrutable arrays of fractional numbers.\n\n\nWithout that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general. That kind of caring is something that could in principle be imbued into an AI but we are not ready and do not currently know how.\n\nAbsent that caring, we get “the AI does not love you, nor does it hate you, and you are made of atoms it can use for something else.”\n\nThe likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include “a 10-year-old trying to play chess against Stockfish 15”, “the 11th century trying to fight the 21st century,” and “Australopithecus trying to fight Homo sapiens“.\n\nTo visualize a hostile superhuman AI, don’t imagine a lifeless book-smart thinker dwelling inside the internet and sending ill-intentioned emails. Visualize an entire alien civilization, thinking at millions of times human speeds, initially confined to computers—in a world of creatures that are, from its perspective, very stupid and very slow. A sufficiently intelligent AI won’t stay confined to computers for long. In today’s world you can email DNA strings to laboratories that will produce proteins on demand, all"}]},{"id":"bb20bbc4-aef4-455d-b993-0ab46961891f","interview":"Eliezer Yudkowsky","data":["text",{"text":"owing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing.\n\nIf somebody builds a too-powerful AI, under present conditions, I expect that every single member of the human species and all biological life on Earth dies shortly thereafter.\n\nThere’s no proposed plan for how we could do any such thing and survive. OpenAI’s openly declared intention is to make some future AI do our AI alignment homework. Just hearing that this is the plan ought to be enough to get any sensible person to panic. The other leading AI lab, DeepMind, has no plan at all.\n\nAn aside: None of this danger depends on whether or not AIs are or can be conscious; it’s intrinsic to the notion of powerful cognitive systems that optimize hard and calculate outputs that meet sufficiently complicated outcome criteria. With that said, I’d be remiss in my moral duties as a human if I didn’t also mention that we have no idea how to determine whether AI systems are aware of themselves—since we have no idea how to decode anything that goes on in the giant inscrutable arrays—and therefore we may at some point inadvertently create digital minds which are truly conscious and ought to have rights and shouldn’t be owned.\n\nThe rule that most people aware of these issues would have endorsed 50 years earlier, was that if an AI system can speak fluently and says it’s self-aware and demands human rights, that ought to be a hard stop on people just casually owning that AI and using it past that point. We already blew past that old line in the sand. And that was probably correct; I agree that current AIs are probably just imitating talk of self-awareness from their training data. But I mark that, with how little insight we have into these systems’ internals, we do not actually know.\n\n\nIf that’s our state of ignorance for GPT-4, and GPT-5 is the same size of giant capability step as from GPT-3 to GPT-4, I think we’ll no longer be able to justifiably say “probably not self-aware” if we let people make GPT-5s. It’ll just be “I don’t know; nobody knows.” If you can’t be sure whether you’re creating a self-aware AI, this is alarming not just because of the moral implications of the “self-aware” part, but because being unsure means you have no idea what you are doing and that is dangerous and you should stop.\n\nOn Feb. 7, Satya Nadella, CEO of Microsoft, publicly gloated that the new Bing would make Google “come out and show that they can dance.” “I want people to know that we made them dance,” he said.\n\nThis is not how the CEO of Microsoft talks in a sane world. It shows an overwhelming gap between how seriously we are taking the problem, and how seriously we needed to take the problem starting 30 years ago.\n\nWe are not going to bridge that gap in six months.\n\nIt took more than 60 years between when the notion of Artificial Intelligence was first proposed and studied, and for us to reach today’s capabilities. Solving safety of "}]},{"id":"e2722fe3-af85-43b6-95c2-c661b3d49841","interview":"Eliezer Yudkowsky","data":["text",{"text":"superhuman intelligence—not perfect safety, safety in the sense of “not killing literally everyone”—could very reasonably take at least half that long. And the thing about trying this with superhuman intelligence is that if you get that wrong on the first try, you do not get to learn from your mistakes, because you are dead. Humanity does not learn from the mistake and dust itself off and try again, as in other challenges we’ve overcome in our history, because we are all gone.\n\nTrying to get anything right on the first really critical try is an extraordinary ask, in science and in engineering. We are not coming in with anything like the approach that would be required to do it successfully. If we held anything in the nascent field of Artificial General Intelligence to the lesser standards of engineering rigor that apply to a bridge meant to carry a couple of thousand cars, the entire field would be shut down tomorrow.\n\nWe are not prepared. We are not on course to be prepared in any reasonable time window. There is no plan. Progress in AI capabilities is running vastly, vastly ahead of progress in AI alignment or even progress in understanding what the hell is going on inside those systems. If we actually do this, we are all going to die.\n\nMany researchers working on these systems think that we’re plunging toward a catastrophe, with more of them daring to say it in private than in public; but they think that they can’t unilaterally stop the forward plunge, that others will go on even if they personally quit their jobs. And so they all think they might as well keep going. This is a stupid state of affairs, and an undignified way for Earth to die, and the rest of humanity ought to step in at this point and help the industry solve its collective action problem.\n\nSome of my friends have recently reported to me that when people outside the AI industry hear about extinction risk from Artificial General Intelligence for the first time, their reaction is “maybe we should not build AGI, then.”\n\nHearing this gave me a tiny flash of hope, because it’s a simpler, more sensible, and frankly saner reaction than I’ve been hearing over the last 20 years of trying to get anyone in the industry to take things seriously. Anyone talking that sanely deserves to hear how bad the situation actually is, and not be told that a six-month moratorium is going to fix it.\n\nOn March 16, my partner sent me this email. (She later gave me permission to excerpt it here.)\n\n“Nina lost a tooth! In the usual way that children do, not out of carelessness! Seeing GPT4 blow away those standardized tests on the same day that Nina hit a childhood milestone brought an emotional surge that swept me off my feet for a minute. It’s all going too fast. I worry that sharing this will heighten your own grief, but I’d rather be known to you than for each of us to suffer alone.”\n\nWhen the insider conversation is about the grief of seeing your daughter lose her first tooth, and thinking she’s not goin"}]},{"id":"e469e045-ded0-41bc-aab2-fa7c30e9f621","interview":"Eliezer Yudkowsky","data":["text",{"text":"g to get a chance to grow up, I believe we are past the point of playing political chess about a six-month moratorium.\n\nIf there was a plan for Earth to survive, if only we passed a six-month moratorium, I would back that plan. There isn’t any such plan.\n\nHere’s what would actually need to be done:\n\nThe moratorium on new large training runs needs to be indefinite and worldwide. There can be no exceptions, including for governments or militaries. If the policy starts with the U.S., then China needs to see that the U.S. is not seeking an advantage but rather trying to prevent a horrifically dangerous technology which can have no true owner and which will kill everyone in the U.S. and in China and on Earth. If I had infinite freedom to write laws, I might carve out a single exception for AIs being trained solely to solve problems in biology and biotechnology, not trained on text from the internet, and not to the level where they start talking or planning; but if that was remotely complicating the issue I would immediately jettison that proposal and say to just shut it all down.\n\nShut down all the large GPU clusters (the large computer farms where the most powerful AIs are refined). Shut down all the large training runs. Put a ceiling on how much computing power anyone is allowed to use in training an AI system, and move it downward over the coming years to compensate for more efficient training algorithms. No exceptions for governments and militaries. Make immediate multinational agreements to prevent the prohibited activities from moving elsewhere. Track all GPUs sold. If intelligence says that a country outside the agreement is building a GPU cluster, be less scared of a shooting conflict between nations than of the moratorium being violated; be willing to destroy a rogue datacenter by airstrike.\n\nFrame nothing as a conflict between national interests, have it clear that anyone talking of arms races is a fool. That we all live or die as one, in this, is not a policy but a fact of nature. Make it explicit in international diplomacy that preventing AI extinction scenarios is considered a priority above preventing a full nuclear exchange, and that allied nuclear countries are willing to run some risk of nuclear exchange if that’s what it takes to reduce the risk of large AI training runs.\n\nThat’s the kind of policy change that would cause my partner and I to hold each other, and say to each other that a miracle happened, and now there’s a chance that maybe Nina will live. The sane people hearing about this for the first time and sensibly saying “maybe we should not” deserve to hear, honestly, what it would take to have that happen. And when your policy ask is that large, the only way it goes through is if policymakers realize that if they conduct business as usual, and do what’s politically easy, that means their own kids are going to die too.\n\nShut it all down.\n\nWe are not ready. We are not on track to be significantly readier in the foreseeable fut"}]},{"id":"bed93255-4534-4541-afc9-88e8d65e7a3f","interview":"Leopold Aschenbrenner","data":["text",{"text":"\nSituational Awareness: The Decade Ahead\nIIIc. Superalignment\nReliably controlling AI systems much smarter than we are is an unsolved technical problem. And while it is a solvable problem, things could very easily go off the rails during a rapid intelligence explosion. Managing this will be extremely tense; failure could easily be catastrophic.\n\nIn this piece:\nThe problem\nThe superalignment problem\nWhat failure looks like\nThe intelligence explosion makes this all incredibly tense\nThe default plan: how we can muddle through\nAligning somewhat-superhuman models\nAutomating alignment research\nSuperdefense\nWhy I’m optimistic, and why I’m scared\nThe old sorcerer\nHas finally gone away!\nNow the spirits he controls\nShall obey my commands.\n…\nI shall work wonders too.\n…\nSir, I’m in desperate straits!\nThe spirits I summoned –\nI can’t get rid of them.\n\nJohann Wolfgang von Goethe, “The Sorcerer’s Apprentice”\nBy this point, you have probably heard of the AI doomers. You might have been intrigued by their arguments, or you might have dismissed them off-hand. You’re loath to read yet another doom-and-gloom meditation.\n\nI am not a doomer. Misaligned superintelligence is probably not the biggest AI risk. But I did spend the past year working on technical research on aligning AI systems as my day-job at OpenAI, working with Ilya and the Superalignment team. There is a very real technical problem: our current alignment techniques (methods to ensure we can reliably control, steer, and trust AI systems) won’t scale to superhuman AI systems. What I want to do is explain what I see as the “default” plan for how we’ll muddle through, and why I’m optimistic. While not enough people are on the ball—we should have much more ambitious efforts to solve this problem!—overall, we’ve gotten lucky with how deep learning has shaken out, there’s a lot of empirical low-hanging fruit that will get us part of the way, and we’ll have the advantage of millions of automated AI researchers to get us the rest of the way.\n\nBut I also want to tell you why I’m worried. Most of all, ensuring alignment doesn’t go awry will require extreme competence in managing the intelligence explosion. If we do rapidly transition from AGI to superintelligence, we will face a situation where, in less than a year, we will go from recognizable human-level systems for which descendants of current alignment techniques will mostly work fine, to much more alien, vastly superhuman systems that pose a qualitatively different, fundamentally novel technical alignment problem; at the same time, going from systems where failure is low-stakes to extremely powerful systems where failure could be catastrophic; all while most of the world is probably going kind of crazy. It makes me pretty nervous.\n\nBy the time the decade is out, we’ll have billions of vastly superhuman AI agents running around. These superhuman AI agents will be capable of extremely complex and creative behavior; we will have no hope of following along. We’ll"}]},{"id":"caa29e41-1306-4654-b939-ca9165be2ea4","interview":"Leopold Aschenbrenner","data":["text",{"text":" be like first graders trying to supervise people with multiple doctorates.\n\nIn essence, we face a problem of handing off trust. By the end of the intelligence explosion, we won’t have any hope of understanding what our billion superintelligences are doing (except as they might choose to explain to us, like they might to a child). And we don’t yet have the technical ability to reliably guarantee even basic side constraints for these systems, like “don’t lie” or “follow the law” or “don’t try to exfiltrate your server”. Reinforcement from human feedback (RLHF) works very well for adding such side constraints for current systems—but RLHF relies on humans being able to understand and supervise AI behavior, which fundamentally won’t scale to superhuman systems.\n\nSimply put, without a very concerted effort, we won’t be able to guarantee that superintelligence won’t go rogue (and this is acknowledged by many leaders in the field). Yes, it may all be fine by default. But we simply don’t know yet. Especially once future AI systems aren’t just trained with imitation learning, but large-scale, long-horizon RL (reinforcement learning), they will acquire unpredictable behaviors of their own, shaped by a trial-and-error process (for example, they may learn to lie or seek power, simply because these are successful strategies in the real world!).\n\nThe stakes will be high enough that hoping for the best simply isn’t a good enough answer on alignment.\n\nThe problem\nThe superalignment problem\nWe’ve been able to develop a very successful method for aligning (i.e., steering/controlling) current AI systems (AI systems dumber than us!): Reinforcement Learning from Human Feedback (RLHF). The idea behind RLHF is simple: the AI system tries stuff, humans rate whether its behavior was good or bad, and then reinforce good behaviors and penalize bad behaviors. That way, it learns to follow human preferences.\n\nIndeed, RLHF has been the key behind the success of ChatGPT and others. Base models had lots of raw smarts but weren’t applying these in a useful way by default; they usually just responded with a garbled mess resembling random internet text. Via RLHF, we can steer their behavior, instilling important basics like instruction-following and helpfulness. RLHF also allows us to bake in safety guardrails: for example, if a user asks me for bioweapon instructions, the model should probably refuse.\n\nThe core technical problem of superalignment is simple: how do we control AI systems (much) smarter than us?\n\nRLHF will predictably break down as AI systems get smarter, and we will face fundamentally new and qualitatively different technical challenges. Imagine, for example, a superhuman AI system generating a million lines of code in a new programming language it invented. If you asked a human rater in an RLHF procedure, “does this code contain any security backdoors?” they simply wouldn’t know. They wouldn’t be able to rate the output as good or bad, safe or unsafe, and so we wo"}]},{"id":"36488964-44b4-4bd5-93e3-32b8dc523b9b","interview":"Leopold Aschenbrenner","data":["text",{"text":"uldn’t be able to reinforce good behaviors and penalize bad behaviors with RLHF.\n\n\nAligning AI systems via human supervision (as in RLHF) won’t scale to superintelligence. Based on illustration from “Weak-to-strong generalization“.\nEven now, AI labs already need to pay expert software engineers to give RLHF ratings for ChatGPT code—the code current models can generate is already pretty advanced! Human labeler-pay has gone from a few dollars for MTurk labelers to ~$100/hour for GPQA questions in the last few years. In the (near) future, even the best human experts spending lots of time won’t be good enough. We’re starting to hit early versions of the superalignment problem in the real world now, and very soon this will be a major issue even just for practically deploying next-generation systems. It’s clear we will need a successor to RLHF that scales to AI capabilities better than human-level, where human supervision breaks down. In some sense, the goal of superalignment research efforts is to repeat the success story of RLHF: make the basic research bets that will be necessary to steer and deploy AI systems a couple years down the line.\n\nWhat failure looks like\nPeople too often just picture a “GPT-6 chatbot,” informing their intuitions that surely these wouldn’t be dangerously misaligned. As discussed previously in this series, the “unhobbling” trajectory points to agents, trained with RL, in the near future. I think Roger’s graphic gets it right:\n\n\nRoger Grosse (Professor at the University of Toronto)\nOne way to think of what we’re trying to accomplish with alignment, from the safety perspective, is add side-constraints. Consider a future powerful “base model” that, in a second stage of training, we train with long-horizon RL to run a business and make money (as a simplified example):\n\nBy default, it may well learn to lie, to commit fraud, to deceive, to hack, to seek power, and so on—simply because these can be successful strategies to make money in the real world!\nWhat we want is to add side-constraints: don’t lie, don’t break the law, etc.\nBut here we come back to the fundamental issue of aligning superhuman systems: we won’t be able to understand what they are doing, and so we won’t be able to notice and penalize bad behavior with RLHF.\nIf we can’t add these side-constraints, it’s not clear what will happen. Maybe we’ll get lucky and things will be benign by default (for example, maybe we can get pretty far without the AI systems having long-horizon goals, or the undesirable behaviors will be  minor). But it’s also totally plausible they’ll learn much more serious undesirable behaviors: they’ll learn to lie, they’ll learn to seek power, they’ll learn to behave nicely when humans are looking and pursue more nefarious strategies when we aren’t watching, and so on.\n\nThe superalignment problem being unsolved means that we simply won’t have the ability to ensure even these basic side constraints for these superintelligence systems, like “will the"}]},{"id":"7ac69b1c-d5ba-47a7-8ca2-612913701c3e","interview":"Leopold Aschenbrenner","data":["text",{"text":"y reliably follow my instructions?” or “will they honestly answer my questions?” or “will they not deceive humans?”. People often associate alignment with some complicated questions about human values, or jump to political controversies, but deciding on what behaviors and values to instill in the model, while important, is a separate problem. The primary problem is that for whatever you want to instill the model (including ensuring very basic things, like “follow the law”!) we don’t yet know how to do that for the very powerful AI systems we are building very soon.\n\nAgain, the consequences of this aren’t totally clear. What is clear is that superintelligence will have vast capabilities—and so misbehavior could fairly easily be catastrophic. What’s more, I expect that within a small number of years, these AI systems will be integrated in many critical systems, including military systems (failure to do so would mean complete dominance by adversaries). It sounds crazy, but remember when everyone was saying we wouldn’t connect AI to the internet? The same will go for things like “we’ll make sure a human is always in the loop!”—as people say today.\n\nAlignment failures then might look like isolated incidents, say, an autonomous agent committing fraud, a model instance self-exfiltrating, an automated researcher falsifying an experimental result, or a drone swarm overstepping rules of engagement. But failures could also be much larger scale or more systematic—in the extreme, failures could look more like a robot rebellion. We’ll have summoned a fairly alien intelligence, one much smarter than us, one whose architecture and training process wasn’t even designed by us but some super-smart previous generation of AI systems, one where we can’t even begin to understand what they’re doing, it’ll be running our military, and its goals will have been learned by a natural-selection-esque process.\n\nUnless we solve alignment—unless we figure out how to instill those side-constraints—there’s no particular reason to expect this small civilization of superintelligences will continue obeying human commands in the long run. It seems totally within the realm of possibilities that at some point they’ll simply conspire to cut out the humans, whether suddenly or gradually.\n\nThe intelligence explosion makes this all incredibly tense\nI am optimistic that superalignment is a solvable technical problem. Just like we developed RLHF, so we can develop the successor to RLHF for superhuman systems and do the science that gives us high confidence in our methods. If things continue to progress iteratively, if we insist on rigorous safety testing and so on, it should all be doable (and I’ll discuss my current best-guess of how we’ll muddle through more in a bit).\n\nWhat makes this incredibly hair-raising is the possibility of an intelligence explosion: that we might make the transition from roughly human-level systems to vastly superhuman systems extremely rapidly, perhaps in less than"}]}],"date":"2025-02-08T22:09:54.479Z"}],"metadata":["v0.2",{"duration":0,"startTimestamp":0,"totalCost":"","author":"","organization":""}]}